{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_371079/3351334044.py:20: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import variational\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activations along the path\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datasets\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def entropy(p, dim = -1, keepdim = False):\n",
    "    return -torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim=dim, keepdim=keepdim)\n",
    "    #return (p * p.log()).sum(dim=dim, keepdim=keepdim)\n",
    "\n",
    "def collect_prob(data_loader, model):\n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=False, num_samples=4000)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, sampler=sampler, shuffle=False)\n",
    "    prob = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(data_loader_small, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            data, target = batch\n",
    "            output = model(data)\n",
    "            prob.append(F.softmax(output, dim=-1).data)\n",
    "    return torch.cat(prob)\n",
    "\n",
    "def get_membership_attack_data(retain_loader, forget_loader, test_loader, model):    \n",
    "    retain_prob = collect_prob(retain_loader, model)\n",
    "    forget_prob = collect_prob(forget_loader, model)\n",
    "    test_prob = collect_prob(test_loader, model)\n",
    "    \n",
    "    X_r = torch.cat([entropy(retain_prob), entropy(test_prob)]).cpu().numpy().reshape(-1, 1)\n",
    "    Y_r = np.concatenate([np.ones(len(retain_prob)), np.zeros(len(test_prob))])\n",
    "    \n",
    "    X_f = entropy(forget_prob).cpu().numpy().reshape(-1, 1)\n",
    "    Y_f = np.concatenate([np.ones(len(forget_prob))])    \n",
    "    return X_f, Y_f, X_r, Y_r\n",
    "\n",
    "def get_membership_attack_prob(retain_loader, forget_loader, test_loader, model):\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader, forget_loader, test_loader, model)\n",
    "    #clf = SVC(C=3,gamma='auto',kernel='rbf')\n",
    "    clf = LogisticRegression(class_weight='balanced',solver='lbfgs',multi_class='multinomial')\n",
    "    clf.fit(X_r, Y_r)\n",
    "    results = clf.predict(X_f)\n",
    "    results1 = clf.predict(X_r)\n",
    "    acc = accuracy_score(results, Y_f)\n",
    "    train_ac = accuracy_score(results1, Y_r)\n",
    "    TP, FP, TN, FN = perf_measure(Y_r, results1)\n",
    "    FPR = FP/(FP+TN)\n",
    "    FNR = FN/(FN+TP)\n",
    "    \n",
    "    print (f\"TP:{TP}, FP{FP}, TN{TN}, FN{FN}\")\n",
    "    print (f\"false negative rate: {FN/(FN+TP)}\")\n",
    "    print (f\"false positive rate: {FP/(FP+TN)}\")\n",
    "    return acc, train_acc, FPR, FNR #results.mean(), results1.mean()\n",
    "    \n",
    "def plot_entropy_dist(model,retain_loader,forget_loader,test_loader_full, title, ax):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from matplotlib.ticker import FuncFormatter\n",
    "    #train_loader_full, valid_loader_full, test_loader_full = datasets.get_loaders(dataset, batch_size=100, seed=0, augment=False, shuffle=False)\n",
    "    #indexes = np.flatnonzero(np.array(train_loader_full.dataset.targets) == class_to_forget)\n",
    "    #replaced = np.random.RandomState(0).choice(indexes, size=100 if num_to_forget==100 else len(indexes), replace=False)\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader,forget_loader,test_loader_full, model)\n",
    "    #np.savetxt('retain_prob.txt', X_r[Y_r==1])\n",
    "    #np.savetxt('forget_prob.txt', X_f)\n",
    "    #np.savetxt('test_prob.txt', X_r[Y_r==0])\n",
    "    sns.distplot(np.log(X_r[Y_r==1]).reshape(-1), kde=False, norm_hist=True, rug=False, label='retain', ax=plt)\n",
    "    sns.distplot(np.log(X_r[Y_r==0]).reshape(-1), kde=False, norm_hist=True, rug=False, label='test', ax=plt)\n",
    "    sns.distplot(np.log(X_f).reshape(-1), kde=False, norm_hist=True, rug=False, label='forget', ax=plt)\n",
    "    plt.legend(prop={'size': 14})\n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.title(title,size=18)\n",
    "    plt.xlabel('Log of Entropy',size=14)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def membership_attack(retain_loader,forget_loader,test_loader,model, name):\n",
    "    prob, train_acc, FPR, FNR = get_membership_attack_prob(retain_loader,forget_loader,test_loader,model)\n",
    "    print(\"Attack prob: \", prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=512, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=512, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics=get_metrics(model,dataloader,criterion,False)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "    return _,_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        #run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.01, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    #print(f\"{name} ->\"\n",
    "    #      f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_activations(model_scrubf, modelf0, delta_w_s, delta_w_m0, data_loader, \\\n",
    "                    loss_fn=nn.CrossEntropyLoss(),\\\n",
    "                    optimizer=torch.optim.SGD, \\\n",
    "                    seed=1,quiet=False):\n",
    "\n",
    "    model_scrubf.eval()\n",
    "    modelf0.eval()\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    \n",
    "    metrics = AverageMeter()    \n",
    "    num_classes = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "        batch = [tensor.to(next(model_scrubf.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        \n",
    "        output_sf = model_scrubf(input)\n",
    "        G_sf = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=True)\n",
    "            grads = torch.cat([g.view(-1) for g in grads])\n",
    "            G_sf.append(grads)\n",
    "\n",
    "        grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_sf = torch.stack(G_sf)#.pow(2)\n",
    "        delta_f_sf_update = torch.matmul(G_sf,delta_w_s.sqrt()*torch.empty_like(delta_w_s).normal_())\n",
    "        G_sf = G_sf.pow(2)\n",
    "        delta_f_sf = torch.matmul(G_sf,delta_w_s)\n",
    "\n",
    "        output_m0 = modelf0(input)\n",
    "        G_m0 = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=True)\n",
    "            grad_m0 = torch.cat([g.view(-1) for g in grads])\n",
    "            G_m0.append(grad_m0)\n",
    "\n",
    "        grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_m0 = torch.stack(G_m0).pow(2)\n",
    "        delta_f_m0 = torch.matmul(G_m0,delta_w_m0)\n",
    "        \n",
    "        kl = ((output_m0 - output_sf).pow(2)/delta_f_m0 + delta_f_sf/delta_f_m0 - torch.log(delta_f_sf/delta_f_m0) - 1).sum()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        output_sf += delta_f_sf_update#delta_f_sf.sqrt()*torch.empty_like(delta_f_sf).normal_()\n",
    "        \n",
    "        loss = loss_fn(output_sf, target)\n",
    "        metrics.update(n=input.size(0), loss=loss.item(), error=get_error(output_sf, target), kl=kl.item())\n",
    "    \n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar100_resnet_1_0_forget_None_lr_0_05_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar100_resnet_1_0_forget_None_lr_0_05_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 100\n",
      "[0] train metrics:{\"loss\": 3.90395583114624, \"error\": 0.9}\n",
      "Learning Rate : 0.05\n",
      "[0] dry_run metrics:{\"loss\": 3.3964400123596192, \"error\": 0.8184}\n",
      "Learning Rate : 0.05\n",
      "[0] test metrics:{\"loss\": 3.425481930923462, \"error\": 0.8199}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 13.21 sec\n",
      "[1] train metrics:{\"loss\": 3.254811417770386, \"error\": 0.78765}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.96 sec\n",
      "[2] train metrics:{\"loss\": 2.767958198547363, \"error\": 0.68885}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.9 sec\n",
      "[3] train metrics:{\"loss\": 2.425668185424805, \"error\": 0.6054}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.98 sec\n",
      "[4] train metrics:{\"loss\": 2.1650457149505615, \"error\": 0.53205}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.94 sec\n",
      "[5] train metrics:{\"loss\": 1.9714893806457519, \"error\": 0.472875}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.96 sec\n",
      "[6] train metrics:{\"loss\": 1.8222509164810181, \"error\": 0.422375}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.94 sec\n",
      "[7] train metrics:{\"loss\": 1.6924866373062133, \"error\": 0.37525}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 7.0 sec\n",
      "[8] train metrics:{\"loss\": 1.5848635959625244, \"error\": 0.32715}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.82 sec\n",
      "[9] train metrics:{\"loss\": 1.4961945726394654, \"error\": 0.28175}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 7.01 sec\n",
      "[10] train metrics:{\"loss\": 1.4256258754730224, \"error\": 0.235625}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.99 sec\n",
      "[11] train metrics:{\"loss\": 1.4073870859146118, \"error\": 0.209125}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.92 sec\n",
      "[12] train metrics:{\"loss\": 1.3633887067794799, \"error\": 0.17375}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 7.12 sec\n",
      "[13] train metrics:{\"loss\": 1.367438131713867, \"error\": 0.151875}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.84 sec\n",
      "[14] train metrics:{\"loss\": 1.360876547241211, \"error\": 0.1326}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.97 sec\n",
      "[15] train metrics:{\"loss\": 1.3357093097686767, \"error\": 0.111875}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 8.29 sec\n",
      "[16] train metrics:{\"loss\": 1.3584872919082642, \"error\": 0.10725}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 7.59 sec\n",
      "[17] train metrics:{\"loss\": 1.3642960355758667, \"error\": 0.101525}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 7.72 sec\n",
      "[18] train metrics:{\"loss\": 1.35577138671875, \"error\": 0.0929}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.83 sec\n",
      "[19] train metrics:{\"loss\": 1.3634055824279785, \"error\": 0.089925}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.98 sec\n",
      "[20] train metrics:{\"loss\": 1.3779572025299072, \"error\": 0.090525}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 7.12 sec\n",
      "[21] train metrics:{\"loss\": 1.3472309638977051, \"error\": 0.077225}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 7.62 sec\n",
      "[22] train metrics:{\"loss\": 1.375188256263733, \"error\": 0.0841}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 7.44 sec\n",
      "[23] train metrics:{\"loss\": 1.3985063148498535, \"error\": 0.089}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 7.03 sec\n",
      "[24] train metrics:{\"loss\": 1.3749998462677002, \"error\": 0.076775}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.83 sec\n",
      "[25] train metrics:{\"loss\": 1.3702189516067504, \"error\": 0.074625}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 7.03 sec\n",
      "[26] train metrics:{\"loss\": 1.3533171112060547, \"error\": 0.070025}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 7.73 sec\n",
      "[27] train metrics:{\"loss\": 1.3735678150177002, \"error\": 0.077175}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 7.25 sec\n",
      "[28] train metrics:{\"loss\": 1.4002151023864746, \"error\": 0.08125}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 6.79 sec\n",
      "[29] train metrics:{\"loss\": 1.4150877195358276, \"error\": 0.080625}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 7.14 sec\n",
      "[30] train metrics:{\"loss\": 1.4351089038848877, \"error\": 0.080525}\n",
      "Learning Rate : 0.05\n",
      "Epoch Time: 7.09 sec\n",
      "Pure training time: 220.86999999999998 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset cifar100 --dataroot=data/cifar-100-python --model resnet --filters 1.0 --lr 0.05 --lossfn ce --num-classes 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar10_resnet_1_0_forget_None_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar10_resnet_1_0_forget_None_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Number of Classes: 10\n",
      "Epoch: [0][0/313]\tTime 0.051 (0.051)\tData 0.024 (0.024)\tLoss 2.5286 (2.5286)\tAcc@1 12.500 (12.500)\tAcc@5 53.906 (53.906)\n",
      " * Acc@1 71.548 Acc@5 97.795\n",
      "[0] test metrics:{\"loss\": 0.6419589270591736, \"error\": 0.2252}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.51 sec\n",
      "Epoch: [1][0/313]\tTime 0.017 (0.017)\tData 0.008 (0.008)\tLoss 0.5148 (0.5148)\tAcc@1 80.469 (80.469)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 84.875 Acc@5 99.525\n",
      "Epoch Time: 5.25 sec\n",
      "Epoch: [2][0/313]\tTime 0.016 (0.016)\tData 0.007 (0.007)\tLoss 0.2382 (0.2382)\tAcc@1 92.969 (92.969)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 91.730 Acc@5 99.855\n",
      "Epoch Time: 5.43 sec\n",
      "Epoch: [3][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.1412 (0.1412)\tAcc@1 93.750 (93.750)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 96.978 Acc@5 99.962\n",
      "Epoch Time: 5.42 sec\n",
      "Epoch: [4][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0410 (0.0410)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.452 Acc@5 100.000\n",
      "Epoch Time: 5.28 sec\n",
      "Epoch: [5][0/313]\tTime 0.016 (0.016)\tData 0.007 (0.007)\tLoss 0.0210 (0.0210)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.927 Acc@5 100.000\n",
      "[5] test metrics:{\"loss\": 0.8287803944587707, \"error\": 0.1718}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.67 sec\n",
      "Epoch: [6][0/313]\tTime 0.022 (0.022)\tData 0.014 (0.014)\tLoss 0.0077 (0.0077)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.970 Acc@5 100.000\n",
      "Epoch Time: 5.14 sec\n",
      "Epoch: [7][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0044 (0.0044)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.990 Acc@5 100.000\n",
      "Epoch Time: 5.23 sec\n",
      "Epoch: [8][0/313]\tTime 0.018 (0.018)\tData 0.011 (0.011)\tLoss 0.0022 (0.0022)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.2 sec\n",
      "Epoch: [9][0/313]\tTime 0.027 (0.027)\tData 0.019 (0.019)\tLoss 0.0012 (0.0012)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 5.2 sec\n",
      "Epoch: [10][0/313]\tTime 0.028 (0.028)\tData 0.020 (0.020)\tLoss 0.0022 (0.0022)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "[10] test metrics:{\"loss\": 0.8962463143348693, \"error\": 0.167}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.35 sec\n",
      "Epoch: [11][0/313]\tTime 0.022 (0.022)\tData 0.015 (0.015)\tLoss 0.0030 (0.0030)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.24 sec\n",
      "Epoch: [12][0/313]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0024 (0.0024)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 5.16 sec\n",
      "Epoch: [13][0/313]\tTime 0.028 (0.028)\tData 0.020 (0.020)\tLoss 0.0016 (0.0016)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.17 sec\n",
      "Epoch: [14][0/313]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0017 (0.0017)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "Epoch Time: 5.16 sec\n",
      "Epoch: [15][0/313]\tTime 0.018 (0.018)\tData 0.007 (0.007)\tLoss 0.0008 (0.0008)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "[15] test metrics:{\"loss\": 0.9364831091880799, \"error\": 0.1685}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.28 sec\n",
      "Epoch: [16][0/313]\tTime 0.023 (0.023)\tData 0.014 (0.014)\tLoss 0.0047 (0.0047)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.18 sec\n",
      "Epoch: [17][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0017 (0.0017)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.19 sec\n",
      "Epoch: [18][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0009 (0.0009)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.09 sec\n",
      "Epoch: [19][0/313]\tTime 0.016 (0.016)\tData 0.007 (0.007)\tLoss 0.0010 (0.0010)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.05 sec\n",
      "Epoch: [20][0/313]\tTime 0.018 (0.018)\tData 0.010 (0.010)\tLoss 0.0008 (0.0008)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "[20] test metrics:{\"loss\": 0.982644875907898, \"error\": 0.1686}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.17 sec\n",
      "Epoch: [21][0/313]\tTime 0.028 (0.028)\tData 0.018 (0.018)\tLoss 0.0014 (0.0014)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.09 sec\n",
      "Epoch: [22][0/313]\tTime 0.016 (0.016)\tData 0.008 (0.008)\tLoss 0.0016 (0.0016)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 5.08 sec\n",
      "Epoch: [23][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0017 (0.0017)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.23 sec\n",
      "Epoch: [24][0/313]\tTime 0.024 (0.024)\tData 0.016 (0.016)\tLoss 0.0011 (0.0011)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.41 sec\n",
      "Epoch: [25][0/313]\tTime 0.025 (0.025)\tData 0.017 (0.017)\tLoss 0.0007 (0.0007)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "[25] test metrics:{\"loss\": 1.0434467685699462, \"error\": 0.1695}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.53 sec\n",
      "Pure training time: 135.73 sec\n"
     ]
    }
   ],
   "source": [
    "%run main_merged.py --dataset cifar10 --model resnet --dataroot=data/cifar-10-batches-py/ --filters 1.0 --lr 0.01 \\\n",
    "--resume checkpoints/cifar100_resnet_1_0_forget_None_lr_0_05_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 5e-4 --batch-size 128 --epochs 26 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar10_resnet_1_0_forget_[5]_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar10_resnet_1_0_forget_[5]_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Number of Classes: 10\n",
      "Epoch: [0][0/313]\tTime 0.028 (0.028)\tData 0.014 (0.014)\tLoss 2.5405 (2.5405)\tAcc@1 14.844 (14.844)\tAcc@5 57.812 (57.812)\n",
      " * Acc@1 75.838 Acc@5 98.410\n",
      "[0] test metrics:{\"loss\": 0.553347797287835, \"error\": 0.1931111109521654}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.31 sec\n",
      "Epoch: [1][0/313]\tTime 0.017 (0.017)\tData 0.007 (0.007)\tLoss 0.4162 (0.4162)\tAcc@1 86.719 (86.719)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 89.357 Acc@5 99.760\n",
      "Epoch Time: 5.16 sec\n",
      "Epoch: [2][0/313]\tTime 0.027 (0.027)\tData 0.017 (0.017)\tLoss 0.1491 (0.1491)\tAcc@1 96.875 (96.875)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 95.542 Acc@5 99.932\n",
      "Epoch Time: 5.17 sec\n",
      "Epoch: [3][0/313]\tTime 0.020 (0.020)\tData 0.010 (0.010)\tLoss 0.0631 (0.0631)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 98.982 Acc@5 99.985\n",
      "Epoch Time: 5.13 sec\n",
      "Epoch: [4][0/313]\tTime 0.016 (0.016)\tData 0.008 (0.008)\tLoss 0.0160 (0.0160)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.875 Acc@5 100.000\n",
      "Epoch Time: 5.09 sec\n",
      "Epoch: [5][0/313]\tTime 0.028 (0.028)\tData 0.019 (0.019)\tLoss 0.0072 (0.0072)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.978 Acc@5 100.000\n",
      "[5] test metrics:{\"loss\": 0.6813768035040961, \"error\": 0.14644444428549871}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.14 sec\n",
      "Epoch: [6][0/313]\tTime 0.025 (0.025)\tData 0.016 (0.016)\tLoss 0.0026 (0.0026)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.990 Acc@5 100.000\n",
      "Epoch Time: 5.14 sec\n",
      "Epoch: [7][0/313]\tTime 0.021 (0.021)\tData 0.011 (0.011)\tLoss 0.0051 (0.0051)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "Epoch Time: 5.1 sec\n",
      "Epoch: [8][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0015 (0.0015)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "Epoch Time: 5.15 sec\n",
      "Epoch: [9][0/313]\tTime 0.025 (0.025)\tData 0.015 (0.015)\tLoss 0.0011 (0.0011)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 5.24 sec\n",
      "Epoch: [10][0/313]\tTime 0.026 (0.026)\tData 0.016 (0.016)\tLoss 0.0019 (0.0019)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "[10] test metrics:{\"loss\": 0.7423057414160834, \"error\": 0.14355555555555555}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.36 sec\n",
      "Epoch: [11][0/313]\tTime 0.033 (0.033)\tData 0.021 (0.021)\tLoss 0.0016 (0.0016)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 5.24 sec\n",
      "Epoch: [12][0/313]\tTime 0.017 (0.017)\tData 0.007 (0.007)\tLoss 0.0016 (0.0016)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 5.21 sec\n",
      "Epoch: [13][0/313]\tTime 0.017 (0.017)\tData 0.007 (0.007)\tLoss 0.0010 (0.0010)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 5.18 sec\n",
      "Epoch: [14][0/313]\tTime 0.024 (0.024)\tData 0.014 (0.014)\tLoss 0.0017 (0.0017)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 5.17 sec\n",
      "Epoch: [15][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0008 (0.0008)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[15] test metrics:{\"loss\": 0.7793772072262234, \"error\": 0.14333333317438762}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.27 sec\n",
      "Epoch: [16][0/313]\tTime 0.029 (0.029)\tData 0.018 (0.018)\tLoss 0.0017 (0.0017)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.16 sec\n",
      "Epoch: [17][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0013 (0.0013)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 5.15 sec\n",
      "Epoch: [18][0/313]\tTime 0.016 (0.016)\tData 0.007 (0.007)\tLoss 0.0009 (0.0009)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.24 sec\n",
      "Epoch: [19][0/313]\tTime 0.016 (0.016)\tData 0.009 (0.009)\tLoss 0.0013 (0.0013)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.16 sec\n",
      "Epoch: [20][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0009 (0.0009)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[20] test metrics:{\"loss\": 0.8344745484458076, \"error\": 0.14366666650772095}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.24 sec\n",
      "Epoch: [21][0/313]\tTime 0.019 (0.019)\tData 0.011 (0.011)\tLoss 0.0011 (0.0011)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.2 sec\n",
      "Epoch: [22][0/313]\tTime 0.019 (0.019)\tData 0.008 (0.008)\tLoss 0.0013 (0.0013)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.18 sec\n",
      "Epoch: [23][0/313]\tTime 0.017 (0.017)\tData 0.008 (0.008)\tLoss 0.0014 (0.0014)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.22 sec\n",
      "Epoch: [24][0/313]\tTime 0.019 (0.019)\tData 0.008 (0.008)\tLoss 0.0015 (0.0015)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.2 sec\n",
      "Epoch: [25][0/313]\tTime 0.017 (0.017)\tData 0.007 (0.007)\tLoss 0.0006 (0.0006)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[25] test metrics:{\"loss\": 0.8943818838331434, \"error\": 0.14366666650772095}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.25 sec\n",
      "Pure training time: 134.82999999999998 sec\n"
     ]
    }
   ],
   "source": [
    "%run main_merged.py --dataset cifar10 --model resnet --dataroot=data/cifar-10-batches-py/ --filters 1 --lr 0.01 \\\n",
    "--resume checkpoints/cifar100_resnet_1_0_forget_None_lr_0_05_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 5e-4 --batch-size 128 --epochs 26 \\\n",
    "--forget-class 5 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 11175178\n"
     ]
    }
   ],
   "source": [
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model0 = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 4000#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model0.cuda()\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _  = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=512, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=256, seed=seed, shuffle=True)\n",
    "\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "36000\n",
      "10000\n",
      "40000\n",
      "{6: 4000, 9: 4000, 4: 4000, 1: 4000, 2: 4000, 8: 4000, 3: 4000, 7: 4000, 5: 4000, 0: 4000}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'sgd'\n",
    "args.gamma = 0.95\n",
    "args.alpha = 0.001\n",
    "args.beta = 0\n",
    "args.smoothing = 0.0\n",
    "args.msteps = 2\n",
    "args.clip = 0.2\n",
    "args.sstart = 3\n",
    "args.kd_T = 4\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_batch_size = 128\n",
    "args.del_batch_size = 512\n",
    "args.sgda_epochs = 3\n",
    "args.sgda_learning_rate = 0.0005\n",
    "args.lr_decay_epochs = [3,5,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> SCRUB unlearning ...\n",
      " * Acc@1 97.314 \n",
      "maximize loss: -41.91\t minimize loss: 0.10\t train_acc: 97.31388854980469\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 85.311 \n",
      "maximize loss: -294.75\t minimize loss: 1.52\t train_acc: 85.31111145019531\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 97.917 \n",
      "maximize loss: 0.00\t minimize loss: 0.11\t train_acc: 97.91666412353516\n"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "acc_vs = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> SCRUB unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True, prefix=\"train_cifar10_resnet\")\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True, prefix=\"forget_cifar10_resnet\")\n",
    "    acc_v, acc5_v, loss_v = validate(valid_loader_full, model_s, criterion_cls, args, True)\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "    acc_vs.append(100-acc_v.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\")\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    \n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n",
    "acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "acc_v, acc5_v, loss_v = validate(valid_loader_full, model_s, criterion_cls, args, True)\n",
    "acc_rs.append(100-acc_r.item())\n",
    "acc_fs.append(100-acc_f.item())\n",
    "acc_vs.append(100-acc_v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.plot(indices, acc_vs, marker='.', alpha=1, label='valid-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('SCRUB retain-, valid- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Model_D_r -> Loss:0.001, Error:2.777777777777778e-05\n",
      "Original_Model_D_f -> Loss:0.001, Error:0.0\n",
      "Original_Model_D_t -> Loss:0.779, Error:0.1728\n"
     ]
    }
   ],
   "source": [
    "m_D_r_activations,m_D_r_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(retain_loader),'Original_Model_D_r')\n",
    "m_D_f_activations,m_D_f_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(forget_loader),'Original_Model_D_f')\n",
    "m_D_t_activations,m_D_t_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(test_loader_full),'Original_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain_Model_D_r -> Loss:0.001, Error:2.777777777777778e-05\n",
      "Retrain_Model_D_f -> Loss:12.73, Error:1.0\n",
      "Retrain_Model_D_t -> Loss:1.817, Error:0.2292\n"
     ]
    }
   ],
   "source": [
    "m0_D_r_activations,m0_D_r_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(retain_loader),'Retrain_Model_D_r')\n",
    "m0_D_f_activations,m0_D_f_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(forget_loader),'Retrain_Model_D_f')\n",
    "m0_D_t_activations,m0_D_t_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(test_loader_full),'Retrain_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRUB_D_r -> Loss:0.031, Error:0.008972222222222222\n",
      "SCRUB_D_f -> Loss:550.672, Error:1.0\n",
      "SCRUB_D_t -> Loss:56.039, Error:0.2369\n"
     ]
    }
   ],
   "source": [
    "ntk_D_r_activations,ntk_D_r_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(retain_loader),'SCRUB_D_r')\n",
    "ntk_D_f_activations,ntk_D_f_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(forget_loader),'SCRUB_D_f')\n",
    "ntk_D_t_activations,ntk_D_t_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(test_loader_full),'SCRUB_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, modelf)\n",
    "#hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-7\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_D_r_activations,fisher_D_r_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(retain_loader),'Fisher_D_r')\n",
    "fisher_D_f_activations,fisher_D_f_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(forget_loader),'Fisher_D_f')\n",
    "fisher_D_t_activations,fisher_D_t_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(test_loader_full),'Fisher_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information in the Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_D_r_activations,finetune_D_r_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(retain_loader),'Finetune_D_r')\n",
    "finetune_D_f_activations,finetune_D_f_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(forget_loader),'Finetune_D_f')\n",
    "finetune_D_t_activations,finetune_D_t_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(test_loader_full),'Finetune_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(5,1,figsize=(9,20))\n",
    "ax = [0,0,0,0,0]\n",
    "plot_entropy_dist(copy.deepcopy(model_s),retain_loader,forget_loader,test_loader_full, 'SCRUB',ax[4])\n",
    "plot_entropy_dist(copy.deepcopy(model),retain_loader,forget_loader,test_loader_full, 'original', ax[0])\n",
    "plot_entropy_dist(copy.deepcopy(model0),retain_loader,forget_loader,test_loader_full, 'retrain', ax[1])\n",
    "plot_entropy_dist(copy.deepcopy(model_ft),retain_loader,forget_loader,test_loader_full, 'finetuned',ax[2])\n",
    "plot_entropy_dist(copy.deepcopy(modelf),retain_loader,forget_loader,test_loader_full, 'fisher',ax[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
