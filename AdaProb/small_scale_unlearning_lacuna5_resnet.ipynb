{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_343642/2175754929.py:24: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import variational\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from itertools import cycle\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init\n",
    "\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_solver(probs, target_distb, num_iter=10, th=0.1, num_newton=30):\n",
    "    # probs: N x K target_distb: 1*K\n",
    "    # entropy = (-1 * probs * torch.log(probs + 1e-6)).sum(1)\n",
    "    weights = (1 / entropy)\n",
    "    N, K = probs.size(0), probs.size(1)\n",
    "\n",
    "    A, w, lam, nu, r, c = probs.numpy(), weights.numpy(), np.ones(N), np.ones(K), np.ones(N), target_distb.numpy()\n",
    "    A_e = A / math.e\n",
    "    X = np.exp(-1 * lam / w)\n",
    "    Y = np.exp(-1 * nu.reshape(1, -1) / w.reshape(-1, 1))\n",
    "    prev_Y = np.zeros(K)\n",
    "    X_t, Y_t = X, Y\n",
    "\n",
    "    for n in range(num_iter):\n",
    "        # Normalization\n",
    "        denom = np.sum(A_e * Y_t, 1)\n",
    "        X_t = r / denom\n",
    "\n",
    "        # Newton method\n",
    "        Y_t = np.zeros(K)\n",
    "        for i in range(K):\n",
    "            Y_t[i] = optimize.newton(f, prev_Y[i], maxiter=num_newton, args=(A_e[:, i], X_t, w, c[i]), tol=th)\n",
    "        prev_Y = Y_t\n",
    "        Y_t = np.exp(-1 * Y_t.reshape(1, -1) / w.reshape(-1, 1))\n",
    "\n",
    "    denom = np.sum(A_e * Y_t, 1)\n",
    "    X_t = r / denom\n",
    "    M = torch.Tensor(A_e * X_t.reshape(-1, 1) * Y_t)\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: lacuna100_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in lacuna100_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 100\n",
      "[0] train metrics:{\"loss\": 3.8155148887634276, \"error\": 0.901875}\n",
      "Learning Rate : 0.1\n",
      "[0] dry_run metrics:{\"loss\": 3.30328556060791, \"error\": 0.82040625}\n",
      "Learning Rate : 0.1\n",
      "[0] test metrics:{\"loss\": 3.3426026260375976, \"error\": 0.8292}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 9.72 sec\n",
      "[1] train metrics:{\"loss\": 2.9475359210968017, \"error\": 0.73296875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.12 sec\n",
      "[2] train metrics:{\"loss\": 2.435648026943207, \"error\": 0.59603125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.2 sec\n",
      "[3] train metrics:{\"loss\": 2.094309967517853, \"error\": 0.48103125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.16 sec\n",
      "[4] train metrics:{\"loss\": 1.8684168701171875, \"error\": 0.39328125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.1 sec\n",
      "[5] train metrics:{\"loss\": 1.7369940962791444, \"error\": 0.3335625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.79 sec\n",
      "[6] train metrics:{\"loss\": 1.6375241208076476, \"error\": 0.27521875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.8 sec\n",
      "[7] train metrics:{\"loss\": 1.6008833703994751, \"error\": 0.2420625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.82 sec\n",
      "[8] train metrics:{\"loss\": 1.5656831736564636, \"error\": 0.2103125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.74 sec\n",
      "[9] train metrics:{\"loss\": 1.5484881372451782, \"error\": 0.18978125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.85 sec\n",
      "[10] train metrics:{\"loss\": 1.548932960987091, \"error\": 0.17840625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.97 sec\n",
      "[11] train metrics:{\"loss\": 1.5422889924049377, \"error\": 0.16584375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.85 sec\n",
      "[12] train metrics:{\"loss\": 1.4952515697479247, \"error\": 0.14746875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.81 sec\n",
      "[13] train metrics:{\"loss\": 1.5707669405937195, \"error\": 0.160125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.78 sec\n",
      "[14] train metrics:{\"loss\": 1.5387192511558532, \"error\": 0.1441875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.77 sec\n",
      "[15] train metrics:{\"loss\": 1.4913351111412048, \"error\": 0.12915625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.74 sec\n",
      "[16] train metrics:{\"loss\": 1.550043649673462, \"error\": 0.1435625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.98 sec\n",
      "[17] train metrics:{\"loss\": 1.5404470491409301, \"error\": 0.136375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.0 sec\n",
      "[18] train metrics:{\"loss\": 1.5013884897232055, \"error\": 0.126375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.09 sec\n",
      "[19] train metrics:{\"loss\": 1.5005063161849976, \"error\": 0.12503125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.99 sec\n",
      "[20] train metrics:{\"loss\": 1.548140968799591, \"error\": 0.13434375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.78 sec\n",
      "[21] train metrics:{\"loss\": 1.463663778781891, \"error\": 0.11421875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.75 sec\n",
      "[22] train metrics:{\"loss\": 1.543300733089447, \"error\": 0.13575}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.66 sec\n",
      "[23] train metrics:{\"loss\": 1.5017543921470642, \"error\": 0.1206875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.63 sec\n",
      "[24] train metrics:{\"loss\": 1.483858111858368, \"error\": 0.1175}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.06 sec\n",
      "[25] train metrics:{\"loss\": 1.513565122127533, \"error\": 0.1264375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.14 sec\n",
      "[26] train metrics:{\"loss\": 1.5126464715003967, \"error\": 0.12475}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.94 sec\n",
      "[27] train metrics:{\"loss\": 1.5156381821632385, \"error\": 0.1235}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.86 sec\n",
      "[28] train metrics:{\"loss\": 1.4932949562072755, \"error\": 0.117}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.85 sec\n",
      "[29] train metrics:{\"loss\": 1.5199323940277099, \"error\": 0.123}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.84 sec\n",
      "[30] train metrics:{\"loss\": 1.5113308339118958, \"error\": 0.120125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.86 sec\n",
      "Pure training time: 152.23000000000002 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset lacuna100 --dataroot=data/lacuna100 --model resnet --filters 0.4 --lr 0.1 --lossfn ce --num-classes 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_lacuna5_resnet_0_4_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in small_lacuna5_resnet_0_4_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 5\n",
      "[0] train metrics:{\"loss\": 1.8157579345703125, \"error\": 0.88000000166893}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.678695526123047, \"error\": 0.8399999998211861}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.21 sec\n",
      "[1] train metrics:{\"loss\": 1.532227437019348, \"error\": 0.7200000016689301}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[2] train metrics:{\"loss\": 1.1645071392059325, \"error\": 0.3820000023841858}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[3] train metrics:{\"loss\": 0.8515839290618896, \"error\": 0.14600000381469727}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[4] train metrics:{\"loss\": 0.6293069076538086, \"error\": 0.08200000143051148}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[5] train metrics:{\"loss\": 0.4754921355247498, \"error\": 0.05599999713897705}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[6] train metrics:{\"loss\": 0.3731359221935272, \"error\": 0.03999999713897705}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[7] train metrics:{\"loss\": 0.3020577273368835, \"error\": 0.030000001907348632}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[8] train metrics:{\"loss\": 0.2539944832324982, \"error\": 0.030000004291534425}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[9] train metrics:{\"loss\": 0.2218881673812866, \"error\": 0.02599999523162842}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[10] train metrics:{\"loss\": 0.19917951357364655, \"error\": 0.027999995231628418}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[11] train metrics:{\"loss\": 0.18326748359203338, \"error\": 0.026000004291534425}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[12] train metrics:{\"loss\": 0.1711251907348633, \"error\": 0.02}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[13] train metrics:{\"loss\": 0.1608934234380722, \"error\": 0.01799999761581421}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[14] train metrics:{\"loss\": 0.15264856874942778, \"error\": 0.01599999761581421}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[15] train metrics:{\"loss\": 0.14568501234054565, \"error\": 0.015999995231628417}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[16] train metrics:{\"loss\": 0.13985104739665985, \"error\": 0.016}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[17] train metrics:{\"loss\": 0.1349688467979431, \"error\": 0.016000006675720217}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[18] train metrics:{\"loss\": 0.13062449622154235, \"error\": 0.01599999761581421}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[19] train metrics:{\"loss\": 0.12668345111608506, \"error\": 0.016000006675720217}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[20] train metrics:{\"loss\": 0.12306421732902527, \"error\": 0.016000006675720217}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[21] train metrics:{\"loss\": 0.11998018693923951, \"error\": 0.016000006675720217}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[22] train metrics:{\"loss\": 0.11719018530845642, \"error\": 0.013999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[23] train metrics:{\"loss\": 0.11445067846775055, \"error\": 0.011999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[24] train metrics:{\"loss\": 0.1119829775094986, \"error\": 0.009999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[25] train metrics:{\"loss\": 0.10976870274543762, \"error\": 0.010000006675720215}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[26] train metrics:{\"loss\": 0.10782692813873292, \"error\": 0.009999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[27] train metrics:{\"loss\": 0.10583692252635955, \"error\": 0.009999995231628417}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[28] train metrics:{\"loss\": 0.10397299706935882, \"error\": 0.007999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[29] train metrics:{\"loss\": 0.10230067992210388, \"error\": 0.005999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[30] train metrics:{\"loss\": 0.10068872320652009, \"error\": 0.005999995231628418}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "Pure training time: 2.440000000000001 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset small_lacuna5 --model resnet --dataroot=data/lacuna10/ --filters 0.4 --lr 0.001 \\\n",
    "--resume checkpoints/lacuna100_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_lacuna5_resnet_0_4_forget_[0]_num_25_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in small_lacuna5_resnet_0_4_forget_[0]_num_25_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Replacing indexes [83 30 56 24 16 23  2 27 28 13 99 92 76 14  0 21  3 29 61 79 35 11 84 44\n",
      " 73]\n",
      "Number of Classes: 5\n",
      "[0] train metrics:{\"loss\": 1.8231099576950074, \"error\": 0.8819999992847443}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.6787854976654053, \"error\": 0.8380000004768372}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.51 sec\n",
      "[1] train metrics:{\"loss\": 1.535523126602173, \"error\": 0.7200000038146973}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[2] train metrics:{\"loss\": 1.1676772794723511, \"error\": 0.4}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[3] train metrics:{\"loss\": 0.8532320075035095, \"error\": 0.15400000381469728}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[4] train metrics:{\"loss\": 0.628086314201355, \"error\": 0.06999999713897705}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[5] train metrics:{\"loss\": 0.47236001086235047, \"error\": 0.04599999952316284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[6] train metrics:{\"loss\": 0.36891079783439634, \"error\": 0.03800000190734863}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[7] train metrics:{\"loss\": 0.2970020779371262, \"error\": 0.034000006675720215}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[8] train metrics:{\"loss\": 0.24895334672927857, \"error\": 0.026000006675720215}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[9] train metrics:{\"loss\": 0.21690951597690583, \"error\": 0.02599999523162842}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[10] train metrics:{\"loss\": 0.1943264113664627, \"error\": 0.027999995231628418}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[11] train metrics:{\"loss\": 0.17799036824703215, \"error\": 0.024000001907348634}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[12] train metrics:{\"loss\": 0.1660956896543503, \"error\": 0.02199999523162842}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[13] train metrics:{\"loss\": 0.15599180829524995, \"error\": 0.01999999761581421}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[14] train metrics:{\"loss\": 0.14807636761665344, \"error\": 0.016}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[15] train metrics:{\"loss\": 0.14129257929325104, \"error\": 0.015999995231628417}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[16] train metrics:{\"loss\": 0.13562139487266542, \"error\": 0.01599999761581421}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[17] train metrics:{\"loss\": 0.13101074111461639, \"error\": 0.016000006675720217}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[18] train metrics:{\"loss\": 0.12684856748580933, \"error\": 0.015999995231628417}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[19] train metrics:{\"loss\": 0.12315675872564316, \"error\": 0.016000006675720217}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[20] train metrics:{\"loss\": 0.11957632911205292, \"error\": 0.016000006675720217}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[21] train metrics:{\"loss\": 0.1166838915348053, \"error\": 0.015999995231628417}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[22] train metrics:{\"loss\": 0.11399599659442902, \"error\": 0.009999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[23] train metrics:{\"loss\": 0.11135616165399552, \"error\": 0.007999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[24] train metrics:{\"loss\": 0.10898221683502197, \"error\": 0.007999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[25] train metrics:{\"loss\": 0.10689608764648438, \"error\": 0.007999995231628417}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[26] train metrics:{\"loss\": 0.10503358495235443, \"error\": 0.007999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[27] train metrics:{\"loss\": 0.10313213658332825, \"error\": 0.005999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[28] train metrics:{\"loss\": 0.10138381671905518, \"error\": 0.006}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[29] train metrics:{\"loss\": 0.09980156290531159, \"error\": 0.006}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[30] train metrics:{\"loss\": 0.09825986158847809, \"error\": 0.005999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "Pure training time: 2.8100000000000005 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset small_lacuna5 --model resnet --dataroot=data/lacuna10/ --filters 0.4 --lr 0.001 \\\n",
    "--resume checkpoints/lacuna100_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 \\\n",
    "--forget-class 0 --num-to-forget 25 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 1787741\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model0 = copy.deepcopy(model)\n",
    "model_initial = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 300#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "model_initial.load_state_dict(torch.load(init_checkpoint))\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)\n",
    "\n",
    "model.cuda()\n",
    "model0.cuda()\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between w(D) and w(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(model,model0):\n",
    "    distance=0\n",
    "    normalization=0\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space='  ' if 'bias' in k else ''\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "        distance+=current_dist\n",
    "        normalization+=current_norm\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 0.07084530328417134\n",
      "Normalized Distance: 0.0008734134402055196\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Retrain']=distance(model,model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance of w(D) from initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntk_init(resume,seed=1):\n",
    "    manual_seed(seed)\n",
    "    model_init = models.get_model(arch, num_classes=num_classes, filters_percentage=filters).to(args.device)\n",
    "    model_init.load_state_dict(torch.load(resume))\n",
    "    return model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "for p in model_init.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 0.9623198989434337\n",
      "Normalized Distance: 0.011868091933688494\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Original_init']=distance(model_init,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.retain_bs = 32\n",
    "args.forget_bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confuse mode: False\n",
      "split mode: train\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [83 30 56 24 16 23  2 27 28 13 99 92 76 14  0 21  3 29 61 79 35 11 84 44\n",
      " 73]\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _ = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=args.forget_bs, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=args.retain_bs, seed=seed, shuffle=True)\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "475\n",
      "500\n",
      "500\n",
      "{0: 100, 1: 100, 2: 100, 3: 100, 4: 100}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'sgd'\n",
    "args.gamma = 1.0\n",
    "args.alpha = 0.5\n",
    "args.beta = 0.95\n",
    "args.smoothing = 0.5\n",
    "args.msteps = 5\n",
    "args.clip = 0.2\n",
    "args.sstart = 10\n",
    "args.kd_T = 2\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_epochs = 10\n",
    "args.sgda_learning_rate = 0.02\n",
    "args.lr_decay_epochs = [5,8,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 0.1#5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> scrub unlearning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihao/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 97.474 \n",
      "maximize loss: -3.08\t minimize loss: 9.75\t train_acc: 97.47367858886719\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 98.316 \n",
      "maximize loss: 19.48\t minimize loss: 38.27\t train_acc: 98.31578826904297\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.789 \n",
      "maximize loss: 46.07\t minimize loss: 65.35\t train_acc: 99.78946685791016\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.579 \n",
      "maximize loss: 67.42\t minimize loss: 85.44\t train_acc: 99.57894134521484\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.158 \n",
      "maximize loss: 81.68\t minimize loss: 99.16\t train_acc: 99.15789031982422\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.158 \n",
      "maximize loss: 0.00\t minimize loss: 104.47\t train_acc: 99.15789031982422\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.579 \n",
      "maximize loss: 0.00\t minimize loss: 104.98\t train_acc: 99.57894134521484\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: 0.00\t minimize loss: 105.56\t train_acc: 99.99999237060547\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: 0.00\t minimize loss: 105.82\t train_acc: 99.99999237060547\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: 0.00\t minimize loss: 105.85\t train_acc: 99.99999237060547\n"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> scrub unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True, prefix=\"train_lacuna5_resnet\")\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True, prefix=\"forget_lacuna5_resnet\")\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\",)\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('scrub retain- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42105865478515625, 18.947372436523438, 65.89473724365234, 73.26315879821777, 60.0, 31.578948974609375, 1.2631607055664062, 0.21053314208984375, 0.21053314208984375, 7.62939453125e-06]\n",
      "[4.0, 0.0, 44.0, 72.0, 16.0, 24.0, 24.0, 28.0, 28.0, 28.0]\n"
     ]
    }
   ],
   "source": [
    "print (acc_rs)\n",
    "print (acc_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTK based Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NTK Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_w_utils(model_init,dataloader,name='complete'):\n",
    "    model_init.eval()\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    G_list = []\n",
    "    f0_minus_y = []\n",
    "    for idx, batch in enumerate(dataloader):#(tqdm(dataloader,leave=False)):\n",
    "        batch = [tensor.to(next(model_init.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        if 'mnist' in args.dataset:\n",
    "            input = input.view(input.shape[0],-1)\n",
    "        target = target.cpu().detach().numpy()\n",
    "        output = model_init(input)\n",
    "        G_sample=[]\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output[0,cls],model_init.parameters(),retain_graph=True)\n",
    "            grads = np.concatenate([g.view(-1).cpu().numpy() for g in grads])\n",
    "            G_sample.append(grads)\n",
    "            G_list.append(grads)\n",
    "        if args.lossfn=='mse':\n",
    "            p = output.cpu().detach().numpy().transpose()\n",
    "            #loss_hess = np.eye(len(p))\n",
    "            target = 2*target-1\n",
    "            f0_y_update = p-target\n",
    "        elif args.lossfn=='ce':\n",
    "            p = torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().transpose()\n",
    "            p[target]-=1\n",
    "            f0_y_update = copy.deepcopy(p)\n",
    "        f0_minus_y.append(f0_y_update)\n",
    "    return np.stack(G_list).transpose(),np.vstack(f0_minus_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobians and Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NTK_data/G_r.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zihao/data/SCRUB-main/small_scale_unlearning.ipynb Cell 44\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zihao/data/SCRUB-main/small_scale_unlearning.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m t2 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zihao/data/SCRUB-main/small_scale_unlearning.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ntk_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m t2\u001b[39m-\u001b[39mt1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zihao/data/SCRUB-main/small_scale_unlearning.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m np\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39mNTK_data/G_r.npy\u001b[39;49m\u001b[39m'\u001b[39;49m,G_r)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zihao/data/SCRUB-main/small_scale_unlearning.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mNTK_data/f0_minus_y_r.npy\u001b[39m\u001b[39m'\u001b[39m,f0_minus_y_r)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zihao/data/SCRUB-main/small_scale_unlearning.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdel\u001b[39;00m G_r, f0_minus_y_r\n",
      "File \u001b[0;32m~/anaconda3/envs/zihao/lib/python3.10/site-packages/numpy/lib/npyio.py:542\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m file\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    541\u001b[0m         file \u001b[39m=\u001b[39m file \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 542\u001b[0m     file_ctx \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(file, \u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    544\u001b[0m \u001b[39mwith\u001b[39;00m file_ctx \u001b[39mas\u001b[39;00m fid:\n\u001b[1;32m    545\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(arr)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NTK_data/G_r.npy'"
     ]
    }
   ],
   "source": [
    "ntk_time = 0\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_r,f0_minus_y_r = delta_w_utils(copy.deepcopy(model),retain_loader,'complete')\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_r.npy',G_r)\n",
    "np.save('NTK_data/f0_minus_y_r.npy',f0_minus_y_r)\n",
    "del G_r, f0_minus_y_r\n",
    "\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_f,f0_minus_y_f = delta_w_utils(copy.deepcopy(model),forget_loader,'retain') \n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_f.npy',G_f)\n",
    "np.save('NTK_data/f0_minus_y_f.npy',f0_minus_y_f)\n",
    "del G_f, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "G_f = np.load('NTK_data/G_f.npy')\n",
    "G = np.concatenate([G_r,G_f],axis=1)\n",
    "\n",
    "np.save('NTK_data/G.npy',G)\n",
    "del G, G_f, G_r\n",
    "\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "f0_minus_y_f = np.load('NTK_data/f0_minus_y_f.npy')\n",
    "f0_minus_y = np.concatenate([f0_minus_y_r,f0_minus_y_f])\n",
    "\n",
    "np.save('NTK_data/f0_minus_y.npy',f0_minus_y)\n",
    "del f0_minus_y, f0_minus_y_r, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only requires access to the gradients and the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.load('NTK_data/G.npy')\n",
    "t1 = time.time()\n",
    "theta = G.transpose().dot(G) + num_total*args.weight_decay*np.eye(G.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G\n",
    "\n",
    "t1 = time.time()\n",
    "theta_inv = np.linalg.inv(theta)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta.npy',theta)\n",
    "del theta\n",
    "\n",
    "G = np.load('NTK_data/G.npy')\n",
    "f0_minus_y = np.load('NTK_data/f0_minus_y.npy')\n",
    "t1 = time.time()\n",
    "w_complete = -G.dot(theta_inv.dot(f0_minus_y))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_inv.npy',theta_inv)\n",
    "np.save('NTK_data/w_complete.npy',w_complete)\n",
    "\n",
    "del G, f0_minus_y, theta_inv, w_complete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "t1 = time.time()\n",
    "theta_r = G_r.transpose().dot(G_r) + num_to_retain*args.weight_decay*np.eye(G_r.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G_r\n",
    "\n",
    "t1 = time.time()\n",
    "theta_r_inv = np.linalg.inv(theta_r)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r.npy',theta_r)\n",
    "del theta_r\n",
    "\n",
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "t1 = time.time()\n",
    "w_retain = -G_r.dot(theta_r_inv.dot(f0_minus_y_r))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r_inv.npy',theta_r_inv)\n",
    "np.save('NTK_data/w_retain.npy',w_retain)\n",
    "\n",
    "del G_r, f0_minus_y_r, theta_r_inv, w_retain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrubbing Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Scrubbing Direction\n",
    "w_complete = np.load('NTK_data/w_complete.npy')\n",
    "w_retain = np.load('NTK_data/w_retain.npy')\n",
    "delta_w = (w_retain-w_complete).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_copy = copy.deepcopy(delta_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Change in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_actual = vectorize_params(model0)-vectorize_params(model)\n",
    "\n",
    "print(f'Actual Norm-: {np.linalg.norm(delta_w_actual)}')\n",
    "print(f'Predtn Norm-: {np.linalg.norm(delta_w)}')\n",
    "scale_ratio = np.linalg.norm(delta_w_actual)/np.linalg.norm(delta_w)\n",
    "print('Actual Scale: {}'.format(scale_ratio))\n",
    "log_dict['actual_scale_ratio']=scale_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trapezium Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred_error = vectorize_params(model)-vectorize_params(model_init)-w_retain.squeeze()\n",
    "print(f\"Delta w -------: {np.linalg.norm(delta_w)}\")\n",
    "\n",
    "inner = np.inner(delta_w/np.linalg.norm(delta_w),m_pred_error/np.linalg.norm(m_pred_error))\n",
    "print(f\"Inner Product--: {inner}\")\n",
    "\n",
    "if inner<0:\n",
    "    angle = np.arccos(inner)-np.pi/2\n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.sin(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "else:\n",
    "    angle = np.arccos(inner) \n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.cos(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "\n",
    "predicted_scale=predicted_norm/np.linalg.norm(delta_w)\n",
    "predicted_scale\n",
    "print(f\"Predicted Scale:  {predicted_scale}\")\n",
    "log_dict['predicted_scale_ratio']=predicted_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Inner Product between Prediction and Actual Scrubbing Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIP(v1,v2):\n",
    "    nip = (np.inner(v1/np.linalg.norm(v1),v2/np.linalg.norm(v2)))\n",
    "    print(nip)\n",
    "    return nip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nip=NIP(delta_w_actual,delta_w)\n",
    "log_dict['nip']=nip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_w_dict(delta_w,model):\n",
    "    # Give normalized delta_w\n",
    "    delta_w_dict = OrderedDict()\n",
    "    params_visited = 0\n",
    "    for k,p in model.named_parameters():\n",
    "        num_params = np.prod(list(p.shape))\n",
    "        update_params = delta_w[params_visited:params_visited+num_params]\n",
    "        delta_w_dict[k] = torch.Tensor(update_params).view_as(p)\n",
    "        params_visited+=num_params\n",
    "    return delta_w_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics,activations,predictions=get_metrics(model,dataloader,criterion,True)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "\n",
    "    return activations,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub using NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=predicted_scale\n",
    "direction = get_delta_w_dict(delta_w,model)\n",
    "\n",
    "model_scrub = copy.deepcopy(model)\n",
    "for k,p in model_scrub.named_parameters():\n",
    "    p.data += (direction[k]*scale).to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune and Fisher Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg\n",
    "\n",
    "def run_neggrad_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    forget_loader: torch.utils.data.DataLoader,\n",
    "                    alpha: float,\n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    quiet=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, (batch_retain,batch_forget) in enumerate(tqdm(zip(data_loader,cycle(forget_loader)), leave=False)):\n",
    "            batch_retain = [tensor.to(next(model.parameters()).device) for tensor in batch_retain]\n",
    "            batch_forget = [tensor.to(next(model.parameters()).device) for tensor in batch_forget]\n",
    "            input_r, target_r = batch_retain\n",
    "            input_f, target_f = batch_forget\n",
    "            output_r = model(input_r)\n",
    "            output_f = model(input_f)\n",
    "            loss = alpha*(loss_fn(output_r, target_r) + l2_penalty(model,model_init,args.weight_decay)) - (1-alpha)*loss_fn(output_f, target_f)\n",
    "            metrics.update(n=input_r.size(0), loss=loss_fn(output_r,target_r).item(), error=get_error(output_r, target_r))\n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def negative_grad(model: nn.Module, data_loader: torch.utils.data.DataLoader, forget_loader: torch.utils.data.DataLoader, alpha: float, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_neggrad_epoch(model, model_init, data_loader, forget_loader, alpha, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "\n",
    "def fk_fientune(model: nn.Module, data_loader: torch.utils.data.DataLoader, args, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.1, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computes the amount of information not forgotten at all layers using the given alpha\n",
    "alpha = 1e-7\n",
    "total_kl = 0\n",
    "torch.manual_seed(seed)\n",
    "for (k, p), (k0, p0) in zip(modelf.named_parameters(), modelf0.named_parameters()):\n",
    "    mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "    mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "    kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "    total_kl += kl\n",
    "    print(k, f'{kl:.1f}')\n",
    "print(\"Total:\", total_kl)\n",
    "log_dict['fisher_info']=total_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-6\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())\n",
    "\n",
    "for i, p in enumerate(modelf0.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test(modelf, retain_loader))\n",
    "print(test(modelf, forget_loader))\n",
    "print(test(modelf, valid_loader_full))\n",
    "print(test(modelf, test_loader_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.ng_alpha = 0.95\n",
    "args.ng_epochs = 10\n",
    "args.ng_lr = 0.01\n",
    "model_ng = copy.deepcopy(model)    \n",
    "negative_grad(model_ng, retain_loader, forget_loader, alpha=args.ng_alpha, epochs=args.ng_epochs, quiet=True, lr=args.ng_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catastrophic Forgetting k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.cfk_lr = 0.01\n",
    "args.cfk_epochs = 10\n",
    "\n",
    "model_cfk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_cfk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    layers = [9]\n",
    "    for k in layers:\n",
    "        for param in model_cfk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    for param in model_cfk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "fk_fientune(model_cfk, retain_loader, args=args, epochs=args.cfk_epochs, quiet=True, lr=args.cfk_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Unlearning k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The last block and classifier of resnet-18\n",
    "(layer4): Sequential(\n",
    "    (0): _ResBlock(\n",
    "      (bn1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(102, 204, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (shortcut): Sequential(\n",
    "        (0): Conv2d(102, 204, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "      )\n",
    "    )\n",
    "    (1): _ResBlock(\n",
    "      (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    )\n",
    "  )\n",
    "(linear): Linear(in_features=204, out_features=5, bias=True)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" The last block and classifier of allcnn\n",
    "AllCNN(\n",
    "  (features): Sequential(\n",
    "    ...\n",
    "    (9): Conv(\n",
    "      (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (2): ReLU()\n",
    "    )\n",
    "    (10): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
    "    (11): Flatten()\n",
    "  )\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=192, out_features=5, bias=True)\n",
    "  )\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.euk_lr = 0.01\n",
    "args.euk_epochs = training_epochs\n",
    "model_euk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_euk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    with torch.no_grad():\n",
    "        for k in layers:\n",
    "            for i in range(0,3):\n",
    "                try:\n",
    "                    model_euk.features[k][i].weight.copy_(model_initial.features[k][i].weight)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have weights\".format(k,i))\n",
    "                try:\n",
    "                    model_euk.features[k][i].bias.copy_(model_initial.features[k][i].bias)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have bias\".format(k,i))\n",
    "        model_euk.classifier[0].weight.copy_(model_initial.classifier[0].weight)\n",
    "        model_euk.classifier[0].bias.copy_(model_initial.classifier[0].bias)\n",
    "    \n",
    "    for k in layers:\n",
    "        for param in model_euk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,2):\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.weight.copy_(model_initial.layer4[i].bn1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.bias.copy_(model_initial.layer4[i].bn1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.weight.copy_(model_initial.layer4[i].conv1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.bias.copy_(model_initial.layer4[i].conv1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.weight.copy_(model_initial.layer4[i].bn2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.bias.copy_(model_initial.layer4[i].bn2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.weight.copy_(model_initial.layer4[i].conv2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.bias.copy_(model_initial.layer4[i].conv2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "        model_euk.layer4[0].shortcut[0].weight.copy_(model_initial.layer4[0].shortcut[0].weight)\n",
    "        \n",
    "    for param in model_euk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "fk_fientune(model_euk, retain_loader, epochs=args.euk_epochs, quiet=True, lr=args.euk_lr, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "_,_=activations_predictions(copy.deepcopy(model),forget_loader,'Original_Model_D_f')\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"c\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_ng),thresh,'NegGrad')\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model_cfk),thresh,'CF-k')\n",
    "readouts[\"f\"] = all_readouts(copy.deepcopy(model_euk),thresh,'EU-k')\n",
    "readouts[\"g\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"h\"] = all_readouts(copy.deepcopy(model_scrub),thresh,'NTK')\n",
    "readouts[\"i\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
