{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393182/3351334044.py:20: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import variational\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activations along the path\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datasets\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def entropy(p, dim = -1, keepdim = False):\n",
    "    return -torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim=dim, keepdim=keepdim)\n",
    "    #return (p * p.log()).sum(dim=dim, keepdim=keepdim)\n",
    "\n",
    "def collect_prob(data_loader, model):\n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=False, num_samples=4000)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, sampler=sampler, shuffle=False)\n",
    "    prob = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(data_loader_small, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            data, target = batch\n",
    "            output = model(data)\n",
    "            prob.append(F.softmax(output, dim=-1).data)\n",
    "    return torch.cat(prob)\n",
    "\n",
    "def get_membership_attack_data(retain_loader, forget_loader, test_loader, model):    \n",
    "    retain_prob = collect_prob(retain_loader, model)\n",
    "    forget_prob = collect_prob(forget_loader, model)\n",
    "    test_prob = collect_prob(test_loader, model)\n",
    "    \n",
    "    X_r = torch.cat([entropy(retain_prob), entropy(test_prob)]).cpu().numpy().reshape(-1, 1)\n",
    "    Y_r = np.concatenate([np.ones(len(retain_prob)), np.zeros(len(test_prob))])\n",
    "    \n",
    "    X_f = entropy(forget_prob).cpu().numpy().reshape(-1, 1)\n",
    "    Y_f = np.concatenate([np.ones(len(forget_prob))])    \n",
    "    return X_f, Y_f, X_r, Y_r\n",
    "\n",
    "def get_membership_attack_prob(retain_loader, forget_loader, test_loader, model):\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader, forget_loader, test_loader, model)\n",
    "    #clf = SVC(C=3,gamma='auto',kernel='rbf')\n",
    "    clf = LogisticRegression(class_weight='balanced',solver='lbfgs',multi_class='multinomial')\n",
    "    clf.fit(X_r, Y_r)\n",
    "    results = clf.predict(X_f)\n",
    "    results1 = clf.predict(X_r)\n",
    "    acc = accuracy_score(results, Y_f)\n",
    "    train_ac = accuracy_score(results1, Y_r)\n",
    "    TP, FP, TN, FN = perf_measure(Y_r, results1)\n",
    "    FPR = FP/(FP+TN)\n",
    "    FNR = FN/(FN+TP)\n",
    "    \n",
    "    print (f\"TP:{TP}, FP{FP}, TN{TN}, FN{FN}\")\n",
    "    print (f\"false negative rate: {FN/(FN+TP)}\")\n",
    "    print (f\"false positive rate: {FP/(FP+TN)}\")\n",
    "    return acc, train_acc, FPR, FNR #results.mean(), results1.mean()\n",
    "    \n",
    "def plot_entropy_dist(model,retain_loader,forget_loader,test_loader_full, title, ax):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from matplotlib.ticker import FuncFormatter\n",
    "    #train_loader_full, valid_loader_full, test_loader_full = datasets.get_loaders(dataset, batch_size=100, seed=0, augment=False, shuffle=False)\n",
    "    #indexes = np.flatnonzero(np.array(train_loader_full.dataset.targets) == class_to_forget)\n",
    "    #replaced = np.random.RandomState(0).choice(indexes, size=100 if num_to_forget==100 else len(indexes), replace=False)\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader,forget_loader,test_loader_full, model)\n",
    "    #np.savetxt('retain_prob.txt', X_r[Y_r==1])\n",
    "    #np.savetxt('forget_prob.txt', X_f)\n",
    "    #np.savetxt('test_prob.txt', X_r[Y_r==0])\n",
    "    sns.distplot(np.log(X_r[Y_r==1]).reshape(-1), kde=False, norm_hist=True, rug=False, label='retain', ax=plt)\n",
    "    sns.distplot(np.log(X_r[Y_r==0]).reshape(-1), kde=False, norm_hist=True, rug=False, label='test', ax=plt)\n",
    "    sns.distplot(np.log(X_f).reshape(-1), kde=False, norm_hist=True, rug=False, label='forget', ax=plt)\n",
    "    plt.legend(prop={'size': 14})\n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.title(title,size=18)\n",
    "    plt.xlabel('Log of Entropy',size=14)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def membership_attack(retain_loader,forget_loader,test_loader,model, name):\n",
    "    prob, train_acc, FPR, FNR = get_membership_attack_prob(retain_loader,forget_loader,test_loader,model)\n",
    "    print(\"Attack prob: \", prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=512, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=512, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics=get_metrics(model,dataloader,criterion,False)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "    return _,_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        #run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.01, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    #print(f\"{name} ->\"\n",
    "    #      f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_activations(model_scrubf, modelf0, delta_w_s, delta_w_m0, data_loader, \\\n",
    "                    loss_fn=nn.CrossEntropyLoss(),\\\n",
    "                    optimizer=torch.optim.SGD, \\\n",
    "                    seed=1,quiet=False):\n",
    "\n",
    "    model_scrubf.eval()\n",
    "    modelf0.eval()\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    \n",
    "    metrics = AverageMeter()    \n",
    "    num_classes = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "        batch = [tensor.to(next(model_scrubf.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        \n",
    "        output_sf = model_scrubf(input)\n",
    "        G_sf = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=True)\n",
    "            grads = torch.cat([g.view(-1) for g in grads])\n",
    "            G_sf.append(grads)\n",
    "\n",
    "        grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_sf = torch.stack(G_sf)#.pow(2)\n",
    "        delta_f_sf_update = torch.matmul(G_sf,delta_w_s.sqrt()*torch.empty_like(delta_w_s).normal_())\n",
    "        G_sf = G_sf.pow(2)\n",
    "        delta_f_sf = torch.matmul(G_sf,delta_w_s)\n",
    "\n",
    "        output_m0 = modelf0(input)\n",
    "        G_m0 = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=True)\n",
    "            grad_m0 = torch.cat([g.view(-1) for g in grads])\n",
    "            G_m0.append(grad_m0)\n",
    "\n",
    "        grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_m0 = torch.stack(G_m0).pow(2)\n",
    "        delta_f_m0 = torch.matmul(G_m0,delta_w_m0)\n",
    "        \n",
    "        kl = ((output_m0 - output_sf).pow(2)/delta_f_m0 + delta_f_sf/delta_f_m0 - torch.log(delta_f_sf/delta_f_m0) - 1).sum()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        output_sf += delta_f_sf_update#delta_f_sf.sqrt()*torch.empty_like(delta_f_sf).normal_()\n",
    "        \n",
    "        loss = loss_fn(output_sf, target)\n",
    "        metrics.update(n=input.size(0), loss=loss.item(), error=get_error(output_sf, target), kl=kl.item())\n",
    "    \n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: lacuna100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in lacuna100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 100\n",
      "[0] train metrics:{\"loss\": 3.8233655672073366, \"error\": 0.91084375}\n",
      "Learning Rate : 0.1\n",
      "[0] dry_run metrics:{\"loss\": 3.3233438425064086, \"error\": 0.8385625}\n",
      "Learning Rate : 0.1\n",
      "[0] test metrics:{\"loss\": 3.3385062034606934, \"error\": 0.8347}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 8.81 sec\n",
      "[1] train metrics:{\"loss\": 3.040908597946167, \"error\": 0.77615625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.24 sec\n",
      "[2] train metrics:{\"loss\": 2.4334757966995237, \"error\": 0.61228125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.24 sec\n",
      "[3] train metrics:{\"loss\": 1.9808078465461731, \"error\": 0.47328125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.29 sec\n",
      "[4] train metrics:{\"loss\": 1.7013714575767518, \"error\": 0.372}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.23 sec\n",
      "[5] train metrics:{\"loss\": 1.4868793172836303, \"error\": 0.29221875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.18 sec\n",
      "[6] train metrics:{\"loss\": 1.3205368399620057, \"error\": 0.22421875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.21 sec\n",
      "[7] train metrics:{\"loss\": 1.2272959723472596, \"error\": 0.18584375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.29 sec\n",
      "[8] train metrics:{\"loss\": 1.133659630537033, \"error\": 0.1455625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.08 sec\n",
      "[9] train metrics:{\"loss\": 1.058427065372467, \"error\": 0.1150625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.14 sec\n",
      "[10] train metrics:{\"loss\": 1.0026730167865754, \"error\": 0.09853125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.21 sec\n",
      "[11] train metrics:{\"loss\": 0.9732379763126373, \"error\": 0.0879375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.11 sec\n",
      "[12] train metrics:{\"loss\": 0.9377931756973267, \"error\": 0.078125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.09 sec\n",
      "[13] train metrics:{\"loss\": 0.9080658020973206, \"error\": 0.07284375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.13 sec\n",
      "[14] train metrics:{\"loss\": 0.9161779451370239, \"error\": 0.07359375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.15 sec\n",
      "[15] train metrics:{\"loss\": 0.8948067147731781, \"error\": 0.06978125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.08 sec\n",
      "[16] train metrics:{\"loss\": 0.9115999133586884, \"error\": 0.07090625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.08 sec\n",
      "[17] train metrics:{\"loss\": 0.8692510938644409, \"error\": 0.06215625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.1 sec\n",
      "[18] train metrics:{\"loss\": 0.850548889875412, \"error\": 0.05765625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.12 sec\n",
      "[19] train metrics:{\"loss\": 0.8475376484394074, \"error\": 0.0595625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.05 sec\n",
      "[20] train metrics:{\"loss\": 0.8996174812316895, \"error\": 0.0716875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.11 sec\n",
      "[21] train metrics:{\"loss\": 0.9307970433235169, \"error\": 0.07428125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.02 sec\n",
      "[22] train metrics:{\"loss\": 0.8617413666248321, \"error\": 0.053}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.11 sec\n",
      "[23] train metrics:{\"loss\": 0.7489767985343934, \"error\": 0.03721875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.22 sec\n",
      "[24] train metrics:{\"loss\": 0.6187533376216888, \"error\": 0.0264375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.25 sec\n",
      "[25] train metrics:{\"loss\": 0.5147787270545959, \"error\": 0.02403125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.13 sec\n",
      "[26] train metrics:{\"loss\": 0.4404430638551712, \"error\": 0.02259375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.96 sec\n",
      "[27] train metrics:{\"loss\": 0.5160071790218353, \"error\": 0.0500625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.76 sec\n",
      "[28] train metrics:{\"loss\": 1.382216766834259, \"error\": 0.2360625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.9 sec\n",
      "[29] train metrics:{\"loss\": 1.0175488970279694, \"error\": 0.0950625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.89 sec\n",
      "[30] train metrics:{\"loss\": 0.8608677124977112, \"error\": 0.05125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.01 sec\n",
      "Pure training time: 127.77 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset lacuna100 --dataroot=data/lacuna100 --model allcnn --filters 1.0 --lr 0.1 --lossfn ce --num-classes 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: lacuna10_allcnn_1_0_forget_None_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in lacuna10_allcnn_1_0_forget_None_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Number of Classes: 10\n",
      "Epoch: [0][0/25]\tTime 0.035 (0.035)\tData 0.007 (0.007)\tLoss 1.7051 (1.7051)\tAcc@1 57.031 (57.031)\tAcc@5 86.719 (86.719)\n",
      " * Acc@1 86.750 Acc@5 97.969\n",
      "[0] test metrics:{\"loss\": 0.2643024432659149, \"error\": 0.050999993324279784}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.47 sec\n",
      "Epoch: [1][0/25]\tTime 0.013 (0.013)\tData 0.008 (0.008)\tLoss 0.2843 (0.2843)\tAcc@1 94.531 (94.531)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 95.562 Acc@5 99.781\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [2][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.1189 (0.1189)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 97.906 Acc@5 99.906\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [3][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0765 (0.0765)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.031 Acc@5 99.906\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [4][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0227 (0.0227)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.531 Acc@5 99.906\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [5][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0466 (0.0466)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.812 Acc@5 100.000\n",
      "[5] test metrics:{\"loss\": 0.08815042459964752, \"error\": 0.019999999046325684}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.43 sec\n",
      "Epoch: [6][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0434 (0.0434)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.969 Acc@5 100.000\n",
      "Epoch Time: 0.32 sec\n",
      "Epoch: [7][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0221 (0.0221)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.32 sec\n",
      "Epoch: [8][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0285 (0.0285)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [9][0/25]\tTime 0.013 (0.013)\tData 0.008 (0.008)\tLoss 0.0129 (0.0129)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.35 sec\n",
      "Epoch: [10][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0126 (0.0126)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[10] test metrics:{\"loss\": 0.0742943023443222, \"error\": 0.018999999046325683}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.43 sec\n",
      "Epoch: [11][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0126 (0.0126)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.32 sec\n",
      "Epoch: [12][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0098 (0.0098)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [13][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0068 (0.0068)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.34 sec\n",
      "Epoch: [14][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0084 (0.0084)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.34 sec\n",
      "Epoch: [15][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0055 (0.0055)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[15] test metrics:{\"loss\": 0.07153536677360535, \"error\": 0.018999999046325683}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.43 sec\n",
      "Epoch: [16][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0069 (0.0069)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [17][0/25]\tTime 0.014 (0.014)\tData 0.008 (0.008)\tLoss 0.0046 (0.0046)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.34 sec\n",
      "Epoch: [18][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0055 (0.0055)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.34 sec\n",
      "Epoch: [19][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0058 (0.0058)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.34 sec\n",
      "Epoch: [20][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0051 (0.0051)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[20] test metrics:{\"loss\": 0.07155667555332183, \"error\": 0.018999999046325683}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.44 sec\n",
      "Epoch: [21][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0072 (0.0072)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.35 sec\n",
      "Epoch: [22][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0040 (0.0040)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.35 sec\n",
      "Epoch: [23][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0038 (0.0038)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [24][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0050 (0.0050)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [25][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0041 (0.0041)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[25] test metrics:{\"loss\": 0.07072802913188934, \"error\": 0.018999999046325683}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.43 sec\n",
      "Pure training time: 8.679999999999998 sec\n"
     ]
    }
   ],
   "source": [
    "%run main_merged.py --dataset lacuna10 --model allcnn --dataroot=data/lacuna10/ --filters 1.0 --lr 0.01 \\\n",
    "--resume checkpoints/lacuna100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 5e-4 --batch-size 128 --epochs 26 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: lacuna10_allcnn_1_0_forget_[5]_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in lacuna10_allcnn_1_0_forget_[5]_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Number of Classes: 10\n",
      "Epoch: [0][0/25]\tTime 0.015 (0.015)\tData 0.008 (0.008)\tLoss 1.6959 (1.6959)\tAcc@1 58.594 (58.594)\tAcc@5 86.719 (86.719)\n",
      " * Acc@1 87.625 Acc@5 98.344\n",
      "[0] test metrics:{\"loss\": 0.21872721067733236, \"error\": 0.03777777777777778}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.45 sec\n",
      "Epoch: [1][0/25]\tTime 0.014 (0.014)\tData 0.008 (0.008)\tLoss 0.2535 (0.2535)\tAcc@1 94.531 (94.531)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 96.188 Acc@5 99.844\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [2][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.1130 (0.1130)\tAcc@1 96.875 (96.875)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 98.219 Acc@5 99.906\n",
      "Epoch Time: 0.34 sec\n",
      "Epoch: [3][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0651 (0.0651)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.281 Acc@5 99.906\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [4][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0248 (0.0248)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.594 Acc@5 99.938\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [5][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0409 (0.0409)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.844 Acc@5 100.000\n",
      "[5] test metrics:{\"loss\": 0.06958604514598847, \"error\": 0.013333333333333334}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.42 sec\n",
      "Epoch: [6][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0351 (0.0351)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.969 Acc@5 100.000\n",
      "Epoch Time: 0.32 sec\n",
      "Epoch: [7][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0185 (0.0185)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [8][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0222 (0.0222)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.34 sec\n",
      "Epoch: [9][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0115 (0.0115)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [10][0/25]\tTime 0.014 (0.014)\tData 0.008 (0.008)\tLoss 0.0107 (0.0107)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[10] test metrics:{\"loss\": 0.05857573831246959, \"error\": 0.014444444444444444}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.44 sec\n",
      "Epoch: [11][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0100 (0.0100)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [12][0/25]\tTime 0.013 (0.013)\tData 0.008 (0.008)\tLoss 0.0080 (0.0080)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [13][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0060 (0.0060)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [14][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0075 (0.0075)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [15][0/25]\tTime 0.015 (0.015)\tData 0.008 (0.008)\tLoss 0.0059 (0.0059)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[15] test metrics:{\"loss\": 0.05612410174475776, \"error\": 0.014444444444444444}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.42 sec\n",
      "Epoch: [16][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0057 (0.0057)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [17][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0034 (0.0034)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [18][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0048 (0.0048)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [19][0/25]\tTime 0.013 (0.013)\tData 0.006 (0.006)\tLoss 0.0046 (0.0046)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [20][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0037 (0.0037)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[20] test metrics:{\"loss\": 0.05446753301554256, \"error\": 0.014444444444444444}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.42 sec\n",
      "Epoch: [21][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0057 (0.0057)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.34 sec\n",
      "Epoch: [22][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0036 (0.0036)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.32 sec\n",
      "Epoch: [23][0/25]\tTime 0.015 (0.015)\tData 0.008 (0.008)\tLoss 0.0031 (0.0031)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [24][0/25]\tTime 0.013 (0.013)\tData 0.006 (0.006)\tLoss 0.0050 (0.0050)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [25][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0039 (0.0039)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[25] test metrics:{\"loss\": 0.054556139496465525, \"error\": 0.015555555555555555}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.43 sec\n",
      "Pure training time: 8.6 sec\n"
     ]
    }
   ],
   "source": [
    "%run main_merged.py --dataset lacuna10 --model allcnn --dataroot=data/lacuna10/ --filters 1 --lr 0.01 \\\n",
    "--resume checkpoints/lacuna100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 5e-4 --batch-size 128 --epochs 26 \\\n",
    "--forget-class 5 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 1620010\n"
     ]
    }
   ],
   "source": [
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model0 = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 4000#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model0.cuda()\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confuse mode: False\n",
      "split mode: train\n",
      "confuse mode: False\n",
      "split mode: train\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _  = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=512, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=256, seed=seed, shuffle=True)\n",
    "\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "2880\n",
      "1000\n",
      "3200\n",
      "{0: 320, 1: 320, 2: 320, 3: 320, 4: 320, 5: 320, 6: 320, 7: 320, 8: 320, 9: 320}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'sgd'\n",
    "args.gamma = 0.9999\n",
    "args.alpha = 0.001\n",
    "args.beta = 0.9\n",
    "args.smoothing = 0.0\n",
    "args.msteps = 4\n",
    "args.clip = 0.2\n",
    "args.sstart = 4\n",
    "args.kd_T = 4\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_batch_size = 32\n",
    "args.del_batch_size = 32\n",
    "args.sgda_epochs = 4\n",
    "args.sgda_learning_rate = 0.003\n",
    "args.lr_decay_epochs = [3,5,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> SCRUB unlearning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihao/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 100.000 \n",
      "maximize loss: -12.98\t minimize loss: 0.00\t train_acc: 100.0\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: -24.37\t minimize loss: 0.00\t train_acc: 100.0\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: -41.58\t minimize loss: 0.01\t train_acc: 100.0\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: -61.95\t minimize loss: 0.01\t train_acc: 100.0\n"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "acc_vs = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> SCRUB unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True, prefix=\"train_lacuna10_allcnn\")\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True, prefix=\"forget_lacuna10_allcnn\")\n",
    "    acc_v, acc5_v, loss_v = validate(valid_loader_full, model_s, criterion_cls, args, True)\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "    acc_vs.append(100-acc_v.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\")\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    \n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n",
    "acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "acc_v, acc5_v, loss_v = validate(valid_loader_full, model_s, criterion_cls, args, True)\n",
    "acc_rs.append(100-acc_r.item())\n",
    "acc_fs.append(100-acc_f.item())\n",
    "acc_vs.append(100-acc_v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.plot(indices, acc_vs, marker='.', alpha=1, label='valid-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('SCRUB retain-, valid- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Model_D_r -> Loss:0.003, Error:0.0\n",
      "Original_Model_D_f -> Loss:0.003, Error:0.0\n",
      "Original_Model_D_t -> Loss:0.066, Error:0.019\n"
     ]
    }
   ],
   "source": [
    "m_D_r_activations,m_D_r_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(retain_loader),'Original_Model_D_r')\n",
    "m_D_f_activations,m_D_f_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(forget_loader),'Original_Model_D_f')\n",
    "m_D_t_activations,m_D_t_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(test_loader_full),'Original_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain_Model_D_r -> Loss:0.003, Error:0.0\n",
      "Retrain_Model_D_f -> Loss:9.442, Error:1.0\n",
      "Retrain_Model_D_t -> Loss:0.977, Error:0.114\n"
     ]
    }
   ],
   "source": [
    "m0_D_r_activations,m0_D_r_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(retain_loader),'Retrain_Model_D_r')\n",
    "m0_D_f_activations,m0_D_f_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(forget_loader),'Retrain_Model_D_f')\n",
    "m0_D_t_activations,m0_D_t_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(test_loader_full),'Retrain_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRUB_D_r -> Loss:0.004, Error:0.0\n",
      "SCRUB_D_f -> Loss:12.647, Error:1.0\n",
      "SCRUB_D_t -> Loss:1.421, Error:0.116\n"
     ]
    }
   ],
   "source": [
    "ntk_D_r_activations,ntk_D_r_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(retain_loader),'SCRUB_D_r')\n",
    "ntk_D_f_activations,ntk_D_f_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(forget_loader),'SCRUB_D_f')\n",
    "ntk_D_t_activations,ntk_D_t_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(test_loader_full),'SCRUB_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, modelf)\n",
    "#hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-7\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_D_r_activations,fisher_D_r_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(retain_loader),'Fisher_D_r')\n",
    "fisher_D_f_activations,fisher_D_f_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(forget_loader),'Fisher_D_f')\n",
    "fisher_D_t_activations,fisher_D_t_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(test_loader_full),'Fisher_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information in the Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_D_r_activations,finetune_D_r_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(retain_loader),'Finetune_D_r')\n",
    "finetune_D_f_activations,finetune_D_f_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(forget_loader),'Finetune_D_f')\n",
    "finetune_D_t_activations,finetune_D_t_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(test_loader_full),'Finetune_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(5,1,figsize=(9,20))\n",
    "ax = [0,0,0,0,0]\n",
    "plot_entropy_dist(copy.deepcopy(model_s),retain_loader,forget_loader,test_loader_full, 'SCRUB',ax[4])\n",
    "plot_entropy_dist(copy.deepcopy(model),retain_loader,forget_loader,test_loader_full, 'original', ax[0])\n",
    "plot_entropy_dist(copy.deepcopy(model0),retain_loader,forget_loader,test_loader_full, 'retrain', ax[1])\n",
    "plot_entropy_dist(copy.deepcopy(model_ft),retain_loader,forget_loader,test_loader_full, 'finetuned',ax[2])\n",
    "plot_entropy_dist(copy.deepcopy(modelf),retain_loader,forget_loader,test_loader_full, 'fisher',ax[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
