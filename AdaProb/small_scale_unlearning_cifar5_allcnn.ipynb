{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_371425/2931265111.py:24: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import variational\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from itertools import cycle\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 100\n",
      "[0] train metrics:{\"loss\": 3.9548908950805663, \"error\": 0.9099}\n",
      "Learning Rate : 0.1\n",
      "[0] dry_run metrics:{\"loss\": 3.6569058528900147, \"error\": 0.864325}\n",
      "Learning Rate : 0.1\n",
      "[0] test metrics:{\"loss\": 3.669260646820068, \"error\": 0.8685}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 14.64 sec\n",
      "[1] train metrics:{\"loss\": 3.4458515224456785, \"error\": 0.82415}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.59 sec\n",
      "[2] train metrics:{\"loss\": 3.016041721725464, \"error\": 0.729925}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.52 sec\n",
      "[3] train metrics:{\"loss\": 2.711107554626465, \"error\": 0.65535}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.75 sec\n",
      "[4] train metrics:{\"loss\": 2.496060283279419, \"error\": 0.593225}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.82 sec\n",
      "[5] train metrics:{\"loss\": 2.329194931793213, \"error\": 0.538525}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.72 sec\n",
      "[6] train metrics:{\"loss\": 2.2086135860443115, \"error\": 0.49755}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.75 sec\n",
      "[7] train metrics:{\"loss\": 2.123908861541748, \"error\": 0.462825}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.38 sec\n",
      "[8] train metrics:{\"loss\": 2.049863201332092, \"error\": 0.431325}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.18 sec\n",
      "[9] train metrics:{\"loss\": 1.9988761100769044, \"error\": 0.40755}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.17 sec\n",
      "[10] train metrics:{\"loss\": 1.9432452823638915, \"error\": 0.379025}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.56 sec\n",
      "[11] train metrics:{\"loss\": 1.9131594074249267, \"error\": 0.35665}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.73 sec\n",
      "[12] train metrics:{\"loss\": 1.8789236625671386, \"error\": 0.332075}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.7 sec\n",
      "[13] train metrics:{\"loss\": 1.8584274074554443, \"error\": 0.31195}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.27 sec\n",
      "[14] train metrics:{\"loss\": 1.835866969680786, \"error\": 0.29505}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.33 sec\n",
      "[15] train metrics:{\"loss\": 1.8279486743927003, \"error\": 0.2791}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.28 sec\n",
      "[16] train metrics:{\"loss\": 1.8283887172698974, \"error\": 0.26425}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.3 sec\n",
      "[17] train metrics:{\"loss\": 1.7811426517486573, \"error\": 0.240475}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.25 sec\n",
      "[18] train metrics:{\"loss\": 1.7966106941223146, \"error\": 0.23225}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.33 sec\n",
      "[19] train metrics:{\"loss\": 1.791147449493408, \"error\": 0.21845}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.37 sec\n",
      "[20] train metrics:{\"loss\": 1.7986805698394774, \"error\": 0.20865}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.79 sec\n",
      "[21] train metrics:{\"loss\": 1.802754467010498, \"error\": 0.20475}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.4 sec\n",
      "[22] train metrics:{\"loss\": 1.793176968383789, \"error\": 0.19225}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.33 sec\n",
      "[23] train metrics:{\"loss\": 1.8183617504119873, \"error\": 0.19235}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.72 sec\n",
      "[24] train metrics:{\"loss\": 1.7976149871826173, \"error\": 0.17805}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.78 sec\n",
      "[25] train metrics:{\"loss\": 1.8099366149902343, \"error\": 0.178875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.75 sec\n",
      "[26] train metrics:{\"loss\": 1.791230718421936, \"error\": 0.168925}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.68 sec\n",
      "[27] train metrics:{\"loss\": 1.8296453895568847, \"error\": 0.176575}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.73 sec\n",
      "[28] train metrics:{\"loss\": 1.79987815284729, \"error\": 0.16505}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.79 sec\n",
      "[29] train metrics:{\"loss\": 1.8186018115997316, \"error\": 0.16935}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 8.02 sec\n",
      "[30] train metrics:{\"loss\": 1.8211508485794068, \"error\": 0.1652}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.89 sec\n",
      "Pure training time: 233.96000000000004 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset cifar100 --dataroot=data/cifar-100-python --model allcnn --filters 1.0 --lr 0.1 --lossfn ce --num-classes 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_cifar5_allcnn_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in small_cifar5_allcnn_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 5\n",
      "[0] train metrics:{\"loss\": 1.711760115623474, \"error\": 0.7340000002384186}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.6461147890090941, \"error\": 0.7060000005364419}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.23 sec\n",
      "[1] train metrics:{\"loss\": 1.625376088142395, \"error\": 0.746000000834465}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[2] train metrics:{\"loss\": 1.5328375396728515, \"error\": 0.7240000023841858}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[3] train metrics:{\"loss\": 1.4741359691619873, \"error\": 0.6880000026226044}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[4] train metrics:{\"loss\": 1.4123272953033448, \"error\": 0.6560000009536743}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.12 sec\n",
      "[5] train metrics:{\"loss\": 1.3396775016784668, \"error\": 0.5860000019073486}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.12 sec\n",
      "[6] train metrics:{\"loss\": 1.2723939123153687, \"error\": 0.5259999997615814}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[7] train metrics:{\"loss\": 1.2122151250839233, \"error\": 0.46000000047683715}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[8] train metrics:{\"loss\": 1.157936420440674, \"error\": 0.41200000095367434}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[9] train metrics:{\"loss\": 1.103785228729248, \"error\": 0.3999999942779541}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[10] train metrics:{\"loss\": 1.055343381881714, \"error\": 0.37200000524520876}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.12 sec\n",
      "[11] train metrics:{\"loss\": 1.0106943554878234, \"error\": 0.3579999990463257}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[12] train metrics:{\"loss\": 0.9717443323135376, \"error\": 0.33799999952316284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[13] train metrics:{\"loss\": 0.9352373170852661, \"error\": 0.3100000057220459}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[14] train metrics:{\"loss\": 0.9033952088356018, \"error\": 0.29400000667572024}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[15] train metrics:{\"loss\": 0.8750443806648255, \"error\": 0.2899999952316284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.13 sec\n",
      "[16] train metrics:{\"loss\": 0.8474231815338135, \"error\": 0.2740000042915344}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[17] train metrics:{\"loss\": 0.8216310529708862, \"error\": 0.27199999952316284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[18] train metrics:{\"loss\": 0.8033852329254151, \"error\": 0.27600000429153443}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[19] train metrics:{\"loss\": 0.777536406993866, \"error\": 0.2580000023841858}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[20] train metrics:{\"loss\": 0.7597041602134704, \"error\": 0.24000000715255737}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[21] train metrics:{\"loss\": 0.741195752620697, \"error\": 0.22800000190734862}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[22] train metrics:{\"loss\": 0.7218428940773011, \"error\": 0.2259999976158142}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[23] train metrics:{\"loss\": 0.7067114396095275, \"error\": 0.22000000524520874}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[24] train metrics:{\"loss\": 0.6904466762542725, \"error\": 0.20599999809265138}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[25] train metrics:{\"loss\": 0.6757208504676819, \"error\": 0.21400000047683715}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.13 sec\n",
      "[26] train metrics:{\"loss\": 0.6622907114028931, \"error\": 0.21000000715255737}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[27] train metrics:{\"loss\": 0.6498556952476502, \"error\": 0.18799999618530275}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[28] train metrics:{\"loss\": 0.6395586590766906, \"error\": 0.18599999809265136}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[29] train metrics:{\"loss\": 0.6236334853172302, \"error\": 0.18400000524520874}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[30] train metrics:{\"loss\": 0.6141241917610168, \"error\": 0.17799999618530274}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.12 sec\n",
      "Pure training time: 3.0000000000000004 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset small_cifar5 --model allcnn --dataroot=data/cifar10/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_cifar5_allcnn_1_0_forget_[0]_num_25_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in small_cifar5_allcnn_1_0_forget_[0]_num_25_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Replacing indexes [83 30 56 24 16 23  2 27 28 13 99 92 76 14  0 21  3 29 61 79 35 11 84 44\n",
      " 73]\n",
      "Number of Classes: 5\n",
      "[0] train metrics:{\"loss\": 1.7446083583831786, \"error\": 0.7520000002384186}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.6380911273956298, \"error\": 0.716000001192093}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.16 sec\n",
      "[1] train metrics:{\"loss\": 1.630987295150757, \"error\": 0.7720000008344651}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[2] train metrics:{\"loss\": 1.5239254789352417, \"error\": 0.7160000023841858}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[3] train metrics:{\"loss\": 1.4662158279418944, \"error\": 0.6739999980926513}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[4] train metrics:{\"loss\": 1.4061570825576781, \"error\": 0.6560000009536743}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[5] train metrics:{\"loss\": 1.3328111085891723, \"error\": 0.601999997138977}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.12 sec\n",
      "[6] train metrics:{\"loss\": 1.2658389024734498, \"error\": 0.5379999973773957}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[7] train metrics:{\"loss\": 1.2040145664215087, \"error\": 0.46999999570846557}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[8] train metrics:{\"loss\": 1.150999900817871, \"error\": 0.436000002861023}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[9] train metrics:{\"loss\": 1.0999460592269898, \"error\": 0.4019999942779541}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[10] train metrics:{\"loss\": 1.0531661491394042, \"error\": 0.37999999380111693}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.13 sec\n",
      "[11] train metrics:{\"loss\": 1.008168876171112, \"error\": 0.3660000038146973}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[12] train metrics:{\"loss\": 0.9705534439086914, \"error\": 0.35000000143051146}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[13] train metrics:{\"loss\": 0.9347562575340271, \"error\": 0.3320000033378601}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[14] train metrics:{\"loss\": 0.9026923551559448, \"error\": 0.3019999976158142}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[15] train metrics:{\"loss\": 0.8743598661422729, \"error\": 0.28599999523162845}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.13 sec\n",
      "[16] train metrics:{\"loss\": 0.8460431132316589, \"error\": 0.2559999995231628}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[17] train metrics:{\"loss\": 0.819923360824585, \"error\": 0.2660000019073486}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[18] train metrics:{\"loss\": 0.8011162314414978, \"error\": 0.2680000042915344}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[19] train metrics:{\"loss\": 0.7742109866142273, \"error\": 0.2500000047683716}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[20] train metrics:{\"loss\": 0.7555031943321228, \"error\": 0.24000000715255737}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.14 sec\n",
      "[21] train metrics:{\"loss\": 0.7368209028244018, \"error\": 0.23199999475479127}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[22] train metrics:{\"loss\": 0.7159987111091614, \"error\": 0.2260000047683716}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[23] train metrics:{\"loss\": 0.700425573348999, \"error\": 0.21600000762939453}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[24] train metrics:{\"loss\": 0.6831855959892273, \"error\": 0.20799999570846558}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[25] train metrics:{\"loss\": 0.6674004855155945, \"error\": 0.20800000047683717}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.14 sec\n",
      "[26] train metrics:{\"loss\": 0.6535353503227234, \"error\": 0.20599999570846558}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[27] train metrics:{\"loss\": 0.6405342206954956, \"error\": 0.20599999809265138}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.13 sec\n",
      "[28] train metrics:{\"loss\": 0.6303138246536255, \"error\": 0.18800000715255738}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[29] train metrics:{\"loss\": 0.6138576941490174, \"error\": 0.18400000524520874}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[30] train metrics:{\"loss\": 0.6039753541946411, \"error\": 0.18000000095367433}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.13 sec\n",
      "Pure training time: 3.1100000000000008 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset small_cifar5 --model allcnn --dataroot=data/cifar10/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 \\\n",
    "--forget-class 0 --num-to-forget 25 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 1619045\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model0 = copy.deepcopy(model)\n",
    "model_initial = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 300#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "model_initial.load_state_dict(torch.load(init_checkpoint))\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)\n",
    "\n",
    "model.cuda()\n",
    "model0.cuda()\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between w(D) and w(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(model,model0):\n",
    "    distance=0\n",
    "    normalization=0\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space='  ' if 'bias' in k else ''\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "        distance+=current_dist\n",
    "        normalization+=current_norm\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 0.1168288008815263\n",
      "Normalized Distance: 0.001444909469452316\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Retrain']=distance(model,model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance of w(D) from initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntk_init(resume,seed=1):\n",
    "    manual_seed(seed)\n",
    "    model_init = models.get_model(arch, num_classes=num_classes, filters_percentage=filters).to(args.device)\n",
    "    model_init.load_state_dict(torch.load(resume))\n",
    "    return model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "for p in model_init.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 0.9483404309151064\n",
      "Normalized Distance: 0.011732403872896827\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Original_init']=distance(model_init,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.retain_bs = 32\n",
    "args.forget_bs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [83 30 56 24 16 23  2 27 28 13 99 92 76 14  0 21  3 29 61 79 35 11 84 44\n",
      " 73]\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _ = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=args.forget_bs, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=args.retain_bs, seed=seed, shuffle=True)\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "475\n",
      "500\n",
      "500\n",
      "{0: 100, 1: 100, 2: 100, 3: 100, 4: 100}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'sgd'\n",
    "args.gamma = 1\n",
    "args.alpha = 0.5\n",
    "args.beta = 0.9\n",
    "args.smoothing = 0.5\n",
    "args.msteps = 5\n",
    "args.clip = 0.2\n",
    "args.sstart = 10\n",
    "args.kd_T = 2\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_epochs = 10\n",
    "args.sgda_learning_rate = 0.02\n",
    "args.lr_decay_epochs = [5,8,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 0.1#5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> scrub unlearning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihao/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 54.105 \n",
      "maximize loss: -1.40\t minimize loss: 11.36\t train_acc: 54.10525894165039\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 66.316 \n",
      "maximize loss: 15.52\t minimize loss: 38.92\t train_acc: 66.31578826904297\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 77.053 \n",
      "maximize loss: 46.14\t minimize loss: 61.89\t train_acc: 77.05262756347656\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 89.053 \n",
      "maximize loss: 66.79\t minimize loss: 78.99\t train_acc: 89.05262756347656\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 93.053 \n",
      "maximize loss: 81.80\t minimize loss: 90.28\t train_acc: 93.05262756347656\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 98.105 \n",
      "maximize loss: 0.00\t minimize loss: 94.41\t train_acc: 98.10525512695312\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.158 \n",
      "maximize loss: 0.00\t minimize loss: 94.88\t train_acc: 99.15789031982422\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: 0.00\t minimize loss: 95.32\t train_acc: 99.99999237060547\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 98.947 \n",
      "maximize loss: 0.00\t minimize loss: 95.56\t train_acc: 98.9473648071289\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.789 \n",
      "maximize loss: 0.00\t minimize loss: 95.58\t train_acc: 99.78946685791016\n"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> scrub unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True, prefix=\"train_cifar5_allcnn\")\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True, prefix=\"forget_cifar5_allcnn\")\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\",)\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('scrub retain- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.894737243652344, 29.052635192871094, 81.26315879821777, 78.52631759643555, 78.94736862182617, 65.89473724365234, 2.9473724365234375, 0.6315841674804688, 0.21053314208984375, 0.21053314208984375]\n",
      "[12.0, 40.0, 0.0, 0.0, 0.0, 48.0, 44.0, 36.0, 40.0, 44.0]\n"
     ]
    }
   ],
   "source": [
    "print (acc_rs)\n",
    "print (acc_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTK based Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NTK Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_w_utils(model_init,dataloader,name='complete'):\n",
    "    model_init.eval()\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    G_list = []\n",
    "    f0_minus_y = []\n",
    "    for idx, batch in enumerate(dataloader):#(tqdm(dataloader,leave=False)):\n",
    "        batch = [tensor.to(next(model_init.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        if 'mnist' in args.dataset:\n",
    "            input = input.view(input.shape[0],-1)\n",
    "        target = target.cpu().detach().numpy()\n",
    "        output = model_init(input)\n",
    "        G_sample=[]\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output[0,cls],model_init.parameters(),retain_graph=True)\n",
    "            grads = np.concatenate([g.view(-1).cpu().numpy() for g in grads])\n",
    "            G_sample.append(grads)\n",
    "            G_list.append(grads)\n",
    "        if args.lossfn=='mse':\n",
    "            p = output.cpu().detach().numpy().transpose()\n",
    "            #loss_hess = np.eye(len(p))\n",
    "            target = 2*target-1\n",
    "            f0_y_update = p-target\n",
    "        elif args.lossfn=='ce':\n",
    "            p = torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().transpose()\n",
    "            p[target]-=1\n",
    "            f0_y_update = copy.deepcopy(p)\n",
    "        f0_minus_y.append(f0_y_update)\n",
    "    return np.stack(G_list).transpose(),np.vstack(f0_minus_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobians and Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk_time = 0\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_r,f0_minus_y_r = delta_w_utils(copy.deepcopy(model),retain_loader,'complete')\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_r.npy',G_r)\n",
    "np.save('NTK_data/f0_minus_y_r.npy',f0_minus_y_r)\n",
    "del G_r, f0_minus_y_r\n",
    "\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_f,f0_minus_y_f = delta_w_utils(copy.deepcopy(model),forget_loader,'retain') \n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_f.npy',G_f)\n",
    "np.save('NTK_data/f0_minus_y_f.npy',f0_minus_y_f)\n",
    "del G_f, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "G_f = np.load('NTK_data/G_f.npy')\n",
    "G = np.concatenate([G_r,G_f],axis=1)\n",
    "\n",
    "np.save('NTK_data/G.npy',G)\n",
    "del G, G_f, G_r\n",
    "\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "f0_minus_y_f = np.load('NTK_data/f0_minus_y_f.npy')\n",
    "f0_minus_y = np.concatenate([f0_minus_y_r,f0_minus_y_f])\n",
    "\n",
    "np.save('NTK_data/f0_minus_y.npy',f0_minus_y)\n",
    "del f0_minus_y, f0_minus_y_r, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only requires access to the gradients and the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.load('NTK_data/G.npy')\n",
    "t1 = time.time()\n",
    "theta = G.transpose().dot(G) + num_total*args.weight_decay*np.eye(G.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G\n",
    "\n",
    "t1 = time.time()\n",
    "theta_inv = np.linalg.inv(theta)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta.npy',theta)\n",
    "del theta\n",
    "\n",
    "G = np.load('NTK_data/G.npy')\n",
    "f0_minus_y = np.load('NTK_data/f0_minus_y.npy')\n",
    "t1 = time.time()\n",
    "w_complete = -G.dot(theta_inv.dot(f0_minus_y))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_inv.npy',theta_inv)\n",
    "np.save('NTK_data/w_complete.npy',w_complete)\n",
    "\n",
    "del G, f0_minus_y, theta_inv, w_complete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "t1 = time.time()\n",
    "theta_r = G_r.transpose().dot(G_r) + num_to_retain*args.weight_decay*np.eye(G_r.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G_r\n",
    "\n",
    "t1 = time.time()\n",
    "theta_r_inv = np.linalg.inv(theta_r)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r.npy',theta_r)\n",
    "del theta_r\n",
    "\n",
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "t1 = time.time()\n",
    "w_retain = -G_r.dot(theta_r_inv.dot(f0_minus_y_r))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r_inv.npy',theta_r_inv)\n",
    "np.save('NTK_data/w_retain.npy',w_retain)\n",
    "\n",
    "del G_r, f0_minus_y_r, theta_r_inv, w_retain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrubbing Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Scrubbing Direction\n",
    "w_complete = np.load('NTK_data/w_complete.npy')\n",
    "w_retain = np.load('NTK_data/w_retain.npy')\n",
    "delta_w = (w_retain-w_complete).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_copy = copy.deepcopy(delta_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Change in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_actual = vectorize_params(model0)-vectorize_params(model)\n",
    "\n",
    "print(f'Actual Norm-: {np.linalg.norm(delta_w_actual)}')\n",
    "print(f'Predtn Norm-: {np.linalg.norm(delta_w)}')\n",
    "scale_ratio = np.linalg.norm(delta_w_actual)/np.linalg.norm(delta_w)\n",
    "print('Actual Scale: {}'.format(scale_ratio))\n",
    "log_dict['actual_scale_ratio']=scale_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trapezium Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred_error = vectorize_params(model)-vectorize_params(model_init)-w_retain.squeeze()\n",
    "print(f\"Delta w -------: {np.linalg.norm(delta_w)}\")\n",
    "\n",
    "inner = np.inner(delta_w/np.linalg.norm(delta_w),m_pred_error/np.linalg.norm(m_pred_error))\n",
    "print(f\"Inner Product--: {inner}\")\n",
    "\n",
    "if inner<0:\n",
    "    angle = np.arccos(inner)-np.pi/2\n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.sin(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "else:\n",
    "    angle = np.arccos(inner) \n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.cos(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "\n",
    "predicted_scale=predicted_norm/np.linalg.norm(delta_w)\n",
    "predicted_scale\n",
    "print(f\"Predicted Scale:  {predicted_scale}\")\n",
    "log_dict['predicted_scale_ratio']=predicted_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Inner Product between Prediction and Actual Scrubbing Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIP(v1,v2):\n",
    "    nip = (np.inner(v1/np.linalg.norm(v1),v2/np.linalg.norm(v2)))\n",
    "    print(nip)\n",
    "    return nip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nip=NIP(delta_w_actual,delta_w)\n",
    "log_dict['nip']=nip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_w_dict(delta_w,model):\n",
    "    # Give normalized delta_w\n",
    "    delta_w_dict = OrderedDict()\n",
    "    params_visited = 0\n",
    "    for k,p in model.named_parameters():\n",
    "        num_params = np.prod(list(p.shape))\n",
    "        update_params = delta_w[params_visited:params_visited+num_params]\n",
    "        delta_w_dict[k] = torch.Tensor(update_params).view_as(p)\n",
    "        params_visited+=num_params\n",
    "    return delta_w_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics,activations,predictions=get_metrics(model,dataloader,criterion,True)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "\n",
    "    return activations,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub using NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=predicted_scale\n",
    "direction = get_delta_w_dict(delta_w,model)\n",
    "\n",
    "model_scrub = copy.deepcopy(model)\n",
    "for k,p in model_scrub.named_parameters():\n",
    "    p.data += (direction[k]*scale).to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune and Fisher Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg\n",
    "\n",
    "def run_neggrad_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    forget_loader: torch.utils.data.DataLoader,\n",
    "                    alpha: float,\n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    quiet=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, (batch_retain,batch_forget) in enumerate(tqdm(zip(data_loader,cycle(forget_loader)), leave=False)):\n",
    "            batch_retain = [tensor.to(next(model.parameters()).device) for tensor in batch_retain]\n",
    "            batch_forget = [tensor.to(next(model.parameters()).device) for tensor in batch_forget]\n",
    "            input_r, target_r = batch_retain\n",
    "            input_f, target_f = batch_forget\n",
    "            output_r = model(input_r)\n",
    "            output_f = model(input_f)\n",
    "            loss = alpha*(loss_fn(output_r, target_r) + l2_penalty(model,model_init,args.weight_decay)) - (1-alpha)*loss_fn(output_f, target_f)\n",
    "            metrics.update(n=input_r.size(0), loss=loss_fn(output_r,target_r).item(), error=get_error(output_r, target_r))\n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def negative_grad(model: nn.Module, data_loader: torch.utils.data.DataLoader, forget_loader: torch.utils.data.DataLoader, alpha: float, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_neggrad_epoch(model, model_init, data_loader, forget_loader, alpha, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "\n",
    "def fk_fientune(model: nn.Module, data_loader: torch.utils.data.DataLoader, args, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.1, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computes the amount of information not forgotten at all layers using the given alpha\n",
    "alpha = 1e-7\n",
    "total_kl = 0\n",
    "torch.manual_seed(seed)\n",
    "for (k, p), (k0, p0) in zip(modelf.named_parameters(), modelf0.named_parameters()):\n",
    "    mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "    mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "    kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "    total_kl += kl\n",
    "    print(k, f'{kl:.1f}')\n",
    "print(\"Total:\", total_kl)\n",
    "log_dict['fisher_info']=total_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-6\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())\n",
    "\n",
    "for i, p in enumerate(modelf0.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test(modelf, retain_loader))\n",
    "print(test(modelf, forget_loader))\n",
    "print(test(modelf, valid_loader_full))\n",
    "print(test(modelf, test_loader_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.ng_alpha = 0.95\n",
    "args.ng_epochs = 10\n",
    "args.ng_lr = 0.01\n",
    "model_ng = copy.deepcopy(model)    \n",
    "negative_grad(model_ng, retain_loader, forget_loader, alpha=args.ng_alpha, epochs=args.ng_epochs, quiet=True, lr=args.ng_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catastrophic Forgetting k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.cfk_lr = 0.01\n",
    "args.cfk_epochs = 10\n",
    "\n",
    "model_cfk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_cfk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    layers = [9]\n",
    "    for k in layers:\n",
    "        for param in model_cfk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    for param in model_cfk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "fk_fientune(model_cfk, retain_loader, args=args, epochs=args.cfk_epochs, quiet=True, lr=args.cfk_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Unlearning k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The last block and classifier of resnet-18\n",
    "(layer4): Sequential(\n",
    "    (0): _ResBlock(\n",
    "      (bn1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(102, 204, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (shortcut): Sequential(\n",
    "        (0): Conv2d(102, 204, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "      )\n",
    "    )\n",
    "    (1): _ResBlock(\n",
    "      (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    )\n",
    "  )\n",
    "(linear): Linear(in_features=204, out_features=5, bias=True)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" The last block and classifier of allcnn\n",
    "AllCNN(\n",
    "  (features): Sequential(\n",
    "    ...\n",
    "    (9): Conv(\n",
    "      (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (2): ReLU()\n",
    "    )\n",
    "    (10): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
    "    (11): Flatten()\n",
    "  )\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=192, out_features=5, bias=True)\n",
    "  )\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.euk_lr = 0.01\n",
    "args.euk_epochs = training_epochs\n",
    "model_euk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_euk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    with torch.no_grad():\n",
    "        for k in layers:\n",
    "            for i in range(0,3):\n",
    "                try:\n",
    "                    model_euk.features[k][i].weight.copy_(model_initial.features[k][i].weight)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have weights\".format(k,i))\n",
    "                try:\n",
    "                    model_euk.features[k][i].bias.copy_(model_initial.features[k][i].bias)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have bias\".format(k,i))\n",
    "        model_euk.classifier[0].weight.copy_(model_initial.classifier[0].weight)\n",
    "        model_euk.classifier[0].bias.copy_(model_initial.classifier[0].bias)\n",
    "    \n",
    "    for k in layers:\n",
    "        for param in model_euk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,2):\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.weight.copy_(model_initial.layer4[i].bn1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.bias.copy_(model_initial.layer4[i].bn1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.weight.copy_(model_initial.layer4[i].conv1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.bias.copy_(model_initial.layer4[i].conv1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.weight.copy_(model_initial.layer4[i].bn2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.bias.copy_(model_initial.layer4[i].bn2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.weight.copy_(model_initial.layer4[i].conv2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.bias.copy_(model_initial.layer4[i].conv2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "        model_euk.layer4[0].shortcut[0].weight.copy_(model_initial.layer4[0].shortcut[0].weight)\n",
    "        \n",
    "    for param in model_euk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "fk_fientune(model_euk, retain_loader, epochs=args.euk_epochs, quiet=True, lr=args.euk_lr, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "_,_=activations_predictions(copy.deepcopy(model),forget_loader,'Original_Model_D_f')\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"c\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_ng),thresh,'NegGrad')\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model_cfk),thresh,'CF-k')\n",
    "readouts[\"f\"] = all_readouts(copy.deepcopy(model_euk),thresh,'EU-k')\n",
    "readouts[\"g\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"h\"] = all_readouts(copy.deepcopy(model_scrub),thresh,'NTK')\n",
    "readouts[\"i\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
