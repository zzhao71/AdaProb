{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_363824/2931265111.py:24: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import variational\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from itertools import cycle\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: lacuna100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in lacuna100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 100\n",
      "[0] train metrics:{\"loss\": 3.8233655672073366, \"error\": 0.91084375}\n",
      "Learning Rate : 0.1\n",
      "[0] dry_run metrics:{\"loss\": 3.3233438425064086, \"error\": 0.8385625}\n",
      "Learning Rate : 0.1\n",
      "[0] test metrics:{\"loss\": 3.3385062034606934, \"error\": 0.8347}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.8 sec\n",
      "[1] train metrics:{\"loss\": 3.040908597946167, \"error\": 0.77615625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.83 sec\n",
      "[2] train metrics:{\"loss\": 2.4334757966995237, \"error\": 0.61228125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.83 sec\n",
      "[3] train metrics:{\"loss\": 1.9808078465461731, \"error\": 0.47328125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.83 sec\n",
      "[4] train metrics:{\"loss\": 1.7013714575767518, \"error\": 0.372}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.84 sec\n",
      "[5] train metrics:{\"loss\": 1.4868793172836303, \"error\": 0.29221875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.8 sec\n",
      "[6] train metrics:{\"loss\": 1.3205368399620057, \"error\": 0.22421875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.74 sec\n",
      "[7] train metrics:{\"loss\": 1.2272959723472596, \"error\": 0.18584375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.83 sec\n",
      "[8] train metrics:{\"loss\": 1.133659630537033, \"error\": 0.1455625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.72 sec\n",
      "[9] train metrics:{\"loss\": 1.058427065372467, \"error\": 0.1150625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.69 sec\n",
      "[10] train metrics:{\"loss\": 1.0026730167865754, \"error\": 0.09853125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.49 sec\n",
      "[11] train metrics:{\"loss\": 0.9732379763126373, \"error\": 0.0879375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.27 sec\n",
      "[12] train metrics:{\"loss\": 0.9377931756973267, \"error\": 0.078125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.23 sec\n",
      "[13] train metrics:{\"loss\": 0.9080658020973206, \"error\": 0.07284375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.26 sec\n",
      "[14] train metrics:{\"loss\": 0.9161779451370239, \"error\": 0.07359375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.31 sec\n",
      "[15] train metrics:{\"loss\": 0.8948067147731781, \"error\": 0.06978125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.27 sec\n",
      "[16] train metrics:{\"loss\": 0.9115999133586884, \"error\": 0.07090625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.38 sec\n",
      "[17] train metrics:{\"loss\": 0.8692510938644409, \"error\": 0.06215625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.41 sec\n",
      "[18] train metrics:{\"loss\": 0.850548889875412, \"error\": 0.05765625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.35 sec\n",
      "[19] train metrics:{\"loss\": 0.8475376484394074, \"error\": 0.0595625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.45 sec\n",
      "[20] train metrics:{\"loss\": 0.8996174812316895, \"error\": 0.0716875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.55 sec\n",
      "[21] train metrics:{\"loss\": 0.9307970433235169, \"error\": 0.07428125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.52 sec\n",
      "[22] train metrics:{\"loss\": 0.8617413666248321, \"error\": 0.053}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.65 sec\n",
      "[23] train metrics:{\"loss\": 0.7489767985343934, \"error\": 0.03721875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.68 sec\n",
      "[24] train metrics:{\"loss\": 0.6187533376216888, \"error\": 0.0264375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.58 sec\n",
      "[25] train metrics:{\"loss\": 0.5147787270545959, \"error\": 0.02403125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.54 sec\n",
      "[26] train metrics:{\"loss\": 0.4404430638551712, \"error\": 0.02259375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.55 sec\n",
      "[27] train metrics:{\"loss\": 0.5160071790218353, \"error\": 0.0500625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.73 sec\n",
      "[28] train metrics:{\"loss\": 1.382216766834259, \"error\": 0.2360625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.47 sec\n",
      "[29] train metrics:{\"loss\": 1.0175488970279694, \"error\": 0.0950625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.39 sec\n",
      "[30] train metrics:{\"loss\": 0.8608677124977112, \"error\": 0.05125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.44 sec\n",
      "Pure training time: 110.69000000000001 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset lacuna100 --dataroot=data/lacuna100 --model allcnn --filters 1.0 --lr 0.1 --lossfn ce --num-classes 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_lacuna5_allcnn_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in small_lacuna5_allcnn_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 5\n",
      "[0] train metrics:{\"loss\": 1.4590839557647706, \"error\": 0.6840000028610229}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.3678623170852662, \"error\": 0.5960000004768372}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.14 sec\n",
      "[1] train metrics:{\"loss\": 1.2981820039749146, \"error\": 0.518000002861023}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[2] train metrics:{\"loss\": 1.0837601623535156, \"error\": 0.24999999570846557}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[3] train metrics:{\"loss\": 0.8607985868453979, \"error\": 0.10200000143051148}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[4] train metrics:{\"loss\": 0.6491048440933228, \"error\": 0.060000004291534424}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[5] train metrics:{\"loss\": 0.47359545302391054, \"error\": 0.04400000667572022}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[6] train metrics:{\"loss\": 0.3508398938179016, \"error\": 0.03799999523162842}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[7] train metrics:{\"loss\": 0.2672734785079956, \"error\": 0.030000006675720215}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[8] train metrics:{\"loss\": 0.2132925441265106, \"error\": 0.028000006675720213}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[9] train metrics:{\"loss\": 0.17869519186019897, \"error\": 0.024000006675720213}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[10] train metrics:{\"loss\": 0.15542979335784912, \"error\": 0.02199999523162842}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[11] train metrics:{\"loss\": 0.14084818971157073, \"error\": 0.01999999761581421}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[12] train metrics:{\"loss\": 0.12873309469223024, \"error\": 0.01799999761581421}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[13] train metrics:{\"loss\": 0.12001674669981002, \"error\": 0.018000006675720215}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[14] train metrics:{\"loss\": 0.1127758259177208, \"error\": 0.01599999761581421}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[15] train metrics:{\"loss\": 0.1069691019654274, \"error\": 0.01}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[16] train metrics:{\"loss\": 0.10179928606748581, \"error\": 0.009999995231628417}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[17] train metrics:{\"loss\": 0.09770696723461152, \"error\": 0.01}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[18] train metrics:{\"loss\": 0.09437720918655396, \"error\": 0.007999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[19] train metrics:{\"loss\": 0.09001460498571395, \"error\": 0.007999995231628417}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[20] train metrics:{\"loss\": 0.08771527725458145, \"error\": 0.004}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[21] train metrics:{\"loss\": 0.08524861389398575, \"error\": 0.004}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[22] train metrics:{\"loss\": 0.08239801812171936, \"error\": 0.003999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[23] train metrics:{\"loss\": 0.08023976457118988, \"error\": 0.004}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[24] train metrics:{\"loss\": 0.0783043841123581, \"error\": 0.003999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[25] train metrics:{\"loss\": 0.07680189085006714, \"error\": 0.004}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[26] train metrics:{\"loss\": 0.075289994597435, \"error\": 0.003999995231628418}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[27] train metrics:{\"loss\": 0.07393369716405869, \"error\": 0.004}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[28] train metrics:{\"loss\": 0.07256700730323791, \"error\": 0.002}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[29] train metrics:{\"loss\": 0.07137462323904037, \"error\": 0.002}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[30] train metrics:{\"loss\": 0.07021671760082245, \"error\": 0.002}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "Pure training time: 1.700000000000001 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset small_lacuna5 --model allcnn --dataroot=data/lacuna10/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/lacuna100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_lacuna5_allcnn_1_0_forget_[0]_num_25_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in small_lacuna5_allcnn_1_0_forget_[0]_num_25_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Replacing indexes [83 30 56 24 16 23  2 27 28 13 99 92 76 14  0 21  3 29 61 79 35 11 84 44\n",
      " 73]\n",
      "Number of Classes: 5\n",
      "[0] train metrics:{\"loss\": 1.4798885374069213, \"error\": 0.7100000028610229}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.3643537063598632, \"error\": 0.5800000004768372}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.35 sec\n",
      "[1] train metrics:{\"loss\": 1.3051997661590575, \"error\": 0.5099999938011169}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[2] train metrics:{\"loss\": 1.0818023109436035, \"error\": 0.2580000047683716}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[3] train metrics:{\"loss\": 0.8598729882240296, \"error\": 0.1180000057220459}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[4] train metrics:{\"loss\": 0.6507702379226684, \"error\": 0.06400000190734863}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[5] train metrics:{\"loss\": 0.4739419188499451, \"error\": 0.03999999523162842}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[6] train metrics:{\"loss\": 0.34988249039649966, \"error\": 0.03599999523162842}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[7] train metrics:{\"loss\": 0.2664496068954468, \"error\": 0.027999995231628418}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[8] train metrics:{\"loss\": 0.21265333986282348, \"error\": 0.028000006675720213}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[9] train metrics:{\"loss\": 0.1775564751625061, \"error\": 0.02199999523162842}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[10] train metrics:{\"loss\": 0.15423362267017365, \"error\": 0.022000006675720215}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[11] train metrics:{\"loss\": 0.1401517364382744, \"error\": 0.02199999761581421}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[12] train metrics:{\"loss\": 0.12785666698217393, \"error\": 0.01799999761581421}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[13] train metrics:{\"loss\": 0.11890657591819763, \"error\": 0.018000006675720215}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[14] train metrics:{\"loss\": 0.1117145220041275, \"error\": 0.01599999761581421}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[15] train metrics:{\"loss\": 0.10598049992322922, \"error\": 0.012}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[16] train metrics:{\"loss\": 0.10086520785093307, \"error\": 0.009999995231628417}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[17] train metrics:{\"loss\": 0.09689262413978576, \"error\": 0.008}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[18] train metrics:{\"loss\": 0.09359157538414001, \"error\": 0.007999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[19] train metrics:{\"loss\": 0.08922278416156769, \"error\": 0.005999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[20] train metrics:{\"loss\": 0.08705470645427704, \"error\": 0.004}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[21] train metrics:{\"loss\": 0.08458097517490387, \"error\": 0.004}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[22] train metrics:{\"loss\": 0.08178346860408783, \"error\": 0.003999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[23] train metrics:{\"loss\": 0.07973912662267685, \"error\": 0.004}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[24] train metrics:{\"loss\": 0.07779766744375229, \"error\": 0.003999997615814209}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[25] train metrics:{\"loss\": 0.07630221545696259, \"error\": 0.004}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[26] train metrics:{\"loss\": 0.07493402379751206, \"error\": 0.003999995231628418}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.06 sec\n",
      "[27] train metrics:{\"loss\": 0.0734548642039299, \"error\": 0.004}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[28] train metrics:{\"loss\": 0.0721021785736084, \"error\": 0.004}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[29] train metrics:{\"loss\": 0.07100023311376571, \"error\": 0.004}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.05 sec\n",
      "[30] train metrics:{\"loss\": 0.06987242239713669, \"error\": 0.002}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "Pure training time: 1.8800000000000012 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset small_lacuna5 --model allcnn --dataroot=data/lacuna10/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/lacuna100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 \\\n",
    "--forget-class 0 --num-to-forget 25 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 1619045\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model0 = copy.deepcopy(model)\n",
    "model_initial = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 300#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "model_initial.load_state_dict(torch.load(init_checkpoint))\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)\n",
    "\n",
    "model.cuda()\n",
    "model0.cuda()\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between w(D) and w(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(model,model0):\n",
    "    distance=0\n",
    "    normalization=0\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space='  ' if 'bias' in k else ''\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "        distance+=current_dist\n",
    "        normalization+=current_norm\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 0.05914850576559511\n",
      "Normalized Distance: 0.0009167917086103514\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Retrain']=distance(model,model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance of w(D) from initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntk_init(resume,seed=1):\n",
    "    manual_seed(seed)\n",
    "    model_init = models.get_model(arch, num_classes=num_classes, filters_percentage=filters).to(args.device)\n",
    "    model_init.load_state_dict(torch.load(resume))\n",
    "    return model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "for p in model_init.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 0.8690820401609124\n",
      "Normalized Distance: 0.013477108689843522\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Original_init']=distance(model_init,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.retain_bs = 32\n",
    "args.forget_bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confuse mode: False\n",
      "split mode: train\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [83 30 56 24 16 23  2 27 28 13 99 92 76 14  0 21  3 29 61 79 35 11 84 44\n",
      " 73]\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _ = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=args.forget_bs, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=args.retain_bs, seed=seed, shuffle=True)\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "475\n",
      "500\n",
      "500\n",
      "{0: 100, 1: 100, 2: 100, 3: 100, 4: 100}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'sgd'\n",
    "args.gamma = 1\n",
    "args.alpha = 0.5\n",
    "args.beta = 0.9\n",
    "args.smoothing = 0.5\n",
    "args.msteps = 5\n",
    "args.clip = 0.2\n",
    "args.sstart = 10\n",
    "args.kd_T = 2\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_epochs = 10\n",
    "args.sgda_learning_rate = 0.02\n",
    "args.lr_decay_epochs = [5,8]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 0.1#5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> scrub unlearning ...\n",
      " * Acc@1 92.000 \n",
      "maximize loss: -4.31\t minimize loss: 7.16\t train_acc: 91.99999237060547\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 96.842 \n",
      "maximize loss: 10.27\t minimize loss: 25.58\t train_acc: 96.84210205078125\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.579 \n",
      "maximize loss: 22.80\t minimize loss: 42.95\t train_acc: 99.57894134521484\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.158 \n",
      "maximize loss: 38.54\t minimize loss: 55.87\t train_acc: 99.15789031982422\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.368 \n",
      "maximize loss: 50.60\t minimize loss: 64.66\t train_acc: 99.36841583251953\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.579 \n",
      "maximize loss: 0.00\t minimize loss: 68.16\t train_acc: 99.57894134521484\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: 0.00\t minimize loss: 68.57\t train_acc: 99.99999237060547\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: 0.00\t minimize loss: 68.87\t train_acc: 99.99999237060547\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: 0.00\t minimize loss: 69.11\t train_acc: 99.99999237060547\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: 0.00\t minimize loss: 69.10\t train_acc: 99.99999237060547\n"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> scrub unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True, prefix=\"train_lacuna5_allcnn\")\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True, prefix=\"forget_lacuna5_allcnn\")\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\",)\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('scrub retain- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21053314208984375, 2.736846923828125, 6.3157958984375, 47.36842346191406, 76.21052742004395, 57.68421173095703, 7.62939453125e-06, 7.62939453125e-06, 7.62939453125e-06, 7.62939453125e-06]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 12.0, 12.0, 12.0]\n"
     ]
    }
   ],
   "source": [
    "print (acc_rs)\n",
    "print (acc_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTK based Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NTK Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_w_utils(model_init,dataloader,name='complete'):\n",
    "    model_init.eval()\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    G_list = []\n",
    "    f0_minus_y = []\n",
    "    for idx, batch in enumerate(dataloader):#(tqdm(dataloader,leave=False)):\n",
    "        batch = [tensor.to(next(model_init.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        if 'mnist' in args.dataset:\n",
    "            input = input.view(input.shape[0],-1)\n",
    "        target = target.cpu().detach().numpy()\n",
    "        output = model_init(input)\n",
    "        G_sample=[]\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output[0,cls],model_init.parameters(),retain_graph=True)\n",
    "            grads = np.concatenate([g.view(-1).cpu().numpy() for g in grads])\n",
    "            G_sample.append(grads)\n",
    "            G_list.append(grads)\n",
    "        if args.lossfn=='mse':\n",
    "            p = output.cpu().detach().numpy().transpose()\n",
    "            #loss_hess = np.eye(len(p))\n",
    "            target = 2*target-1\n",
    "            f0_y_update = p-target\n",
    "        elif args.lossfn=='ce':\n",
    "            p = torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().transpose()\n",
    "            p[target]-=1\n",
    "            f0_y_update = copy.deepcopy(p)\n",
    "        f0_minus_y.append(f0_y_update)\n",
    "    return np.stack(G_list).transpose(),np.vstack(f0_minus_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobians and Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk_time = 0\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_r,f0_minus_y_r = delta_w_utils(copy.deepcopy(model),retain_loader,'complete')\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_r.npy',G_r)\n",
    "np.save('NTK_data/f0_minus_y_r.npy',f0_minus_y_r)\n",
    "del G_r, f0_minus_y_r\n",
    "\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_f,f0_minus_y_f = delta_w_utils(copy.deepcopy(model),forget_loader,'retain') \n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_f.npy',G_f)\n",
    "np.save('NTK_data/f0_minus_y_f.npy',f0_minus_y_f)\n",
    "del G_f, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "G_f = np.load('NTK_data/G_f.npy')\n",
    "G = np.concatenate([G_r,G_f],axis=1)\n",
    "\n",
    "np.save('NTK_data/G.npy',G)\n",
    "del G, G_f, G_r\n",
    "\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "f0_minus_y_f = np.load('NTK_data/f0_minus_y_f.npy')\n",
    "f0_minus_y = np.concatenate([f0_minus_y_r,f0_minus_y_f])\n",
    "\n",
    "np.save('NTK_data/f0_minus_y.npy',f0_minus_y)\n",
    "del f0_minus_y, f0_minus_y_r, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only requires access to the gradients and the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.load('NTK_data/G.npy')\n",
    "t1 = time.time()\n",
    "theta = G.transpose().dot(G) + num_total*args.weight_decay*np.eye(G.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G\n",
    "\n",
    "t1 = time.time()\n",
    "theta_inv = np.linalg.inv(theta)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta.npy',theta)\n",
    "del theta\n",
    "\n",
    "G = np.load('NTK_data/G.npy')\n",
    "f0_minus_y = np.load('NTK_data/f0_minus_y.npy')\n",
    "t1 = time.time()\n",
    "w_complete = -G.dot(theta_inv.dot(f0_minus_y))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_inv.npy',theta_inv)\n",
    "np.save('NTK_data/w_complete.npy',w_complete)\n",
    "\n",
    "del G, f0_minus_y, theta_inv, w_complete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "t1 = time.time()\n",
    "theta_r = G_r.transpose().dot(G_r) + num_to_retain*args.weight_decay*np.eye(G_r.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G_r\n",
    "\n",
    "t1 = time.time()\n",
    "theta_r_inv = np.linalg.inv(theta_r)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r.npy',theta_r)\n",
    "del theta_r\n",
    "\n",
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "t1 = time.time()\n",
    "w_retain = -G_r.dot(theta_r_inv.dot(f0_minus_y_r))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r_inv.npy',theta_r_inv)\n",
    "np.save('NTK_data/w_retain.npy',w_retain)\n",
    "\n",
    "del G_r, f0_minus_y_r, theta_r_inv, w_retain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrubbing Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Scrubbing Direction\n",
    "w_complete = np.load('NTK_data/w_complete.npy')\n",
    "w_retain = np.load('NTK_data/w_retain.npy')\n",
    "delta_w = (w_retain-w_complete).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_copy = copy.deepcopy(delta_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Change in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_actual = vectorize_params(model0)-vectorize_params(model)\n",
    "\n",
    "print(f'Actual Norm-: {np.linalg.norm(delta_w_actual)}')\n",
    "print(f'Predtn Norm-: {np.linalg.norm(delta_w)}')\n",
    "scale_ratio = np.linalg.norm(delta_w_actual)/np.linalg.norm(delta_w)\n",
    "print('Actual Scale: {}'.format(scale_ratio))\n",
    "log_dict['actual_scale_ratio']=scale_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trapezium Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred_error = vectorize_params(model)-vectorize_params(model_init)-w_retain.squeeze()\n",
    "print(f\"Delta w -------: {np.linalg.norm(delta_w)}\")\n",
    "\n",
    "inner = np.inner(delta_w/np.linalg.norm(delta_w),m_pred_error/np.linalg.norm(m_pred_error))\n",
    "print(f\"Inner Product--: {inner}\")\n",
    "\n",
    "if inner<0:\n",
    "    angle = np.arccos(inner)-np.pi/2\n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.sin(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "else:\n",
    "    angle = np.arccos(inner) \n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.cos(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "\n",
    "predicted_scale=predicted_norm/np.linalg.norm(delta_w)\n",
    "predicted_scale\n",
    "print(f\"Predicted Scale:  {predicted_scale}\")\n",
    "log_dict['predicted_scale_ratio']=predicted_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Inner Product between Prediction and Actual Scrubbing Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIP(v1,v2):\n",
    "    nip = (np.inner(v1/np.linalg.norm(v1),v2/np.linalg.norm(v2)))\n",
    "    print(nip)\n",
    "    return nip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nip=NIP(delta_w_actual,delta_w)\n",
    "log_dict['nip']=nip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_w_dict(delta_w,model):\n",
    "    # Give normalized delta_w\n",
    "    delta_w_dict = OrderedDict()\n",
    "    params_visited = 0\n",
    "    for k,p in model.named_parameters():\n",
    "        num_params = np.prod(list(p.shape))\n",
    "        update_params = delta_w[params_visited:params_visited+num_params]\n",
    "        delta_w_dict[k] = torch.Tensor(update_params).view_as(p)\n",
    "        params_visited+=num_params\n",
    "    return delta_w_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics,activations,predictions=get_metrics(model,dataloader,criterion,True)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "\n",
    "    return activations,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub using NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=predicted_scale\n",
    "direction = get_delta_w_dict(delta_w,model)\n",
    "\n",
    "model_scrub = copy.deepcopy(model)\n",
    "for k,p in model_scrub.named_parameters():\n",
    "    p.data += (direction[k]*scale).to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune and Fisher Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg\n",
    "\n",
    "def run_neggrad_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    forget_loader: torch.utils.data.DataLoader,\n",
    "                    alpha: float,\n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    quiet=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, (batch_retain,batch_forget) in enumerate(tqdm(zip(data_loader,cycle(forget_loader)), leave=False)):\n",
    "            batch_retain = [tensor.to(next(model.parameters()).device) for tensor in batch_retain]\n",
    "            batch_forget = [tensor.to(next(model.parameters()).device) for tensor in batch_forget]\n",
    "            input_r, target_r = batch_retain\n",
    "            input_f, target_f = batch_forget\n",
    "            output_r = model(input_r)\n",
    "            output_f = model(input_f)\n",
    "            loss = alpha*(loss_fn(output_r, target_r) + l2_penalty(model,model_init,args.weight_decay)) - (1-alpha)*loss_fn(output_f, target_f)\n",
    "            metrics.update(n=input_r.size(0), loss=loss_fn(output_r,target_r).item(), error=get_error(output_r, target_r))\n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def negative_grad(model: nn.Module, data_loader: torch.utils.data.DataLoader, forget_loader: torch.utils.data.DataLoader, alpha: float, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_neggrad_epoch(model, model_init, data_loader, forget_loader, alpha, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "\n",
    "def fk_fientune(model: nn.Module, data_loader: torch.utils.data.DataLoader, args, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.1, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computes the amount of information not forgotten at all layers using the given alpha\n",
    "alpha = 1e-7\n",
    "total_kl = 0\n",
    "torch.manual_seed(seed)\n",
    "for (k, p), (k0, p0) in zip(modelf.named_parameters(), modelf0.named_parameters()):\n",
    "    mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "    mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "    kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "    total_kl += kl\n",
    "    print(k, f'{kl:.1f}')\n",
    "print(\"Total:\", total_kl)\n",
    "log_dict['fisher_info']=total_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-6\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())\n",
    "\n",
    "for i, p in enumerate(modelf0.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test(modelf, retain_loader))\n",
    "print(test(modelf, forget_loader))\n",
    "print(test(modelf, valid_loader_full))\n",
    "print(test(modelf, test_loader_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.ng_alpha = 0.95\n",
    "args.ng_epochs = 10\n",
    "args.ng_lr = 0.01\n",
    "model_ng = copy.deepcopy(model)    \n",
    "negative_grad(model_ng, retain_loader, forget_loader, alpha=args.ng_alpha, epochs=args.ng_epochs, quiet=True, lr=args.ng_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catastrophic Forgetting k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.cfk_lr = 0.01\n",
    "args.cfk_epochs = 10\n",
    "\n",
    "model_cfk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_cfk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    layers = [9]\n",
    "    for k in layers:\n",
    "        for param in model_cfk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    for param in model_cfk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "fk_fientune(model_cfk, retain_loader, args=args, epochs=args.cfk_epochs, quiet=True, lr=args.cfk_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Unlearning k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The last block and classifier of resnet-18\n",
    "(layer4): Sequential(\n",
    "    (0): _ResBlock(\n",
    "      (bn1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(102, 204, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (shortcut): Sequential(\n",
    "        (0): Conv2d(102, 204, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "      )\n",
    "    )\n",
    "    (1): _ResBlock(\n",
    "      (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    )\n",
    "  )\n",
    "(linear): Linear(in_features=204, out_features=5, bias=True)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" The last block and classifier of allcnn\n",
    "AllCNN(\n",
    "  (features): Sequential(\n",
    "    ...\n",
    "    (9): Conv(\n",
    "      (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (2): ReLU()\n",
    "    )\n",
    "    (10): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
    "    (11): Flatten()\n",
    "  )\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=192, out_features=5, bias=True)\n",
    "  )\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.euk_lr = 0.01\n",
    "args.euk_epochs = training_epochs\n",
    "model_euk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_euk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    with torch.no_grad():\n",
    "        for k in layers:\n",
    "            for i in range(0,3):\n",
    "                try:\n",
    "                    model_euk.features[k][i].weight.copy_(model_initial.features[k][i].weight)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have weights\".format(k,i))\n",
    "                try:\n",
    "                    model_euk.features[k][i].bias.copy_(model_initial.features[k][i].bias)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have bias\".format(k,i))\n",
    "        model_euk.classifier[0].weight.copy_(model_initial.classifier[0].weight)\n",
    "        model_euk.classifier[0].bias.copy_(model_initial.classifier[0].bias)\n",
    "    \n",
    "    for k in layers:\n",
    "        for param in model_euk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,2):\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.weight.copy_(model_initial.layer4[i].bn1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.bias.copy_(model_initial.layer4[i].bn1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.weight.copy_(model_initial.layer4[i].conv1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.bias.copy_(model_initial.layer4[i].conv1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.weight.copy_(model_initial.layer4[i].bn2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.bias.copy_(model_initial.layer4[i].bn2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.weight.copy_(model_initial.layer4[i].conv2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.bias.copy_(model_initial.layer4[i].conv2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "        model_euk.layer4[0].shortcut[0].weight.copy_(model_initial.layer4[0].shortcut[0].weight)\n",
    "        \n",
    "    for param in model_euk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "fk_fientune(model_euk, retain_loader, epochs=args.euk_epochs, quiet=True, lr=args.euk_lr, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "_,_=activations_predictions(copy.deepcopy(model),forget_loader,'Original_Model_D_f')\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"c\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_ng),thresh,'NegGrad')\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model_cfk),thresh,'CF-k')\n",
    "readouts[\"f\"] = all_readouts(copy.deepcopy(model_euk),thresh,'EU-k')\n",
    "readouts[\"g\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"h\"] = all_readouts(copy.deepcopy(model_scrub),thresh,'NTK')\n",
    "readouts[\"i\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
