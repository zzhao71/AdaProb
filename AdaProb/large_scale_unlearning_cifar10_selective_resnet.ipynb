{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13910/3351334044.py:20: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import variational\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activations along the path\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datasets\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def entropy(p, dim = -1, keepdim = False):\n",
    "    return -torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim=dim, keepdim=keepdim)\n",
    "    #return (p * p.log()).sum(dim=dim, keepdim=keepdim)\n",
    "\n",
    "def collect_prob(data_loader, model):\n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=False, num_samples=4000)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, sampler=sampler, shuffle=False)\n",
    "    prob = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(data_loader_small, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            data, target = batch\n",
    "            output = model(data)\n",
    "            prob.append(F.softmax(output, dim=-1).data)\n",
    "    return torch.cat(prob)\n",
    "\n",
    "def get_membership_attack_data(retain_loader, forget_loader, test_loader, model):    \n",
    "    retain_prob = collect_prob(retain_loader, model)\n",
    "    forget_prob = collect_prob(forget_loader, model)\n",
    "    test_prob = collect_prob(test_loader, model)\n",
    "    \n",
    "    X_r = torch.cat([entropy(retain_prob), entropy(test_prob)]).cpu().numpy().reshape(-1, 1)\n",
    "    Y_r = np.concatenate([np.ones(len(retain_prob)), np.zeros(len(test_prob))])\n",
    "    \n",
    "    X_f = entropy(forget_prob).cpu().numpy().reshape(-1, 1)\n",
    "    Y_f = np.concatenate([np.ones(len(forget_prob))])    \n",
    "    return X_f, Y_f, X_r, Y_r\n",
    "\n",
    "def get_membership_attack_prob(retain_loader, forget_loader, test_loader, model):\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader, forget_loader, test_loader, model)\n",
    "    #clf = SVC(C=3,gamma='auto',kernel='rbf')\n",
    "    clf = LogisticRegression(class_weight='balanced',solver='lbfgs',multi_class='multinomial')\n",
    "    clf.fit(X_r, Y_r)\n",
    "    results = clf.predict(X_f)\n",
    "    results1 = clf.predict(X_r)\n",
    "    acc = accuracy_score(results, Y_f)\n",
    "    train_ac = accuracy_score(results1, Y_r)\n",
    "    TP, FP, TN, FN = perf_measure(Y_r, results1)\n",
    "    FPR = FP/(FP+TN)\n",
    "    FNR = FN/(FN+TP)\n",
    "    \n",
    "    print (f\"TP:{TP}, FP{FP}, TN{TN}, FN{FN}\")\n",
    "    print (f\"false negative rate: {FN/(FN+TP)}\")\n",
    "    print (f\"false positive rate: {FP/(FP+TN)}\")\n",
    "    return acc, train_acc, FPR, FNR #results.mean(), results1.mean()\n",
    "    \n",
    "def plot_entropy_dist(model,retain_loader,forget_loader,test_loader_full, title, ax):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from matplotlib.ticker import FuncFormatter\n",
    "    #train_loader_full, valid_loader_full, test_loader_full = datasets.get_loaders(dataset, batch_size=100, seed=0, augment=False, shuffle=False)\n",
    "    #indexes = np.flatnonzero(np.array(train_loader_full.dataset.targets) == class_to_forget)\n",
    "    #replaced = np.random.RandomState(0).choice(indexes, size=100 if num_to_forget==100 else len(indexes), replace=False)\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader,forget_loader,test_loader_full, model)\n",
    "    #np.savetxt('retain_prob.txt', X_r[Y_r==1])\n",
    "    #np.savetxt('forget_prob.txt', X_f)\n",
    "    #np.savetxt('test_prob.txt', X_r[Y_r==0])\n",
    "    sns.distplot(np.log(X_r[Y_r==1]).reshape(-1), kde=False, norm_hist=True, rug=False, label='retain', ax=plt)\n",
    "    sns.distplot(np.log(X_r[Y_r==0]).reshape(-1), kde=False, norm_hist=True, rug=False, label='test', ax=plt)\n",
    "    sns.distplot(np.log(X_f).reshape(-1), kde=False, norm_hist=True, rug=False, label='forget', ax=plt)\n",
    "    plt.legend(prop={'size': 14})\n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.title(title,size=18)\n",
    "    plt.xlabel('Log of Entropy',size=14)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def membership_attack(retain_loader,forget_loader,test_loader,model, name):\n",
    "    prob, train_acc, FPR, FNR = get_membership_attack_prob(retain_loader,forget_loader,test_loader,model)\n",
    "    print(\"Attack prob: \", prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=512, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=512, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics=get_metrics(model,dataloader,criterion,False)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "    return _,_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        #run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.01, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    #print(f\"{name} ->\"\n",
    "    #      f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_activations(model_scrubf, modelf0, delta_w_s, delta_w_m0, data_loader, \\\n",
    "                    loss_fn=nn.CrossEntropyLoss(),\\\n",
    "                    optimizer=torch.optim.SGD, \\\n",
    "                    seed=1,quiet=False):\n",
    "\n",
    "    model_scrubf.eval()\n",
    "    modelf0.eval()\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    \n",
    "    metrics = AverageMeter()    \n",
    "    num_classes = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "        batch = [tensor.to(next(model_scrubf.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        \n",
    "        output_sf = model_scrubf(input)\n",
    "        G_sf = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=True)\n",
    "            grads = torch.cat([g.view(-1) for g in grads])\n",
    "            G_sf.append(grads)\n",
    "\n",
    "        grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_sf = torch.stack(G_sf)#.pow(2)\n",
    "        delta_f_sf_update = torch.matmul(G_sf,delta_w_s.sqrt()*torch.empty_like(delta_w_s).normal_())\n",
    "        G_sf = G_sf.pow(2)\n",
    "        delta_f_sf = torch.matmul(G_sf,delta_w_s)\n",
    "\n",
    "        output_m0 = modelf0(input)\n",
    "        G_m0 = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=True)\n",
    "            grad_m0 = torch.cat([g.view(-1) for g in grads])\n",
    "            G_m0.append(grad_m0)\n",
    "\n",
    "        grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_m0 = torch.stack(G_m0).pow(2)\n",
    "        delta_f_m0 = torch.matmul(G_m0,delta_w_m0)\n",
    "        \n",
    "        kl = ((output_m0 - output_sf).pow(2)/delta_f_m0 + delta_f_sf/delta_f_m0 - torch.log(delta_f_sf/delta_f_m0) - 1).sum()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        output_sf += delta_f_sf_update#delta_f_sf.sqrt()*torch.empty_like(delta_f_sf).normal_()\n",
    "        \n",
    "        loss = loss_fn(output_sf, target)\n",
    "        metrics.update(n=input.size(0), loss=loss.item(), error=get_error(output_sf, target), kl=kl.item())\n",
    "    \n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar100_resnet_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar100_resnet_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 100\n",
      "[0] train metrics:{\"loss\": 4.110220985412598, \"error\": 0.9239}\n",
      "Learning Rate : 0.1\n",
      "[0] dry_run metrics:{\"loss\": 3.7692216533660887, \"error\": 0.87745}\n",
      "Learning Rate : 0.1\n",
      "[0] test metrics:{\"loss\": 3.779144535446167, \"error\": 0.8744}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 13.21 sec\n",
      "[1] train metrics:{\"loss\": 3.6166512355804445, \"error\": 0.84515}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.04 sec\n",
      "[2] train metrics:{\"loss\": 3.262185221862793, \"error\": 0.767475}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.79 sec\n",
      "[3] train metrics:{\"loss\": 2.9241571712493895, \"error\": 0.6889}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.83 sec\n",
      "[4] train metrics:{\"loss\": 2.6573989303588865, \"error\": 0.61595}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.71 sec\n",
      "[5] train metrics:{\"loss\": 2.456750422286987, \"error\": 0.55295}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.71 sec\n",
      "[6] train metrics:{\"loss\": 2.316490512084961, \"error\": 0.494525}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.76 sec\n",
      "[7] train metrics:{\"loss\": 2.2176277866363527, \"error\": 0.44985}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.82 sec\n",
      "[8] train metrics:{\"loss\": 2.1540924285888674, \"error\": 0.404375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.6 sec\n",
      "[9] train metrics:{\"loss\": 2.1174500984191895, \"error\": 0.37015}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.7 sec\n",
      "[10] train metrics:{\"loss\": 2.0962327156066896, \"error\": 0.33205}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.77 sec\n",
      "[11] train metrics:{\"loss\": 2.089509313583374, \"error\": 0.2955}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.68 sec\n",
      "[12] train metrics:{\"loss\": 2.1187580627441407, \"error\": 0.27275}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.73 sec\n",
      "[13] train metrics:{\"loss\": 2.1520710746765137, \"error\": 0.2503}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.58 sec\n",
      "[14] train metrics:{\"loss\": 2.1659040351867676, \"error\": 0.222975}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.33 sec\n",
      "[15] train metrics:{\"loss\": 2.180283085632324, \"error\": 0.202875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.74 sec\n",
      "[16] train metrics:{\"loss\": 2.237203318405151, \"error\": 0.195875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.76 sec\n",
      "[17] train metrics:{\"loss\": 2.256527083206177, \"error\": 0.18455}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.24 sec\n",
      "[18] train metrics:{\"loss\": 2.3006882884979247, \"error\": 0.180475}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.76 sec\n",
      "[19] train metrics:{\"loss\": 2.2771842372894286, \"error\": 0.164375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.7 sec\n",
      "[20] train metrics:{\"loss\": 2.332470198059082, \"error\": 0.1652}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 8.15 sec\n",
      "[21] train metrics:{\"loss\": 2.330265349960327, \"error\": 0.1593}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.76 sec\n",
      "[22] train metrics:{\"loss\": 2.4015751735687254, \"error\": 0.164125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.3 sec\n",
      "[23] train metrics:{\"loss\": 2.3445411685943602, \"error\": 0.1472}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.08 sec\n",
      "[24] train metrics:{\"loss\": 2.3646815967559816, \"error\": 0.151725}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.76 sec\n",
      "[25] train metrics:{\"loss\": 2.3665282775878906, \"error\": 0.14435}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 8.1 sec\n",
      "[26] train metrics:{\"loss\": 2.345229354476929, \"error\": 0.14115}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.84 sec\n",
      "[27] train metrics:{\"loss\": 2.3579494285583498, \"error\": 0.142225}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.92 sec\n",
      "[28] train metrics:{\"loss\": 2.3899746318817137, \"error\": 0.1456}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.77 sec\n",
      "[29] train metrics:{\"loss\": 2.4067670585632324, \"error\": 0.145975}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.78 sec\n",
      "[30] train metrics:{\"loss\": 2.4091349899291994, \"error\": 0.14255}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.81 sec\n",
      "Pure training time: 216.63 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset cifar100 --dataroot=data/cifar100 --model resnet --filters 1.0 --lr 0.1 --lossfn ce --num-classes 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar10_resnet_1_0_forget_None_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar10_resnet_1_0_forget_None_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Number of Classes: 10\n",
      "Epoch: [0][0/313]\tTime 0.058 (0.058)\tData 0.026 (0.026)\tLoss 2.4113 (2.4113)\tAcc@1 10.156 (10.156)\tAcc@5 57.031 (57.031)\n",
      " * Acc@1 68.905 Acc@5 97.155\n",
      "[0] test metrics:{\"loss\": 0.6794950475692749, \"error\": 0.2376}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.46 sec\n",
      "Epoch: [1][0/313]\tTime 0.025 (0.025)\tData 0.016 (0.016)\tLoss 0.5532 (0.5532)\tAcc@1 80.469 (80.469)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 81.650 Acc@5 99.257\n",
      "Epoch Time: 5.13 sec\n",
      "Epoch: [2][0/313]\tTime 0.029 (0.029)\tData 0.020 (0.020)\tLoss 0.4603 (0.4603)\tAcc@1 83.594 (83.594)\tAcc@5 98.438 (98.438)\n",
      " * Acc@1 87.710 Acc@5 99.645\n",
      "Epoch Time: 5.16 sec\n",
      "Epoch: [3][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.2124 (0.2124)\tAcc@1 92.969 (92.969)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 92.802 Acc@5 99.875\n",
      "Epoch Time: 5.26 sec\n",
      "Epoch: [4][0/313]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.1803 (0.1803)\tAcc@1 92.969 (92.969)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 97.130 Acc@5 99.962\n",
      "Epoch Time: 5.16 sec\n",
      "Epoch: [5][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0355 (0.0355)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.290 Acc@5 99.997\n",
      "[5] test metrics:{\"loss\": 0.7918343296051026, \"error\": 0.1845}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.23 sec\n",
      "Epoch: [6][0/313]\tTime 0.027 (0.027)\tData 0.019 (0.019)\tLoss 0.0267 (0.0267)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.885 Acc@5 100.000\n",
      "Epoch Time: 5.02 sec\n",
      "Epoch: [7][0/313]\tTime 0.018 (0.018)\tData 0.007 (0.007)\tLoss 0.0203 (0.0203)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.965 Acc@5 100.000\n",
      "Epoch Time: 5.05 sec\n",
      "Epoch: [8][0/313]\tTime 0.016 (0.016)\tData 0.007 (0.007)\tLoss 0.0051 (0.0051)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.987 Acc@5 100.000\n",
      "Epoch Time: 5.08 sec\n",
      "Epoch: [9][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0040 (0.0040)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "Epoch Time: 5.03 sec\n",
      "Epoch: [10][0/313]\tTime 0.026 (0.026)\tData 0.017 (0.017)\tLoss 0.0034 (0.0034)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "[10] test metrics:{\"loss\": 0.9320603881835937, \"error\": 0.1763}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.2 sec\n",
      "Epoch: [11][0/313]\tTime 0.021 (0.021)\tData 0.013 (0.013)\tLoss 0.0033 (0.0033)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "Epoch Time: 5.01 sec\n",
      "Epoch: [12][0/313]\tTime 0.019 (0.019)\tData 0.010 (0.010)\tLoss 0.0034 (0.0034)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.01 sec\n",
      "Epoch: [13][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0033 (0.0033)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.01 sec\n",
      "Epoch: [14][0/313]\tTime 0.020 (0.020)\tData 0.013 (0.013)\tLoss 0.0029 (0.0029)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 5.04 sec\n",
      "Epoch: [15][0/313]\tTime 0.025 (0.025)\tData 0.017 (0.017)\tLoss 0.0026 (0.0026)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "[15] test metrics:{\"loss\": 0.9978206403732299, \"error\": 0.1747}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.2 sec\n",
      "Epoch: [16][0/313]\tTime 0.025 (0.025)\tData 0.016 (0.016)\tLoss 0.0074 (0.0074)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.09 sec\n",
      "Epoch: [17][0/313]\tTime 0.016 (0.016)\tData 0.007 (0.007)\tLoss 0.0025 (0.0025)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.92 sec\n",
      "Epoch: [18][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0016 (0.0016)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "Epoch Time: 5.21 sec\n",
      "Epoch: [19][0/313]\tTime 0.017 (0.017)\tData 0.010 (0.010)\tLoss 0.0018 (0.0018)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.14 sec\n",
      "Epoch: [20][0/313]\tTime 0.025 (0.025)\tData 0.017 (0.017)\tLoss 0.0016 (0.0016)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "[20] test metrics:{\"loss\": 1.064859248161316, \"error\": 0.1747}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.45 sec\n",
      "Epoch: [21][0/313]\tTime 0.020 (0.020)\tData 0.012 (0.012)\tLoss 0.0025 (0.0025)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.18 sec\n",
      "Epoch: [22][0/313]\tTime 0.018 (0.018)\tData 0.010 (0.010)\tLoss 0.0023 (0.0023)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.22 sec\n",
      "Epoch: [23][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0027 (0.0027)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.23 sec\n",
      "Epoch: [24][0/313]\tTime 0.027 (0.027)\tData 0.019 (0.019)\tLoss 0.0028 (0.0028)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.18 sec\n",
      "Epoch: [25][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0017 (0.0017)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[25] test metrics:{\"loss\": 1.1471838713645934, \"error\": 0.1728}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.4 sec\n",
      "Pure training time: 133.19 sec\n"
     ]
    }
   ],
   "source": [
    "%run main_merged.py --dataset cifar10 --model resnet --dataroot=data/cifar10/ --filters 1.0 --lr 0.01 \\\n",
    "--resume checkpoints/cifar100_resnet_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 5e-4 --batch-size 128 --epochs 26 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar10_resnet_1_0_forget_[5]_num_100_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar10_resnet_1_0_forget_[5]_num_100_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [22644  6987 36335 23839  1559  5648 18421  4310 12085 10658 22192  2404\n",
      " 28077 23062 33361 18035 28713 24244 36749 30401 34869 14428  9997 10500\n",
      "  9429  5670 31993 23103 36858 37665 18393   364 10553 33173 11641 18201\n",
      "  5622 34816  9309 26329 22289 20137 25274 32709 29643 36973 38557 16388\n",
      " 26011 18122 13663  3617 33612 19687 13240 11287  6150 38579 21265 25977\n",
      "  2308 39186 16293 31764  6557 31314  9730  5069 15650 14230 20863 17632\n",
      "  3979 30893 11810 32154 25290 37680  9940 31707 12928 15234 22597 35439\n",
      " 33722 14472  9259 15288  1960  3135 22250 26104 26497 32327   386 19083\n",
      " 37138 13509 39902  7136]\n",
      "Number of Classes: 10\n",
      "Epoch: [0][0/313]\tTime 0.040 (0.040)\tData 0.025 (0.025)\tLoss 2.4173 (2.4173)\tAcc@1 10.938 (10.938)\tAcc@5 56.250 (56.250)\n",
      " * Acc@1 68.917 Acc@5 97.175\n",
      "[0] test metrics:{\"loss\": 0.6776201425552368, \"error\": 0.2363}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.48 sec\n",
      "Epoch: [1][0/313]\tTime 0.020 (0.020)\tData 0.012 (0.012)\tLoss 0.5485 (0.5485)\tAcc@1 80.469 (80.469)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 81.680 Acc@5 99.257\n",
      "Epoch Time: 5.09 sec\n",
      "Epoch: [2][0/313]\tTime 0.016 (0.016)\tData 0.008 (0.008)\tLoss 0.4472 (0.4472)\tAcc@1 83.594 (83.594)\tAcc@5 98.438 (98.438)\n",
      " * Acc@1 87.750 Acc@5 99.640\n",
      "Epoch Time: 5.0 sec\n",
      "Epoch: [3][0/313]\tTime 0.021 (0.021)\tData 0.013 (0.013)\tLoss 0.2127 (0.2127)\tAcc@1 93.750 (93.750)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 92.882 Acc@5 99.870\n",
      "Epoch Time: 5.05 sec\n",
      "Epoch: [4][0/313]\tTime 0.028 (0.028)\tData 0.018 (0.018)\tLoss 0.1807 (0.1807)\tAcc@1 92.969 (92.969)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 97.152 Acc@5 99.962\n",
      "Epoch Time: 5.06 sec\n",
      "Epoch: [5][0/313]\tTime 0.017 (0.017)\tData 0.007 (0.007)\tLoss 0.0326 (0.0326)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.275 Acc@5 99.997\n",
      "[5] test metrics:{\"loss\": 0.7923706416130066, \"error\": 0.1852}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.16 sec\n",
      "Epoch: [6][0/313]\tTime 0.029 (0.029)\tData 0.020 (0.020)\tLoss 0.0267 (0.0267)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.887 Acc@5 100.000\n",
      "Epoch Time: 5.03 sec\n",
      "Epoch: [7][0/313]\tTime 0.017 (0.017)\tData 0.007 (0.007)\tLoss 0.0206 (0.0206)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.955 Acc@5 100.000\n",
      "Epoch Time: 5.04 sec\n",
      "Epoch: [8][0/313]\tTime 0.023 (0.023)\tData 0.014 (0.014)\tLoss 0.0048 (0.0048)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.987 Acc@5 100.000\n",
      "Epoch Time: 5.03 sec\n",
      "Epoch: [9][0/313]\tTime 0.020 (0.020)\tData 0.011 (0.011)\tLoss 0.0041 (0.0041)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.993 Acc@5 100.000\n",
      "Epoch Time: 5.05 sec\n",
      "Epoch: [10][0/313]\tTime 0.017 (0.017)\tData 0.008 (0.008)\tLoss 0.0034 (0.0034)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.993 Acc@5 100.000\n",
      "[10] test metrics:{\"loss\": 0.9332153671264648, \"error\": 0.1769}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.11 sec\n",
      "Epoch: [11][0/313]\tTime 0.018 (0.018)\tData 0.010 (0.010)\tLoss 0.0033 (0.0033)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "Epoch Time: 5.08 sec\n",
      "Epoch: [12][0/313]\tTime 0.019 (0.019)\tData 0.011 (0.011)\tLoss 0.0033 (0.0033)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.06 sec\n",
      "Epoch: [13][0/313]\tTime 0.016 (0.016)\tData 0.008 (0.008)\tLoss 0.0034 (0.0034)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.01 sec\n",
      "Epoch: [14][0/313]\tTime 0.016 (0.016)\tData 0.007 (0.007)\tLoss 0.0031 (0.0031)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 4.97 sec\n",
      "Epoch: [15][0/313]\tTime 0.016 (0.016)\tData 0.008 (0.008)\tLoss 0.0027 (0.0027)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "[15] test metrics:{\"loss\": 0.9991885855674744, \"error\": 0.175}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.05 sec\n",
      "Epoch: [16][0/313]\tTime 0.037 (0.037)\tData 0.028 (0.028)\tLoss 0.0064 (0.0064)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.98 sec\n",
      "Epoch: [17][0/313]\tTime 0.028 (0.028)\tData 0.019 (0.019)\tLoss 0.0024 (0.0024)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.94 sec\n",
      "Epoch: [18][0/313]\tTime 0.016 (0.016)\tData 0.007 (0.007)\tLoss 0.0015 (0.0015)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "Epoch Time: 4.99 sec\n",
      "Epoch: [19][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0018 (0.0018)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.28 sec\n",
      "Epoch: [20][0/313]\tTime 0.017 (0.017)\tData 0.007 (0.007)\tLoss 0.0016 (0.0016)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "[20] test metrics:{\"loss\": 1.066207071876526, \"error\": 0.175}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.34 sec\n",
      "Epoch: [21][0/313]\tTime 0.025 (0.025)\tData 0.016 (0.016)\tLoss 0.0026 (0.0026)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.3 sec\n",
      "Epoch: [22][0/313]\tTime 0.017 (0.017)\tData 0.007 (0.007)\tLoss 0.0023 (0.0023)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.19 sec\n",
      "Epoch: [23][0/313]\tTime 0.020 (0.020)\tData 0.012 (0.012)\tLoss 0.0027 (0.0027)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.1 sec\n",
      "Epoch: [24][0/313]\tTime 0.026 (0.026)\tData 0.019 (0.019)\tLoss 0.0029 (0.0029)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 5.15 sec\n",
      "Epoch: [25][0/313]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0018 (0.0018)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[25] test metrics:{\"loss\": 1.1482664091110228, \"error\": 0.1752}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 6.21 sec\n",
      "Pure training time: 131.93 sec\n"
     ]
    }
   ],
   "source": [
    "%run main_merged.py --dataset cifar10 --model resnet --dataroot=data/cifar10/ --filters 1 --lr 0.01 \\\n",
    "--resume checkpoints/cifar100_resnet_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 5e-4 --batch-size 128 --epochs 26 \\\n",
    "--forget-class 5 --num-to-forget 100 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 11175178\n"
     ]
    }
   ],
   "source": [
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model0 = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 4000#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model0.cuda()\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [22644  6987 36335 23839  1559  5648 18421  4310 12085 10658 22192  2404\n",
      " 28077 23062 33361 18035 28713 24244 36749 30401 34869 14428  9997 10500\n",
      "  9429  5670 31993 23103 36858 37665 18393   364 10553 33173 11641 18201\n",
      "  5622 34816  9309 26329 22289 20137 25274 32709 29643 36973 38557 16388\n",
      " 26011 18122 13663  3617 33612 19687 13240 11287  6150 38579 21265 25977\n",
      "  2308 39186 16293 31764  6557 31314  9730  5069 15650 14230 20863 17632\n",
      "  3979 30893 11810 32154 25290 37680  9940 31707 12928 15234 22597 35439\n",
      " 33722 14472  9259 15288  1960  3135 22250 26104 26497 32327   386 19083\n",
      " 37138 13509 39902  7136]\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _  = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=512, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=256, seed=seed, shuffle=True)\n",
    "\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "39900\n",
      "10000\n",
      "40000\n",
      "{6: 4000, 9: 4000, 4: 4000, 1: 4000, 2: 4000, 8: 4000, 3: 4000, 7: 4000, 5: 4000, 0: 4000}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'sgd'\n",
    "args.gamma = 0.99\n",
    "args.alpha = 0.001\n",
    "args.beta = 0\n",
    "args.smoothing = 0.0\n",
    "args.msteps = 5\n",
    "args.clip = 0.2\n",
    "args.sstart = 5\n",
    "args.kd_T = 4\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_batch_size = 64\n",
    "args.del_batch_size = 16\n",
    "args.sgda_epochs = 2\n",
    "args.sgda_learning_rate = 0.005\n",
    "args.lr_decay_epochs = [3,5,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> SCRUB unlearning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihao/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 98.138 \n",
      "maximize loss: -27.22\t minimize loss: 0.11\t train_acc: 98.1378402709961\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 96.947 \n",
      "maximize loss: -93.53\t minimize loss: 0.17\t train_acc: 96.9473648071289\n"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "acc_vs = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> SCRUB unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True, prefix=\"train_cifar10_selective_resnet\")\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True, prefix=\"forget_cifar10_selective_resnet\")\n",
    "    acc_v, acc5_v, loss_v = validate(valid_loader_full, model_s, criterion_cls, args, True)\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "    acc_vs.append(100-acc_v.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\")\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    \n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n",
    "acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "acc_v, acc5_v, loss_v = validate(valid_loader_full, model_s, criterion_cls, args, True)\n",
    "acc_rs.append(100-acc_r.item())\n",
    "acc_fs.append(100-acc_f.item())\n",
    "acc_vs.append(100-acc_v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.plot(indices, acc_vs, marker='.', alpha=1, label='valid-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('SCRUB retain-, valid- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Model_D_r -> Loss:0.001, Error:2.506265664160401e-05\n",
      "Original_Model_D_f -> Loss:0.001, Error:0.0\n",
      "Original_Model_D_t -> Loss:0.779, Error:0.1728\n"
     ]
    }
   ],
   "source": [
    "m_D_r_activations,m_D_r_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(retain_loader),'Original_Model_D_r')\n",
    "m_D_f_activations,m_D_f_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(forget_loader),'Original_Model_D_f')\n",
    "m_D_t_activations,m_D_t_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(test_loader_full),'Original_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m m0_D_r_activations,m0_D_r_predictions\u001b[38;5;241m=\u001b[39m\u001b[43mactivations_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRetrain_Model_D_r\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m m0_D_f_activations,m0_D_f_predictions\u001b[38;5;241m=\u001b[39mactivations_predictions(copy\u001b[38;5;241m.\u001b[39mdeepcopy(model0),copy\u001b[38;5;241m.\u001b[39mdeepcopy(forget_loader),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrain_Model_D_f\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m m0_D_t_activations,m0_D_t_predictions\u001b[38;5;241m=\u001b[39mactivations_predictions(copy\u001b[38;5;241m.\u001b[39mdeepcopy(model0),copy\u001b[38;5;241m.\u001b[39mdeepcopy(test_loader_full),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrain_Model_D_t\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36mactivations_predictions\u001b[0;34m(model, dataloader, name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactivations_predictions\u001b[39m(model,dataloader,name):\n\u001b[1;32m      2\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 3\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> Loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mround(metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Error:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     log_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m, in \u001b[0;36mget_metrics\u001b[0;34m(model, dataloader, criterion, samples_correctness, use_bn, delta_w, scrub_act)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdataset:\n\u001b[1;32m     22\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mview(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scrub_act:\n\u001b[1;32m     25\u001b[0m     G \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/data/SCRUB-main/models.py:346\u001b[0m, in \u001b[0;36mResNet18.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    344\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(out)\n\u001b[1;32m    345\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(out)\n\u001b[0;32m--> 346\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(out)\n\u001b[1;32m    348\u001b[0m out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mavg_pool2d(out, \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/data/SCRUB-main/models.py:317\u001b[0m, in \u001b[0;36m_ResBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    315\u001b[0m shortcut \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut(out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortcut\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    316\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(out)\n\u001b[0;32m--> 317\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    318\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m shortcut\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m0_D_r_activations,m0_D_r_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(retain_loader),'Retrain_Model_D_r')\n",
    "m0_D_f_activations,m0_D_f_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(forget_loader),'Retrain_Model_D_f')\n",
    "m0_D_t_activations,m0_D_t_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(test_loader_full),'Retrain_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRUB_D_r -> Loss:0.007, Error:0.0013784461152882206\n",
      "SCRUB_D_f -> Loss:19.092, Error:0.74\n",
      "SCRUB_D_t -> Loss:1.112, Error:0.1887\n"
     ]
    }
   ],
   "source": [
    "ntk_D_r_activations,ntk_D_r_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(retain_loader),'SCRUB_D_r')\n",
    "ntk_D_f_activations,ntk_D_f_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(forget_loader),'SCRUB_D_f')\n",
    "ntk_D_t_activations,ntk_D_t_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(test_loader_full),'SCRUB_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, modelf)\n",
    "#hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-7\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_D_r_activations,fisher_D_r_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(retain_loader),'Fisher_D_r')\n",
    "fisher_D_f_activations,fisher_D_f_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(forget_loader),'Fisher_D_f')\n",
    "fisher_D_t_activations,fisher_D_t_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(test_loader_full),'Fisher_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information in the Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_D_r_activations,finetune_D_r_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(retain_loader),'Finetune_D_r')\n",
    "finetune_D_f_activations,finetune_D_f_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(forget_loader),'Finetune_D_f')\n",
    "finetune_D_t_activations,finetune_D_t_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(test_loader_full),'Finetune_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(5,1,figsize=(9,20))\n",
    "ax = [0,0,0,0,0]\n",
    "plot_entropy_dist(copy.deepcopy(model_s),retain_loader,forget_loader,test_loader_full, 'SCRUB',ax[4])\n",
    "plot_entropy_dist(copy.deepcopy(model),retain_loader,forget_loader,test_loader_full, 'original', ax[0])\n",
    "plot_entropy_dist(copy.deepcopy(model0),retain_loader,forget_loader,test_loader_full, 'retrain', ax[1])\n",
    "plot_entropy_dist(copy.deepcopy(model_ft),retain_loader,forget_loader,test_loader_full, 'finetuned',ax[2])\n",
    "plot_entropy_dist(copy.deepcopy(modelf),retain_loader,forget_loader,test_loader_full, 'fisher',ax[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
