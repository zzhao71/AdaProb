{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18547/3351334044.py:20: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import variational\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activations along the path\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datasets\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def entropy(p, dim = -1, keepdim = False):\n",
    "    return -torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim=dim, keepdim=keepdim)\n",
    "    #return (p * p.log()).sum(dim=dim, keepdim=keepdim)\n",
    "\n",
    "def collect_prob(data_loader, model):\n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=False, num_samples=4000)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, sampler=sampler, shuffle=False)\n",
    "    prob = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(data_loader_small, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            data, target = batch\n",
    "            output = model(data)\n",
    "            prob.append(F.softmax(output, dim=-1).data)\n",
    "    return torch.cat(prob)\n",
    "\n",
    "def get_membership_attack_data(retain_loader, forget_loader, test_loader, model):    \n",
    "    retain_prob = collect_prob(retain_loader, model)\n",
    "    forget_prob = collect_prob(forget_loader, model)\n",
    "    test_prob = collect_prob(test_loader, model)\n",
    "    \n",
    "    X_r = torch.cat([entropy(retain_prob), entropy(test_prob)]).cpu().numpy().reshape(-1, 1)\n",
    "    Y_r = np.concatenate([np.ones(len(retain_prob)), np.zeros(len(test_prob))])\n",
    "    \n",
    "    X_f = entropy(forget_prob).cpu().numpy().reshape(-1, 1)\n",
    "    Y_f = np.concatenate([np.ones(len(forget_prob))])    \n",
    "    return X_f, Y_f, X_r, Y_r\n",
    "\n",
    "def get_membership_attack_prob(retain_loader, forget_loader, test_loader, model):\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader, forget_loader, test_loader, model)\n",
    "    #clf = SVC(C=3,gamma='auto',kernel='rbf')\n",
    "    clf = LogisticRegression(class_weight='balanced',solver='lbfgs',multi_class='multinomial')\n",
    "    clf.fit(X_r, Y_r)\n",
    "    results = clf.predict(X_f)\n",
    "    results1 = clf.predict(X_r)\n",
    "    acc = accuracy_score(results, Y_f)\n",
    "    train_ac = accuracy_score(results1, Y_r)\n",
    "    TP, FP, TN, FN = perf_measure(Y_r, results1)\n",
    "    FPR = FP/(FP+TN)\n",
    "    FNR = FN/(FN+TP)\n",
    "    \n",
    "    print (f\"TP:{TP}, FP{FP}, TN{TN}, FN{FN}\")\n",
    "    print (f\"false negative rate: {FN/(FN+TP)}\")\n",
    "    print (f\"false positive rate: {FP/(FP+TN)}\")\n",
    "    return acc, train_acc, FPR, FNR #results.mean(), results1.mean()\n",
    "    \n",
    "def plot_entropy_dist(model,retain_loader,forget_loader,test_loader_full, title, ax):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from matplotlib.ticker import FuncFormatter\n",
    "    #train_loader_full, valid_loader_full, test_loader_full = datasets.get_loaders(dataset, batch_size=100, seed=0, augment=False, shuffle=False)\n",
    "    #indexes = np.flatnonzero(np.array(train_loader_full.dataset.targets) == class_to_forget)\n",
    "    #replaced = np.random.RandomState(0).choice(indexes, size=100 if num_to_forget==100 else len(indexes), replace=False)\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader,forget_loader,test_loader_full, model)\n",
    "    #np.savetxt('retain_prob.txt', X_r[Y_r==1])\n",
    "    #np.savetxt('forget_prob.txt', X_f)\n",
    "    #np.savetxt('test_prob.txt', X_r[Y_r==0])\n",
    "    sns.distplot(np.log(X_r[Y_r==1]).reshape(-1), kde=False, norm_hist=True, rug=False, label='retain', ax=plt)\n",
    "    sns.distplot(np.log(X_r[Y_r==0]).reshape(-1), kde=False, norm_hist=True, rug=False, label='test', ax=plt)\n",
    "    sns.distplot(np.log(X_f).reshape(-1), kde=False, norm_hist=True, rug=False, label='forget', ax=plt)\n",
    "    plt.legend(prop={'size': 14})\n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.title(title,size=18)\n",
    "    plt.xlabel('Log of Entropy',size=14)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def membership_attack(retain_loader,forget_loader,test_loader,model, name):\n",
    "    prob, train_acc, FPR, FNR = get_membership_attack_prob(retain_loader,forget_loader,test_loader,model)\n",
    "    print(\"Attack prob: \", prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=512, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=512, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics=get_metrics(model,dataloader,criterion,False)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "    return _,_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        #run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.01, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    #print(f\"{name} ->\"\n",
    "    #      f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_activations(model_scrubf, modelf0, delta_w_s, delta_w_m0, data_loader, \\\n",
    "                    loss_fn=nn.CrossEntropyLoss(),\\\n",
    "                    optimizer=torch.optim.SGD, \\\n",
    "                    seed=1,quiet=False):\n",
    "\n",
    "    model_scrubf.eval()\n",
    "    modelf0.eval()\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    \n",
    "    metrics = AverageMeter()    \n",
    "    num_classes = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "        batch = [tensor.to(next(model_scrubf.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        \n",
    "        output_sf = model_scrubf(input)\n",
    "        G_sf = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=True)\n",
    "            grads = torch.cat([g.view(-1) for g in grads])\n",
    "            G_sf.append(grads)\n",
    "\n",
    "        grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_sf = torch.stack(G_sf)#.pow(2)\n",
    "        delta_f_sf_update = torch.matmul(G_sf,delta_w_s.sqrt()*torch.empty_like(delta_w_s).normal_())\n",
    "        G_sf = G_sf.pow(2)\n",
    "        delta_f_sf = torch.matmul(G_sf,delta_w_s)\n",
    "\n",
    "        output_m0 = modelf0(input)\n",
    "        G_m0 = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=True)\n",
    "            grad_m0 = torch.cat([g.view(-1) for g in grads])\n",
    "            G_m0.append(grad_m0)\n",
    "\n",
    "        grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_m0 = torch.stack(G_m0).pow(2)\n",
    "        delta_f_m0 = torch.matmul(G_m0,delta_w_m0)\n",
    "        \n",
    "        kl = ((output_m0 - output_sf).pow(2)/delta_f_m0 + delta_f_sf/delta_f_m0 - torch.log(delta_f_sf/delta_f_m0) - 1).sum()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        output_sf += delta_f_sf_update#delta_f_sf.sqrt()*torch.empty_like(delta_f_sf).normal_()\n",
    "        \n",
    "        loss = loss_fn(output_sf, target)\n",
    "        metrics.update(n=input.size(0), loss=loss.item(), error=get_error(output_sf, target), kl=kl.item())\n",
    "    \n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: lacuna100_resnet_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in lacuna100_resnet_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 100\n",
      "[0] train metrics:{\"loss\": 3.987484888076782, \"error\": 0.923}\n",
      "Learning Rate : 0.1\n",
      "[0] dry_run metrics:{\"loss\": 3.395326801300049, \"error\": 0.83653125}\n",
      "Learning Rate : 0.1\n",
      "[0] test metrics:{\"loss\": 3.4219942169189452, \"error\": 0.8447}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 10.43 sec\n",
      "[1] train metrics:{\"loss\": 3.1922737522125244, \"error\": 0.78334375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.71 sec\n",
      "[2] train metrics:{\"loss\": 2.6385717277526854, \"error\": 0.63821875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.65 sec\n",
      "[3] train metrics:{\"loss\": 2.220917667865753, \"error\": 0.51859375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.6 sec\n",
      "[4] train metrics:{\"loss\": 1.9556603002548218, \"error\": 0.41671875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.65 sec\n",
      "[5] train metrics:{\"loss\": 1.722495491027832, \"error\": 0.32259375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.77 sec\n",
      "[6] train metrics:{\"loss\": 1.5956408495903016, \"error\": 0.26075}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.63 sec\n",
      "[7] train metrics:{\"loss\": 1.567377972126007, \"error\": 0.22371875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.71 sec\n",
      "[8] train metrics:{\"loss\": 1.5533827967643739, \"error\": 0.1940625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.69 sec\n",
      "[9] train metrics:{\"loss\": 1.523199393749237, \"error\": 0.16503125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.6 sec\n",
      "[10] train metrics:{\"loss\": 1.531425853729248, \"error\": 0.150625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.8 sec\n",
      "[11] train metrics:{\"loss\": 1.521689467906952, \"error\": 0.1343125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.67 sec\n",
      "[12] train metrics:{\"loss\": 1.5069730429649353, \"error\": 0.1225625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.65 sec\n",
      "[13] train metrics:{\"loss\": 1.5114346590042114, \"error\": 0.12234375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.75 sec\n",
      "[14] train metrics:{\"loss\": 1.517757547855377, \"error\": 0.1201875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.85 sec\n",
      "[15] train metrics:{\"loss\": 1.5435997090339661, \"error\": 0.1189375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.68 sec\n",
      "[16] train metrics:{\"loss\": 1.5590977020263672, \"error\": 0.11846875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.48 sec\n",
      "[17] train metrics:{\"loss\": 1.543018394470215, \"error\": 0.10815625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.65 sec\n",
      "[18] train metrics:{\"loss\": 1.5397232418060303, \"error\": 0.10703125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.65 sec\n",
      "[19] train metrics:{\"loss\": 1.5722574791908264, \"error\": 0.11421875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.57 sec\n",
      "[20] train metrics:{\"loss\": 1.5940280084609986, \"error\": 0.1075}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.68 sec\n",
      "[21] train metrics:{\"loss\": 1.5399379086494447, \"error\": 0.0955625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.84 sec\n",
      "[22] train metrics:{\"loss\": 1.5899784002304078, \"error\": 0.1098125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.65 sec\n",
      "[23] train metrics:{\"loss\": 1.6024198775291443, \"error\": 0.1053125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.72 sec\n",
      "[24] train metrics:{\"loss\": 1.549412570476532, \"error\": 0.09359375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.77 sec\n",
      "[25] train metrics:{\"loss\": 1.580799273967743, \"error\": 0.1025}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.66 sec\n",
      "[26] train metrics:{\"loss\": 1.577813358783722, \"error\": 0.10015625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.62 sec\n",
      "[27] train metrics:{\"loss\": 1.5733731870651244, \"error\": 0.09746875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.81 sec\n",
      "[28] train metrics:{\"loss\": 1.586123302936554, \"error\": 0.10090625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.74 sec\n",
      "[29] train metrics:{\"loss\": 1.5767607717514038, \"error\": 0.094625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.72 sec\n",
      "[30] train metrics:{\"loss\": 1.5994566655158997, \"error\": 0.1015}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.83 sec\n",
      "Pure training time: 176.17000000000007 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run main.py --dataset lacuna100 --dataroot=data/lacuna100 --model resnet --filters 1.0 --lr 0.1 --lossfn ce --num-classes 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: lacuna10_resnet_1_0_forget_None_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in lacuna10_resnet_1_0_forget_None_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Number of Classes: 10\n",
      "Epoch: [0][0/25]\tTime 0.111 (0.111)\tData 0.007 (0.007)\tLoss 0.8869 (0.8869)\tAcc@1 87.500 (87.500)\tAcc@5 99.219 (99.219)\n",
      " * Acc@1 90.781 Acc@5 99.000\n",
      "[0] test metrics:{\"loss\": 0.20946366262435914, \"error\": 0.055999995708465575}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.91 sec\n",
      "Epoch: [1][0/25]\tTime 0.016 (0.016)\tData 0.008 (0.008)\tLoss 0.1985 (0.1985)\tAcc@1 92.969 (92.969)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 95.500 Acc@5 99.750\n",
      "Epoch Time: 0.42 sec\n",
      "Epoch: [2][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0561 (0.0561)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 98.250 Acc@5 99.969\n",
      "Epoch Time: 0.42 sec\n",
      "Epoch: [3][0/25]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0369 (0.0369)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.469 Acc@5 100.000\n",
      "Epoch Time: 0.43 sec\n",
      "Epoch: [4][0/25]\tTime 0.015 (0.015)\tData 0.006 (0.006)\tLoss 0.0171 (0.0171)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.812 Acc@5 100.000\n",
      "Epoch Time: 0.42 sec\n",
      "Epoch: [5][0/25]\tTime 0.017 (0.017)\tData 0.007 (0.007)\tLoss 0.0164 (0.0164)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.969 Acc@5 100.000\n",
      "[5] test metrics:{\"loss\": 0.11269214117527009, \"error\": 0.030999996662139893}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.58 sec\n",
      "Epoch: [6][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0156 (0.0156)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.969 Acc@5 100.000\n",
      "Epoch Time: 0.42 sec\n",
      "Epoch: [7][0/25]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0074 (0.0074)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.969 Acc@5 100.000\n",
      "Epoch Time: 0.44 sec\n",
      "Epoch: [8][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0066 (0.0066)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.42 sec\n",
      "Epoch: [9][0/25]\tTime 0.014 (0.014)\tData 0.006 (0.006)\tLoss 0.0062 (0.0062)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.42 sec\n",
      "Epoch: [10][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0055 (0.0055)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[10] test metrics:{\"loss\": 0.1133830133676529, \"error\": 0.030999996662139893}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.58 sec\n",
      "Epoch: [11][0/25]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0038 (0.0038)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.44 sec\n",
      "Epoch: [12][0/25]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0031 (0.0031)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.43 sec\n",
      "Epoch: [13][0/25]\tTime 0.014 (0.014)\tData 0.006 (0.006)\tLoss 0.0025 (0.0025)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.43 sec\n",
      "Epoch: [14][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0040 (0.0040)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.43 sec\n",
      "Epoch: [15][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0024 (0.0024)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[15] test metrics:{\"loss\": 0.11351716929674148, \"error\": 0.029999996662139893}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.58 sec\n",
      "Epoch: [16][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0025 (0.0025)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.42 sec\n",
      "Epoch: [17][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0029 (0.0029)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.43 sec\n",
      "Epoch: [18][0/25]\tTime 0.014 (0.014)\tData 0.006 (0.006)\tLoss 0.0023 (0.0023)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.42 sec\n",
      "Epoch: [19][0/25]\tTime 0.016 (0.016)\tData 0.006 (0.006)\tLoss 0.0021 (0.0021)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.43 sec\n",
      "Epoch: [20][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0020 (0.0020)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[20] test metrics:{\"loss\": 0.11552823621034622, \"error\": 0.02899999666213989}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.57 sec\n",
      "Epoch: [21][0/25]\tTime 0.015 (0.015)\tData 0.007 (0.007)\tLoss 0.0028 (0.0028)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.42 sec\n",
      "Epoch: [22][0/25]\tTime 0.016 (0.016)\tData 0.006 (0.006)\tLoss 0.0016 (0.0016)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.43 sec\n",
      "Epoch: [23][0/25]\tTime 0.014 (0.014)\tData 0.006 (0.006)\tLoss 0.0015 (0.0015)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.42 sec\n",
      "Epoch: [24][0/25]\tTime 0.014 (0.014)\tData 0.006 (0.006)\tLoss 0.0011 (0.0011)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.43 sec\n",
      "Epoch: [25][0/25]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0014 (0.0014)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[25] test metrics:{\"loss\": 0.11844244855642319, \"error\": 0.027999996662139894}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.59 sec\n",
      "Pure training time: 11.389999999999997 sec\n"
     ]
    }
   ],
   "source": [
    "%run main_merged.py --dataset lacuna10 --model resnet --dataroot=data/lacuna10/ --filters 1.0 --lr 0.01 \\\n",
    "--resume checkpoints/lacuna100_resnet_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 5e-4 --batch-size 128 --epochs 26 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: lacuna10_resnet_1_0_forget_[5]_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in lacuna10_resnet_1_0_forget_[5]_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Number of Classes: 10\n",
      "Epoch: [0][0/25]\tTime 0.019 (0.019)\tData 0.007 (0.007)\tLoss 0.9343 (0.9343)\tAcc@1 82.812 (82.812)\tAcc@5 99.219 (99.219)\n",
      " * Acc@1 90.469 Acc@5 98.969\n",
      "[0] test metrics:{\"loss\": 0.18473745082737877, \"error\": 0.052222222222222225}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.17 sec\n",
      "Epoch: [1][0/25]\tTime 0.016 (0.016)\tData 0.007 (0.007)\tLoss 0.1682 (0.1682)\tAcc@1 94.531 (94.531)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 96.250 Acc@5 99.812\n",
      "Epoch Time: 0.84 sec\n",
      "Epoch: [2][0/25]\tTime 0.025 (0.025)\tData 0.007 (0.007)\tLoss 0.0502 (0.0502)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 98.719 Acc@5 99.969\n",
      "Epoch Time: 0.85 sec\n",
      "Epoch: [3][0/25]\tTime 0.023 (0.023)\tData 0.008 (0.008)\tLoss 0.0285 (0.0285)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.531 Acc@5 100.000\n",
      "Epoch Time: 0.85 sec\n",
      "Epoch: [4][0/25]\tTime 0.023 (0.023)\tData 0.007 (0.007)\tLoss 0.0157 (0.0157)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.875 Acc@5 100.000\n",
      "Epoch Time: 0.84 sec\n",
      "Epoch: [5][0/25]\tTime 0.022 (0.022)\tData 0.007 (0.007)\tLoss 0.0134 (0.0134)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.969 Acc@5 100.000\n",
      "[5] test metrics:{\"loss\": 0.09016695853219264, \"error\": 0.023333333333333334}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.04 sec\n",
      "Epoch: [6][0/25]\tTime 0.019 (0.019)\tData 0.007 (0.007)\tLoss 0.0151 (0.0151)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.969 Acc@5 100.000\n",
      "Epoch Time: 0.82 sec\n",
      "Epoch: [7][0/25]\tTime 0.018 (0.018)\tData 0.007 (0.007)\tLoss 0.0065 (0.0065)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.969 Acc@5 100.000\n",
      "Epoch Time: 0.82 sec\n",
      "Epoch: [8][0/25]\tTime 0.021 (0.021)\tData 0.007 (0.007)\tLoss 0.0071 (0.0071)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.8 sec\n",
      "Epoch: [9][0/25]\tTime 0.022 (0.022)\tData 0.008 (0.008)\tLoss 0.0043 (0.0043)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.82 sec\n",
      "Epoch: [10][0/25]\tTime 0.036 (0.036)\tData 0.022 (0.022)\tLoss 0.0045 (0.0045)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[10] test metrics:{\"loss\": 0.08962296456202037, \"error\": 0.024444444444444446}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.06 sec\n",
      "Epoch: [11][0/25]\tTime 0.020 (0.020)\tData 0.008 (0.008)\tLoss 0.0031 (0.0031)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.81 sec\n",
      "Epoch: [12][0/25]\tTime 0.018 (0.018)\tData 0.008 (0.008)\tLoss 0.0021 (0.0021)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.83 sec\n",
      "Epoch: [13][0/25]\tTime 0.025 (0.025)\tData 0.007 (0.007)\tLoss 0.0019 (0.0019)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.83 sec\n",
      "Epoch: [14][0/25]\tTime 0.017 (0.017)\tData 0.007 (0.007)\tLoss 0.0031 (0.0031)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.82 sec\n",
      "Epoch: [15][0/25]\tTime 0.018 (0.018)\tData 0.007 (0.007)\tLoss 0.0021 (0.0021)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[15] test metrics:{\"loss\": 0.09083591905112068, \"error\": 0.024444444444444446}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.01 sec\n",
      "Epoch: [16][0/25]\tTime 0.022 (0.022)\tData 0.008 (0.008)\tLoss 0.0024 (0.0024)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.84 sec\n",
      "Epoch: [17][0/25]\tTime 0.023 (0.023)\tData 0.007 (0.007)\tLoss 0.0028 (0.0028)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.82 sec\n",
      "Epoch: [18][0/25]\tTime 0.018 (0.018)\tData 0.007 (0.007)\tLoss 0.0019 (0.0019)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.81 sec\n",
      "Epoch: [19][0/25]\tTime 0.017 (0.017)\tData 0.007 (0.007)\tLoss 0.0023 (0.0023)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.81 sec\n",
      "Epoch: [20][0/25]\tTime 0.019 (0.019)\tData 0.007 (0.007)\tLoss 0.0017 (0.0017)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[20] test metrics:{\"loss\": 0.09075596125175556, \"error\": 0.023333333333333334}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.06 sec\n",
      "Epoch: [21][0/25]\tTime 0.021 (0.021)\tData 0.012 (0.012)\tLoss 0.0034 (0.0034)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.82 sec\n",
      "Epoch: [22][0/25]\tTime 0.025 (0.025)\tData 0.007 (0.007)\tLoss 0.0011 (0.0011)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.85 sec\n",
      "Epoch: [23][0/25]\tTime 0.022 (0.022)\tData 0.007 (0.007)\tLoss 0.0016 (0.0016)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.82 sec\n",
      "Epoch: [24][0/25]\tTime 0.021 (0.021)\tData 0.007 (0.007)\tLoss 0.0010 (0.0010)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.79 sec\n",
      "Epoch: [25][0/25]\tTime 0.022 (0.022)\tData 0.007 (0.007)\tLoss 0.0015 (0.0015)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[25] test metrics:{\"loss\": 0.09366644783980317, \"error\": 0.023333333333333334}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.08 sec\n",
      "Pure training time: 21.370000000000005 sec\n"
     ]
    }
   ],
   "source": [
    "%run main_merged.py --dataset lacuna10 --model resnet --dataroot=data/lacuna10/ --filters 1 --lr 0.01 \\\n",
    "--resume checkpoints/lacuna100_resnet_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 5e-4 --batch-size 128 --epochs 26 \\\n",
    "--forget-class 5 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 11175178\n"
     ]
    }
   ],
   "source": [
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model0 = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 4000#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model0.cuda()\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _  = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=512, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=256, seed=seed, shuffle=True)\n",
    "\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "36000\n",
      "10000\n",
      "40000\n",
      "{6: 4000, 9: 4000, 4: 4000, 1: 4000, 2: 4000, 8: 4000, 3: 4000, 7: 4000, 5: 4000, 0: 4000}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'sgd'\n",
    "args.gamma = 0.99\n",
    "args.alpha = 0.001\n",
    "args.beta = 0\n",
    "args.smoothing = 0.0\n",
    "args.msteps = 2\n",
    "args.clip = 0.2\n",
    "args.sstart = 10\n",
    "args.kd_T = 4\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_batch_size = 128\n",
    "args.del_batch_size = 32\n",
    "args.sgda_epochs = 3\n",
    "args.sgda_learning_rate = 0.0005\n",
    "args.lr_decay_epochs = [3,5,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> SCRUB unlearning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihao/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 97.453 \n",
      "maximize loss: -36.63\t minimize loss: 0.08\t train_acc: 97.4527816772461\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 89.581 \n",
      "maximize loss: -195.19\t minimize loss: 0.75\t train_acc: 89.58055877685547\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 98.564 \n",
      "maximize loss: 0.00\t minimize loss: 0.07\t train_acc: 98.56388854980469\n"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "acc_vs = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> SCRUB unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "    acc_v, acc5_v, loss_v = validate(valid_loader_full, model_s, criterion_cls, args, True)\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "    acc_vs.append(100-acc_v.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\")\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    \n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n",
    "acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "acc_v, acc5_v, loss_v = validate(valid_loader_full, model_s, criterion_cls, args, True)\n",
    "acc_rs.append(100-acc_r.item())\n",
    "acc_fs.append(100-acc_f.item())\n",
    "acc_vs.append(100-acc_v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.plot(indices, acc_vs, marker='.', alpha=1, label='valid-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('SCRUB retain-, valid- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Model_D_r -> Loss:0.001, Error:2.777777777777778e-05\n",
      "Original_Model_D_f -> Loss:0.001, Error:0.0\n",
      "Original_Model_D_t -> Loss:0.779, Error:0.1728\n"
     ]
    }
   ],
   "source": [
    "m_D_r_activations,m_D_r_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(retain_loader),'Original_Model_D_r')\n",
    "m_D_f_activations,m_D_f_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(forget_loader),'Original_Model_D_f')\n",
    "m_D_t_activations,m_D_t_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(test_loader_full),'Original_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain_Model_D_r -> Loss:0.001, Error:5.555555555555556e-05\n",
      "Retrain_Model_D_f -> Loss:12.568, Error:1.0\n",
      "Retrain_Model_D_t -> Loss:1.838, Error:0.233\n"
     ]
    }
   ],
   "source": [
    "m0_D_r_activations,m0_D_r_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(retain_loader),'Retrain_Model_D_r')\n",
    "m0_D_f_activations,m0_D_f_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(forget_loader),'Retrain_Model_D_f')\n",
    "m0_D_t_activations,m0_D_t_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(test_loader_full),'Retrain_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRUB_D_r -> Loss:0.02, Error:0.004888888888888889\n",
      "SCRUB_D_f -> Loss:200.373, Error:1.0\n",
      "SCRUB_D_t -> Loss:21.293, Error:0.2438\n"
     ]
    }
   ],
   "source": [
    "ntk_D_r_activations,ntk_D_r_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(retain_loader),'SCRUB_D_r')\n",
    "ntk_D_f_activations,ntk_D_f_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(forget_loader),'SCRUB_D_f')\n",
    "ntk_D_t_activations,ntk_D_t_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(test_loader_full),'SCRUB_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, modelf)\n",
    "#hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-7\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_D_r_activations,fisher_D_r_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(retain_loader),'Fisher_D_r')\n",
    "fisher_D_f_activations,fisher_D_f_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(forget_loader),'Fisher_D_f')\n",
    "fisher_D_t_activations,fisher_D_t_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(test_loader_full),'Fisher_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information in the Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_D_r_activations,finetune_D_r_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(retain_loader),'Finetune_D_r')\n",
    "finetune_D_f_activations,finetune_D_f_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(forget_loader),'Finetune_D_f')\n",
    "finetune_D_t_activations,finetune_D_t_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(test_loader_full),'Finetune_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(5,1,figsize=(9,20))\n",
    "ax = [0,0,0,0,0]\n",
    "plot_entropy_dist(copy.deepcopy(model_s),retain_loader,forget_loader,test_loader_full, 'SCRUB',ax[4])\n",
    "plot_entropy_dist(copy.deepcopy(model),retain_loader,forget_loader,test_loader_full, 'original', ax[0])\n",
    "plot_entropy_dist(copy.deepcopy(model0),retain_loader,forget_loader,test_loader_full, 'retrain', ax[1])\n",
    "plot_entropy_dist(copy.deepcopy(model_ft),retain_loader,forget_loader,test_loader_full, 'finetuned',ax[2])\n",
    "plot_entropy_dist(copy.deepcopy(modelf),retain_loader,forget_loader,test_loader_full, 'fisher',ax[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
