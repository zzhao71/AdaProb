{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import variational\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from itertools import cycle\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 100\n",
      "[0] train metrics:{\"loss\": 3.9548908950805663, \"error\": 0.9099}\n",
      "Learning Rate : 0.1\n",
      "[0] dry_run metrics:{\"loss\": 3.6569058528900147, \"error\": 0.864325}\n",
      "Learning Rate : 0.1\n",
      "[0] test metrics:{\"loss\": 3.669260646820068, \"error\": 0.8685}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 20.08 sec\n",
      "[1] train metrics:{\"loss\": 3.4458515224456785, \"error\": 0.82415}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 11.94 sec\n",
      "[2] train metrics:{\"loss\": 3.016041721725464, \"error\": 0.729925}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 11.91 sec\n",
      "[3] train metrics:{\"loss\": 2.711107554626465, \"error\": 0.65535}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 11.85 sec\n",
      "[4] train metrics:{\"loss\": 2.496060283279419, \"error\": 0.593225}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 11.92 sec\n",
      "[5] train metrics:{\"loss\": 2.329194931793213, \"error\": 0.538525}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.93 sec\n",
      "[6] train metrics:{\"loss\": 2.2086135860443115, \"error\": 0.49755}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.36 sec\n",
      "[7] train metrics:{\"loss\": 2.123908861541748, \"error\": 0.462825}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.34 sec\n",
      "[8] train metrics:{\"loss\": 2.049863201332092, \"error\": 0.431325}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 10.9 sec\n",
      "[9] train metrics:{\"loss\": 1.9988761100769044, \"error\": 0.40755}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 9.36 sec\n",
      "[10] train metrics:{\"loss\": 1.9432452823638915, \"error\": 0.379025}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 8.28 sec\n",
      "[11] train metrics:{\"loss\": 1.9131594074249267, \"error\": 0.35665}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 11.38 sec\n",
      "[12] train metrics:{\"loss\": 1.8789236625671386, \"error\": 0.332075}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 8.61 sec\n",
      "[13] train metrics:{\"loss\": 1.8584274074554443, \"error\": 0.31195}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 9.38 sec\n",
      "[14] train metrics:{\"loss\": 1.835866969680786, \"error\": 0.29505}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 10.99 sec\n",
      "[15] train metrics:{\"loss\": 1.8279486743927003, \"error\": 0.2791}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.27 sec\n",
      "[16] train metrics:{\"loss\": 1.8283887172698974, \"error\": 0.26425}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.47 sec\n",
      "[17] train metrics:{\"loss\": 1.7811426517486573, \"error\": 0.240475}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.59 sec\n",
      "[18] train metrics:{\"loss\": 1.7966106941223146, \"error\": 0.23225}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.6 sec\n",
      "[19] train metrics:{\"loss\": 1.791147449493408, \"error\": 0.21845}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.52 sec\n",
      "[20] train metrics:{\"loss\": 1.7986805698394774, \"error\": 0.20865}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.56 sec\n",
      "[21] train metrics:{\"loss\": 1.802754467010498, \"error\": 0.20475}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.59 sec\n",
      "[22] train metrics:{\"loss\": 1.793176968383789, \"error\": 0.19225}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.62 sec\n",
      "[23] train metrics:{\"loss\": 1.8183617504119873, \"error\": 0.19235}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.53 sec\n",
      "[24] train metrics:{\"loss\": 1.7976149871826173, \"error\": 0.17805}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.59 sec\n",
      "[25] train metrics:{\"loss\": 1.8099366149902343, \"error\": 0.178875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.57 sec\n",
      "[26] train metrics:{\"loss\": 1.791230718421936, \"error\": 0.168925}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.59 sec\n",
      "[27] train metrics:{\"loss\": 1.8296453895568847, \"error\": 0.176575}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.56 sec\n",
      "[28] train metrics:{\"loss\": 1.79987815284729, \"error\": 0.16505}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.53 sec\n",
      "[29] train metrics:{\"loss\": 1.8186018115997316, \"error\": 0.16935}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.45 sec\n",
      "[30] train metrics:{\"loss\": 1.8211508485794068, \"error\": 0.1652}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.58 sec\n",
      "Pure training time: 255.77 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run main.py --dataset cifar100 --dataroot=data/cifar-100-python --model allcnn --filters 1.0 --lr 0.1 --lossfn ce --num-classes 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_cifar5_allcnn_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in small_cifar5_allcnn_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 5\n",
      "[0] train metrics:{\"loss\": 1.711760115623474, \"error\": 0.7340000002384186}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.6461147890090941, \"error\": 0.7060000005364419}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.16 sec\n",
      "[1] train metrics:{\"loss\": 1.625376088142395, \"error\": 0.746000000834465}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[2] train metrics:{\"loss\": 1.5328375396728515, \"error\": 0.7240000023841858}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[3] train metrics:{\"loss\": 1.4741359691619873, \"error\": 0.6880000026226044}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[4] train metrics:{\"loss\": 1.4123272953033448, \"error\": 0.6560000009536743}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[5] train metrics:{\"loss\": 1.3396775016784668, \"error\": 0.5860000019073486}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[6] train metrics:{\"loss\": 1.2723939123153687, \"error\": 0.5259999997615814}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[7] train metrics:{\"loss\": 1.2122151250839233, \"error\": 0.46000000047683715}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[8] train metrics:{\"loss\": 1.157936420440674, \"error\": 0.41200000095367434}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[9] train metrics:{\"loss\": 1.103785228729248, \"error\": 0.3999999942779541}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[10] train metrics:{\"loss\": 1.055343381881714, \"error\": 0.37200000524520876}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[11] train metrics:{\"loss\": 1.0106943554878234, \"error\": 0.3579999990463257}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[12] train metrics:{\"loss\": 0.9717443323135376, \"error\": 0.33799999952316284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[13] train metrics:{\"loss\": 0.9352373170852661, \"error\": 0.3100000057220459}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[14] train metrics:{\"loss\": 0.9033952088356018, \"error\": 0.29400000667572024}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[15] train metrics:{\"loss\": 0.8750443806648255, \"error\": 0.2899999952316284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[16] train metrics:{\"loss\": 0.8474231815338135, \"error\": 0.2740000042915344}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[17] train metrics:{\"loss\": 0.8216310529708862, \"error\": 0.27199999952316284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[18] train metrics:{\"loss\": 0.8033852329254151, \"error\": 0.27600000429153443}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[19] train metrics:{\"loss\": 0.777536406993866, \"error\": 0.2580000023841858}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[20] train metrics:{\"loss\": 0.7597041602134704, \"error\": 0.24000000715255737}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[21] train metrics:{\"loss\": 0.741195752620697, \"error\": 0.22800000190734862}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[22] train metrics:{\"loss\": 0.7218428940773011, \"error\": 0.2259999976158142}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[23] train metrics:{\"loss\": 0.7067114396095275, \"error\": 0.22000000524520874}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[24] train metrics:{\"loss\": 0.6904466762542725, \"error\": 0.20599999809265138}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[25] train metrics:{\"loss\": 0.6757208504676819, \"error\": 0.21400000047683715}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[26] train metrics:{\"loss\": 0.6622907114028931, \"error\": 0.21000000715255737}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[27] train metrics:{\"loss\": 0.6498556952476502, \"error\": 0.18799999618530275}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[28] train metrics:{\"loss\": 0.6395586590766906, \"error\": 0.18599999809265136}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[29] train metrics:{\"loss\": 0.6236334853172302, \"error\": 0.18400000524520874}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[30] train metrics:{\"loss\": 0.6141241917610168, \"error\": 0.17799999618530274}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "Pure training time: 2.580000000000001 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset small_cifar5 --model allcnn --dataroot=data/cifar10/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_cifar5_allcnn_1_0_forget_[0, 1, 2, 3, 4, 5]_num_300_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in small_cifar5_allcnn_1_0_forget_[0, 1, 2, 3, 4, 5]_num_300_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Replacing indexes [129 280 440 384 225 416   1 435 346 314 474 317 161 131 375 198 327 465\n",
      " 330 176 459  89 265 377  65 171  18 350 476  20 100 212 368 193 227 194\n",
      " 175 312 209 309 217  24 482  84  55 448 441 203 141 480 407 291 259 231\n",
      " 478 409  32  67 496 318 462 251 155 284 498 256 221  66  40 310  41  77\n",
      " 352 150 382 152  69  14 457 436 413 222 358  12 120 179 289 415  37 281\n",
      "   2  25 451 103 282 387 117 468 395 146 428 490  53 336 154 393 199 109\n",
      "   7 232 463 297 455 460 188 223 371 353 114 431 215 165  28 269 180 236\n",
      " 414 307  68 205 372 491 321 258 344 240  70 160 452 379 134  30 183  10\n",
      " 286  17 479 357 332 345 119 335 192 178 283 267  29 276 142 337 244 277\n",
      " 429  94 228 173 301 285   3  42 163 391 170 388  90 364 294 159 172 472\n",
      " 397 400 230  13 200 275 216 182 489 157 305 381 214   6 108 399  54  35\n",
      " 174 268  99 235 418 112 398 128 386 471 419 458 340 147 347  23  98 189\n",
      " 118 420 484 266 144  60 306 226 432 151 186 169  11 339 264 279 181 394\n",
      " 126 204 348 127 376 488  57 378 130 467 237 473 383  74 166 106 243  91\n",
      " 369 300 405 370 439 156 334  85 190  39 111 374 328 427   4 342 104 331\n",
      " 438 164 105 241 442 253 262 390 304  82 425  71  87  64 270  93 303 274\n",
      " 485 408 389 403 153 101 333   5  44 113 220 140]\n",
      "Number of Classes: 5\n",
      "[0] train metrics:{\"loss\": 1.6730850343704224, \"error\": 0.7120000026226043}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.656779167175293, \"error\": 0.7100000002980232}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.15 sec\n",
      "[1] train metrics:{\"loss\": 1.5997375535964966, \"error\": 0.7419999986886978}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[2] train metrics:{\"loss\": 1.519848593711853, \"error\": 0.7380000002384186}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[3] train metrics:{\"loss\": 1.4500239868164062, \"error\": 0.6840000028610229}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[4] train metrics:{\"loss\": 1.3808948135375976, \"error\": 0.6039999992847442}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[5] train metrics:{\"loss\": 1.301662230491638, \"error\": 0.5340000019073486}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[6] train metrics:{\"loss\": 1.2285507898330688, \"error\": 0.47199999570846557}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[7] train metrics:{\"loss\": 1.1608472490310668, \"error\": 0.40399999666213987}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[8] train metrics:{\"loss\": 1.0982680158615112, \"error\": 0.33199999952316284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[9] train metrics:{\"loss\": 1.0363691787719727, \"error\": 0.2919999952316284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[10] train metrics:{\"loss\": 0.9853648414611816, \"error\": 0.3260000033378601}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[11] train metrics:{\"loss\": 0.9335386543273926, \"error\": 0.3040000066757202}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[12] train metrics:{\"loss\": 0.8859839100837708, \"error\": 0.27000000047683714}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[13] train metrics:{\"loss\": 0.8452795376777649, \"error\": 0.24400000715255737}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[14] train metrics:{\"loss\": 0.8070784845352172, \"error\": 0.22000000762939453}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[15] train metrics:{\"loss\": 0.7728410420417786, \"error\": 0.21000000524520873}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[16] train metrics:{\"loss\": 0.7391342105865478, \"error\": 0.20000000524520875}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[17] train metrics:{\"loss\": 0.7071670937538147, \"error\": 0.18200000286102294}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[18] train metrics:{\"loss\": 0.6823869533538819, \"error\": 0.16200000762939454}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[19] train metrics:{\"loss\": 0.6509597244262695, \"error\": 0.16000000762939454}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[20] train metrics:{\"loss\": 0.6246025652885437, \"error\": 0.1500000057220459}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[21] train metrics:{\"loss\": 0.600079152584076, \"error\": 0.15200000047683715}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[22] train metrics:{\"loss\": 0.5728805670738221, \"error\": 0.14399999618530274}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[23] train metrics:{\"loss\": 0.550903811454773, \"error\": 0.13399999427795412}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[24] train metrics:{\"loss\": 0.5281814522743226, \"error\": 0.1219999942779541}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[25] train metrics:{\"loss\": 0.5076581506729126, \"error\": 0.11199999666213989}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[26] train metrics:{\"loss\": 0.4853826947212219, \"error\": 0.09999999666213989}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[27] train metrics:{\"loss\": 0.46953974413871763, \"error\": 0.10599999904632568}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[28] train metrics:{\"loss\": 0.449963604927063, \"error\": 0.1019999942779541}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[29] train metrics:{\"loss\": 0.4279073371887207, \"error\": 0.08600000619888305}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[30] train metrics:{\"loss\": 0.41176775050163267, \"error\": 0.06999999952316284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "Pure training time: 2.5500000000000003 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset small_cifar5 --model allcnn --dataroot=data/cifar10/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 \\\n",
    "--forget-class 0,1,2,3,4,5 --num-to-forget 300 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 1619045\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model0 = copy.deepcopy(model)\n",
    "model_initial = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 300#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "model_initial.load_state_dict(torch.load(init_checkpoint))\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)\n",
    "\n",
    "model.cuda()\n",
    "model0.cuda()\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between w(D) and w(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(model,model0):\n",
    "    distance=0\n",
    "    normalization=0\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space='  ' if 'bias' in k else ''\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "        distance+=current_dist\n",
    "        normalization+=current_norm\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 0.5020030041371059\n",
      "Normalized Distance: 0.0062086479438128945\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Retrain']=distance(model,model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance of w(D) from initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntk_init(resume,seed=1):\n",
    "    manual_seed(seed)\n",
    "    model_init = models.get_model(arch, num_classes=num_classes, filters_percentage=filters).to(args.device)\n",
    "    model_init.load_state_dict(torch.load(resume))\n",
    "    return model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "for p in model_init.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 0.9483404309151064\n",
      "Normalized Distance: 0.011732403872896827\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Original_init']=distance(model_init,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.retain_bs = 32\n",
    "args.forget_bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [129 280 440 384 225 416   1 435 346 314 474 317 161 131 375 198 327 465\n",
      " 330 176 459  89 265 377  65 171  18 350 476  20 100 212 368 193 227 194\n",
      " 175 312 209 309 217  24 482  84  55 448 441 203 141 480 407 291 259 231\n",
      " 478 409  32  67 496 318 462 251 155 284 498 256 221  66  40 310  41  77\n",
      " 352 150 382 152  69  14 457 436 413 222 358  12 120 179 289 415  37 281\n",
      "   2  25 451 103 282 387 117 468 395 146 428 490  53 336 154 393 199 109\n",
      "   7 232 463 297 455 460 188 223 371 353 114 431 215 165  28 269 180 236\n",
      " 414 307  68 205 372 491 321 258 344 240  70 160 452 379 134  30 183  10\n",
      " 286  17 479 357 332 345 119 335 192 178 283 267  29 276 142 337 244 277\n",
      " 429  94 228 173 301 285   3  42 163 391 170 388  90 364 294 159 172 472\n",
      " 397 400 230  13 200 275 216 182 489 157 305 381 214   6 108 399  54  35\n",
      " 174 268  99 235 418 112 398 128 386 471 419 458 340 147 347  23  98 189\n",
      " 118 420 484 266 144  60 306 226 432 151 186 169  11 339 264 279 181 394\n",
      " 126 204 348 127 376 488  57 378 130 467 237 473 383  74 166 106 243  91\n",
      " 369 300 405 370 439 156 334  85 190  39 111 374 328 427   4 342 104 331\n",
      " 438 164 105 241 442 253 262 390 304  82 425  71  87  64 270  93 303 274\n",
      " 485 408 389 403 153 101 333   5  44 113 220 140]\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _ = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=args.forget_bs, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=args.retain_bs, seed=seed, shuffle=True)\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "200\n",
      "500\n",
      "500\n",
      "{0: 100, 1: 100, 2: 100, 3: 100, 4: 100}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'adam'\n",
    "args.gamma = 1\n",
    "args.alpha = 0.5\n",
    "args.beta = 0\n",
    "args.smoothing = 0.5\n",
    "args.msteps = 3\n",
    "args.clip = 0.2\n",
    "args.sstart = 10\n",
    "args.kd_T = 2\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_epochs = 10\n",
    "args.sgda_learning_rate = 0.0005\n",
    "args.lr_decay_epochs = [5,8,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 0.1#5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> scrub unlearning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihao/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 61.500 \n",
      "maximize loss: 0.71\t minimize loss: 6.92\t train_acc: 61.5\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 77.000 \n",
      "maximize loss: 8.87\t minimize loss: 14.38\t train_acc: 77.0\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 84.000 \n",
      "maximize loss: 16.31\t minimize loss: 21.31\t train_acc: 84.0\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 90.500 \n",
      "maximize loss: 0.00\t minimize loss: 24.87\t train_acc: 90.5\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 93.500 \n",
      "maximize loss: 0.00\t minimize loss: 28.17\t train_acc: 93.5\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 96.500 \n",
      "maximize loss: 0.00\t minimize loss: 30.19\t train_acc: 96.5\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 95.500 \n",
      "maximize loss: 0.00\t minimize loss: 30.48\t train_acc: 95.5\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 96.500 \n",
      "maximize loss: 0.00\t minimize loss: 30.78\t train_acc: 96.5\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 93.500 \n",
      "maximize loss: 0.00\t minimize loss: 31.01\t train_acc: 93.5\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 95.000 \n",
      "maximize loss: 0.00\t minimize loss: 31.01\t train_acc: 95.0\n"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> scrub unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\",)\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('scrub retain- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19.0, 23.0, 19.5, 18.0, 10.0, 6.5, 4.5, 3.0, 3.0, 3.0]\n",
      "[16.666664123535156, 33.0, 35.0, 42.666664123535156, 31.333328247070312, 28.333328247070312, 23.0, 22.0, 21.0, 21.333328247070312]\n"
     ]
    }
   ],
   "source": [
    "print (acc_rs)\n",
    "print (acc_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTK based Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NTK Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_w_utils(model_init,dataloader,name='complete'):\n",
    "    model_init.eval()\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    G_list = []\n",
    "    f0_minus_y = []\n",
    "    for idx, batch in enumerate(dataloader):#(tqdm(dataloader,leave=False)):\n",
    "        batch = [tensor.to(next(model_init.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        if 'mnist' in args.dataset:\n",
    "            input = input.view(input.shape[0],-1)\n",
    "        target = target.cpu().detach().numpy()\n",
    "        output = model_init(input)\n",
    "        G_sample=[]\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output[0,cls],model_init.parameters(),retain_graph=True)\n",
    "            grads = np.concatenate([g.view(-1).cpu().numpy() for g in grads])\n",
    "            G_sample.append(grads)\n",
    "            G_list.append(grads)\n",
    "        if args.lossfn=='mse':\n",
    "            p = output.cpu().detach().numpy().transpose()\n",
    "            #loss_hess = np.eye(len(p))\n",
    "            target = 2*target-1\n",
    "            f0_y_update = p-target\n",
    "        elif args.lossfn=='ce':\n",
    "            p = torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().transpose()\n",
    "            p[target]-=1\n",
    "            f0_y_update = copy.deepcopy(p)\n",
    "        f0_minus_y.append(f0_y_update)\n",
    "    return np.stack(G_list).transpose(),np.vstack(f0_minus_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobians and Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk_time = 0\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_r,f0_minus_y_r = delta_w_utils(copy.deepcopy(model),retain_loader,'complete')\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_r.npy',G_r)\n",
    "np.save('NTK_data/f0_minus_y_r.npy',f0_minus_y_r)\n",
    "del G_r, f0_minus_y_r\n",
    "\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_f,f0_minus_y_f = delta_w_utils(copy.deepcopy(model),forget_loader,'retain') \n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_f.npy',G_f)\n",
    "np.save('NTK_data/f0_minus_y_f.npy',f0_minus_y_f)\n",
    "del G_f, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "G_f = np.load('NTK_data/G_f.npy')\n",
    "G = np.concatenate([G_r,G_f],axis=1)\n",
    "\n",
    "np.save('NTK_data/G.npy',G)\n",
    "del G, G_f, G_r\n",
    "\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "f0_minus_y_f = np.load('NTK_data/f0_minus_y_f.npy')\n",
    "f0_minus_y = np.concatenate([f0_minus_y_r,f0_minus_y_f])\n",
    "\n",
    "np.save('NTK_data/f0_minus_y.npy',f0_minus_y)\n",
    "del f0_minus_y, f0_minus_y_r, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only requires access to the gradients and the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.load('NTK_data/G.npy')\n",
    "t1 = time.time()\n",
    "theta = G.transpose().dot(G) + num_total*args.weight_decay*np.eye(G.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G\n",
    "\n",
    "t1 = time.time()\n",
    "theta_inv = np.linalg.inv(theta)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta.npy',theta)\n",
    "del theta\n",
    "\n",
    "G = np.load('NTK_data/G.npy')\n",
    "f0_minus_y = np.load('NTK_data/f0_minus_y.npy')\n",
    "t1 = time.time()\n",
    "w_complete = -G.dot(theta_inv.dot(f0_minus_y))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_inv.npy',theta_inv)\n",
    "np.save('NTK_data/w_complete.npy',w_complete)\n",
    "\n",
    "del G, f0_minus_y, theta_inv, w_complete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "t1 = time.time()\n",
    "theta_r = G_r.transpose().dot(G_r) + num_to_retain*args.weight_decay*np.eye(G_r.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G_r\n",
    "\n",
    "t1 = time.time()\n",
    "theta_r_inv = np.linalg.inv(theta_r)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r.npy',theta_r)\n",
    "del theta_r\n",
    "\n",
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "t1 = time.time()\n",
    "w_retain = -G_r.dot(theta_r_inv.dot(f0_minus_y_r))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r_inv.npy',theta_r_inv)\n",
    "np.save('NTK_data/w_retain.npy',w_retain)\n",
    "\n",
    "del G_r, f0_minus_y_r, theta_r_inv, w_retain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrubbing Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Scrubbing Direction\n",
    "w_complete = np.load('NTK_data/w_complete.npy')\n",
    "w_retain = np.load('NTK_data/w_retain.npy')\n",
    "delta_w = (w_retain-w_complete).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_copy = copy.deepcopy(delta_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Change in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_actual = vectorize_params(model0)-vectorize_params(model)\n",
    "\n",
    "print(f'Actual Norm-: {np.linalg.norm(delta_w_actual)}')\n",
    "print(f'Predtn Norm-: {np.linalg.norm(delta_w)}')\n",
    "scale_ratio = np.linalg.norm(delta_w_actual)/np.linalg.norm(delta_w)\n",
    "print('Actual Scale: {}'.format(scale_ratio))\n",
    "log_dict['actual_scale_ratio']=scale_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trapezium Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred_error = vectorize_params(model)-vectorize_params(model_init)-w_retain.squeeze()\n",
    "print(f\"Delta w -------: {np.linalg.norm(delta_w)}\")\n",
    "\n",
    "inner = np.inner(delta_w/np.linalg.norm(delta_w),m_pred_error/np.linalg.norm(m_pred_error))\n",
    "print(f\"Inner Product--: {inner}\")\n",
    "\n",
    "if inner<0:\n",
    "    angle = np.arccos(inner)-np.pi/2\n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.sin(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "else:\n",
    "    angle = np.arccos(inner) \n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.cos(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "\n",
    "predicted_scale=predicted_norm/np.linalg.norm(delta_w)\n",
    "predicted_scale\n",
    "print(f\"Predicted Scale:  {predicted_scale}\")\n",
    "log_dict['predicted_scale_ratio']=predicted_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Inner Product between Prediction and Actual Scrubbing Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIP(v1,v2):\n",
    "    nip = (np.inner(v1/np.linalg.norm(v1),v2/np.linalg.norm(v2)))\n",
    "    print(nip)\n",
    "    return nip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nip=NIP(delta_w_actual,delta_w)\n",
    "log_dict['nip']=nip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_w_dict(delta_w,model):\n",
    "    # Give normalized delta_w\n",
    "    delta_w_dict = OrderedDict()\n",
    "    params_visited = 0\n",
    "    for k,p in model.named_parameters():\n",
    "        num_params = np.prod(list(p.shape))\n",
    "        update_params = delta_w[params_visited:params_visited+num_params]\n",
    "        delta_w_dict[k] = torch.Tensor(update_params).view_as(p)\n",
    "        params_visited+=num_params\n",
    "    return delta_w_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics,activations,predictions=get_metrics(model,dataloader,criterion,True)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "\n",
    "    return activations,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub using NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=predicted_scale\n",
    "direction = get_delta_w_dict(delta_w,model)\n",
    "\n",
    "model_scrub = copy.deepcopy(model)\n",
    "for k,p in model_scrub.named_parameters():\n",
    "    p.data += (direction[k]*scale).to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune and Fisher Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg\n",
    "\n",
    "def run_neggrad_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    forget_loader: torch.utils.data.DataLoader,\n",
    "                    alpha: float,\n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    quiet=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, (batch_retain,batch_forget) in enumerate(tqdm(zip(data_loader,cycle(forget_loader)), leave=False)):\n",
    "            batch_retain = [tensor.to(next(model.parameters()).device) for tensor in batch_retain]\n",
    "            batch_forget = [tensor.to(next(model.parameters()).device) for tensor in batch_forget]\n",
    "            input_r, target_r = batch_retain\n",
    "            input_f, target_f = batch_forget\n",
    "            output_r = model(input_r)\n",
    "            output_f = model(input_f)\n",
    "            loss = alpha*(loss_fn(output_r, target_r) + l2_penalty(model,model_init,args.weight_decay)) - (1-alpha)*loss_fn(output_f, target_f)\n",
    "            metrics.update(n=input_r.size(0), loss=loss_fn(output_r,target_r).item(), error=get_error(output_r, target_r))\n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def negative_grad(model: nn.Module, data_loader: torch.utils.data.DataLoader, forget_loader: torch.utils.data.DataLoader, alpha: float, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_neggrad_epoch(model, model_init, data_loader, forget_loader, alpha, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "\n",
    "def fk_fientune(model: nn.Module, data_loader: torch.utils.data.DataLoader, args, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.1, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computes the amount of information not forgotten at all layers using the given alpha\n",
    "alpha = 1e-7\n",
    "total_kl = 0\n",
    "torch.manual_seed(seed)\n",
    "for (k, p), (k0, p0) in zip(modelf.named_parameters(), modelf0.named_parameters()):\n",
    "    mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "    mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "    kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "    total_kl += kl\n",
    "    print(k, f'{kl:.1f}')\n",
    "print(\"Total:\", total_kl)\n",
    "log_dict['fisher_info']=total_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-6\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())\n",
    "\n",
    "for i, p in enumerate(modelf0.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test(modelf, retain_loader))\n",
    "print(test(modelf, forget_loader))\n",
    "print(test(modelf, valid_loader_full))\n",
    "print(test(modelf, test_loader_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.ng_alpha = 0.95\n",
    "args.ng_epochs = 10\n",
    "args.ng_lr = 0.01\n",
    "model_ng = copy.deepcopy(model)    \n",
    "negative_grad(model_ng, retain_loader, forget_loader, alpha=args.ng_alpha, epochs=args.ng_epochs, quiet=True, lr=args.ng_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catastrophic Forgetting k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.cfk_lr = 0.01\n",
    "args.cfk_epochs = 10\n",
    "\n",
    "model_cfk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_cfk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    layers = [9]\n",
    "    for k in layers:\n",
    "        for param in model_cfk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    for param in model_cfk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "fk_fientune(model_cfk, retain_loader, args=args, epochs=args.cfk_epochs, quiet=True, lr=args.cfk_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Unlearning k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The last block and classifier of resnet-18\n",
    "(layer4): Sequential(\n",
    "    (0): _ResBlock(\n",
    "      (bn1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(102, 204, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (shortcut): Sequential(\n",
    "        (0): Conv2d(102, 204, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "      )\n",
    "    )\n",
    "    (1): _ResBlock(\n",
    "      (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    )\n",
    "  )\n",
    "(linear): Linear(in_features=204, out_features=5, bias=True)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" The last block and classifier of allcnn\n",
    "AllCNN(\n",
    "  (features): Sequential(\n",
    "    ...\n",
    "    (9): Conv(\n",
    "      (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (2): ReLU()\n",
    "    )\n",
    "    (10): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
    "    (11): Flatten()\n",
    "  )\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=192, out_features=5, bias=True)\n",
    "  )\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.euk_lr = 0.01\n",
    "args.euk_epochs = training_epochs\n",
    "model_euk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_euk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    with torch.no_grad():\n",
    "        for k in layers:\n",
    "            for i in range(0,3):\n",
    "                try:\n",
    "                    model_euk.features[k][i].weight.copy_(model_initial.features[k][i].weight)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have weights\".format(k,i))\n",
    "                try:\n",
    "                    model_euk.features[k][i].bias.copy_(model_initial.features[k][i].bias)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have bias\".format(k,i))\n",
    "        model_euk.classifier[0].weight.copy_(model_initial.classifier[0].weight)\n",
    "        model_euk.classifier[0].bias.copy_(model_initial.classifier[0].bias)\n",
    "    \n",
    "    for k in layers:\n",
    "        for param in model_euk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,2):\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.weight.copy_(model_initial.layer4[i].bn1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.bias.copy_(model_initial.layer4[i].bn1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.weight.copy_(model_initial.layer4[i].conv1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.bias.copy_(model_initial.layer4[i].conv1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.weight.copy_(model_initial.layer4[i].bn2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.bias.copy_(model_initial.layer4[i].bn2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.weight.copy_(model_initial.layer4[i].conv2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.bias.copy_(model_initial.layer4[i].conv2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "        model_euk.layer4[0].shortcut[0].weight.copy_(model_initial.layer4[0].shortcut[0].weight)\n",
    "        \n",
    "    for param in model_euk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "fk_fientune(model_euk, retain_loader, epochs=args.euk_epochs, quiet=True, lr=args.euk_lr, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "_,_=activations_predictions(copy.deepcopy(model),forget_loader,'Original_Model_D_f')\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"c\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_ng),thresh,'NegGrad')\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model_cfk),thresh,'CF-k')\n",
    "readouts[\"f\"] = all_readouts(copy.deepcopy(model_euk),thresh,'EU-k')\n",
    "readouts[\"g\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"h\"] = all_readouts(copy.deepcopy(model_scrub),thresh,'NTK')\n",
    "readouts[\"i\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
