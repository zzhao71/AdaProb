{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_392359/3351334044.py:20: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import variational\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activations along the path\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datasets\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def entropy(p, dim = -1, keepdim = False):\n",
    "    return -torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim=dim, keepdim=keepdim)\n",
    "    #return (p * p.log()).sum(dim=dim, keepdim=keepdim)\n",
    "\n",
    "def collect_prob(data_loader, model):\n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=False, num_samples=4000)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, sampler=sampler, shuffle=False)\n",
    "    prob = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(data_loader_small, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            data, target = batch\n",
    "            output = model(data)\n",
    "            prob.append(F.softmax(output, dim=-1).data)\n",
    "    return torch.cat(prob)\n",
    "\n",
    "def get_membership_attack_data(retain_loader, forget_loader, test_loader, model):    \n",
    "    retain_prob = collect_prob(retain_loader, model)\n",
    "    forget_prob = collect_prob(forget_loader, model)\n",
    "    test_prob = collect_prob(test_loader, model)\n",
    "    \n",
    "    X_r = torch.cat([entropy(retain_prob), entropy(test_prob)]).cpu().numpy().reshape(-1, 1)\n",
    "    Y_r = np.concatenate([np.ones(len(retain_prob)), np.zeros(len(test_prob))])\n",
    "    \n",
    "    X_f = entropy(forget_prob).cpu().numpy().reshape(-1, 1)\n",
    "    Y_f = np.concatenate([np.ones(len(forget_prob))])    \n",
    "    return X_f, Y_f, X_r, Y_r\n",
    "\n",
    "def get_membership_attack_prob(retain_loader, forget_loader, test_loader, model):\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader, forget_loader, test_loader, model)\n",
    "    #clf = SVC(C=3,gamma='auto',kernel='rbf')\n",
    "    clf = LogisticRegression(class_weight='balanced',solver='lbfgs',multi_class='multinomial')\n",
    "    clf.fit(X_r, Y_r)\n",
    "    results = clf.predict(X_f)\n",
    "    results1 = clf.predict(X_r)\n",
    "    acc = accuracy_score(results, Y_f)\n",
    "    train_ac = accuracy_score(results1, Y_r)\n",
    "    TP, FP, TN, FN = perf_measure(Y_r, results1)\n",
    "    FPR = FP/(FP+TN)\n",
    "    FNR = FN/(FN+TP)\n",
    "    \n",
    "    print (f\"TP:{TP}, FP{FP}, TN{TN}, FN{FN}\")\n",
    "    print (f\"false negative rate: {FN/(FN+TP)}\")\n",
    "    print (f\"false positive rate: {FP/(FP+TN)}\")\n",
    "    return acc, train_acc, FPR, FNR #results.mean(), results1.mean()\n",
    "    \n",
    "def plot_entropy_dist(model,retain_loader,forget_loader,test_loader_full, title, ax):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from matplotlib.ticker import FuncFormatter\n",
    "    #train_loader_full, valid_loader_full, test_loader_full = datasets.get_loaders(dataset, batch_size=100, seed=0, augment=False, shuffle=False)\n",
    "    #indexes = np.flatnonzero(np.array(train_loader_full.dataset.targets) == class_to_forget)\n",
    "    #replaced = np.random.RandomState(0).choice(indexes, size=100 if num_to_forget==100 else len(indexes), replace=False)\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader,forget_loader,test_loader_full, model)\n",
    "    #np.savetxt('retain_prob.txt', X_r[Y_r==1])\n",
    "    #np.savetxt('forget_prob.txt', X_f)\n",
    "    #np.savetxt('test_prob.txt', X_r[Y_r==0])\n",
    "    sns.distplot(np.log(X_r[Y_r==1]).reshape(-1), kde=False, norm_hist=True, rug=False, label='retain', ax=plt)\n",
    "    sns.distplot(np.log(X_r[Y_r==0]).reshape(-1), kde=False, norm_hist=True, rug=False, label='test', ax=plt)\n",
    "    sns.distplot(np.log(X_f).reshape(-1), kde=False, norm_hist=True, rug=False, label='forget', ax=plt)\n",
    "    plt.legend(prop={'size': 14})\n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.title(title,size=18)\n",
    "    plt.xlabel('Log of Entropy',size=14)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def membership_attack(retain_loader,forget_loader,test_loader,model, name):\n",
    "    prob, train_acc, FPR, FNR = get_membership_attack_prob(retain_loader,forget_loader,test_loader,model)\n",
    "    print(\"Attack prob: \", prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=512, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=512, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics=get_metrics(model,dataloader,criterion,False)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "    return _,_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        #run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.01, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    #print(f\"{name} ->\"\n",
    "    #      f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_activations(model_scrubf, modelf0, delta_w_s, delta_w_m0, data_loader, \\\n",
    "                    loss_fn=nn.CrossEntropyLoss(),\\\n",
    "                    optimizer=torch.optim.SGD, \\\n",
    "                    seed=1,quiet=False):\n",
    "\n",
    "    model_scrubf.eval()\n",
    "    modelf0.eval()\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    \n",
    "    metrics = AverageMeter()    \n",
    "    num_classes = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "        batch = [tensor.to(next(model_scrubf.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        \n",
    "        output_sf = model_scrubf(input)\n",
    "        G_sf = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=True)\n",
    "            grads = torch.cat([g.view(-1) for g in grads])\n",
    "            G_sf.append(grads)\n",
    "\n",
    "        grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_sf = torch.stack(G_sf)#.pow(2)\n",
    "        delta_f_sf_update = torch.matmul(G_sf,delta_w_s.sqrt()*torch.empty_like(delta_w_s).normal_())\n",
    "        G_sf = G_sf.pow(2)\n",
    "        delta_f_sf = torch.matmul(G_sf,delta_w_s)\n",
    "\n",
    "        output_m0 = modelf0(input)\n",
    "        G_m0 = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=True)\n",
    "            grad_m0 = torch.cat([g.view(-1) for g in grads])\n",
    "            G_m0.append(grad_m0)\n",
    "\n",
    "        grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_m0 = torch.stack(G_m0).pow(2)\n",
    "        delta_f_m0 = torch.matmul(G_m0,delta_w_m0)\n",
    "        \n",
    "        kl = ((output_m0 - output_sf).pow(2)/delta_f_m0 + delta_f_sf/delta_f_m0 - torch.log(delta_f_sf/delta_f_m0) - 1).sum()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        output_sf += delta_f_sf_update#delta_f_sf.sqrt()*torch.empty_like(delta_f_sf).normal_()\n",
    "        \n",
    "        loss = loss_fn(output_sf, target)\n",
    "        metrics.update(n=input.size(0), loss=loss.item(), error=get_error(output_sf, target), kl=kl.item())\n",
    "    \n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 100\n",
      "[0] train metrics:{\"loss\": 3.9548908950805663, \"error\": 0.9099}\n",
      "Learning Rate : 0.1\n",
      "[0] dry_run metrics:{\"loss\": 3.6569058528900147, \"error\": 0.864325}\n",
      "Learning Rate : 0.1\n",
      "[0] test metrics:{\"loss\": 3.669260646820068, \"error\": 0.8685}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 11.44 sec\n",
      "[1] train metrics:{\"loss\": 3.4458515224456785, \"error\": 0.82415}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.27 sec\n",
      "[2] train metrics:{\"loss\": 3.016041721725464, \"error\": 0.729925}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.39 sec\n",
      "[3] train metrics:{\"loss\": 2.711107554626465, \"error\": 0.65535}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.46 sec\n",
      "[4] train metrics:{\"loss\": 2.496060283279419, \"error\": 0.593225}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.37 sec\n",
      "[5] train metrics:{\"loss\": 2.329194931793213, \"error\": 0.538525}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.67 sec\n",
      "[6] train metrics:{\"loss\": 2.2086135860443115, \"error\": 0.49755}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.55 sec\n",
      "[7] train metrics:{\"loss\": 2.123908861541748, \"error\": 0.462825}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.54 sec\n",
      "[8] train metrics:{\"loss\": 2.049863201332092, \"error\": 0.431325}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.26 sec\n",
      "[9] train metrics:{\"loss\": 1.9988761100769044, \"error\": 0.40755}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.25 sec\n",
      "[10] train metrics:{\"loss\": 1.9432452823638915, \"error\": 0.379025}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.36 sec\n",
      "[11] train metrics:{\"loss\": 1.9131594074249267, \"error\": 0.35665}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.36 sec\n",
      "[12] train metrics:{\"loss\": 1.8789236625671386, \"error\": 0.332075}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.93 sec\n",
      "[13] train metrics:{\"loss\": 1.8584274074554443, \"error\": 0.31195}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.0 sec\n",
      "[14] train metrics:{\"loss\": 1.835866969680786, \"error\": 0.29505}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.38 sec\n",
      "[15] train metrics:{\"loss\": 1.8279486743927003, \"error\": 0.2791}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.44 sec\n",
      "[16] train metrics:{\"loss\": 1.8283887172698974, \"error\": 0.26425}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.98 sec\n",
      "[17] train metrics:{\"loss\": 1.7811426517486573, \"error\": 0.240475}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.06 sec\n",
      "[18] train metrics:{\"loss\": 1.7966106941223146, \"error\": 0.23225}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.92 sec\n",
      "[19] train metrics:{\"loss\": 1.791147449493408, \"error\": 0.21845}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.06 sec\n",
      "[20] train metrics:{\"loss\": 1.7986805698394774, \"error\": 0.20865}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.08 sec\n",
      "[21] train metrics:{\"loss\": 1.802754467010498, \"error\": 0.20475}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.09 sec\n",
      "[22] train metrics:{\"loss\": 1.793176968383789, \"error\": 0.19225}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.14 sec\n",
      "[23] train metrics:{\"loss\": 1.8183617504119873, \"error\": 0.19235}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 5.0 sec\n",
      "[24] train metrics:{\"loss\": 1.7976149871826173, \"error\": 0.17805}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.98 sec\n",
      "[25] train metrics:{\"loss\": 1.8099366149902343, \"error\": 0.178875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.95 sec\n",
      "[26] train metrics:{\"loss\": 1.791230718421936, \"error\": 0.168925}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.97 sec\n",
      "[27] train metrics:{\"loss\": 1.8296453895568847, \"error\": 0.176575}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.89 sec\n",
      "[28] train metrics:{\"loss\": 1.79987815284729, \"error\": 0.16505}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.96 sec\n",
      "[29] train metrics:{\"loss\": 1.8186018115997316, \"error\": 0.16935}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.99 sec\n",
      "[30] train metrics:{\"loss\": 1.8211508485794068, \"error\": 0.1652}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.87 sec\n",
      "Pure training time: 160.96999999999997 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset cifar100 --dataroot=data/cifar-100-python --model allcnn --filters 1.0 --lr 0.1 --lossfn ce --num-classes 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar10_allcnn_1_0_forget_None_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar10_allcnn_1_0_forget_None_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Number of Classes: 10\n",
      "Epoch: [0][0/313]\tTime 0.123 (0.123)\tData 0.022 (0.022)\tLoss 2.3841 (2.3841)\tAcc@1 16.406 (16.406)\tAcc@5 53.125 (53.125)\n",
      " * Acc@1 68.640 Acc@5 97.112\n",
      "[0] test metrics:{\"loss\": 0.6585341669082642, \"error\": 0.2298}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 5.27 sec\n",
      "Epoch: [1][0/313]\tTime 0.016 (0.016)\tData 0.010 (0.010)\tLoss 0.5724 (0.5724)\tAcc@1 81.250 (81.250)\tAcc@5 99.219 (99.219)\n",
      " * Acc@1 80.620 Acc@5 99.275\n",
      "Epoch Time: 4.18 sec\n",
      "Epoch: [2][0/313]\tTime 0.015 (0.015)\tData 0.009 (0.009)\tLoss 0.4610 (0.4610)\tAcc@1 85.938 (85.938)\tAcc@5 99.219 (99.219)\n",
      " * Acc@1 85.045 Acc@5 99.567\n",
      "Epoch Time: 4.19 sec\n",
      "Epoch: [3][0/313]\tTime 0.019 (0.019)\tData 0.012 (0.012)\tLoss 0.3552 (0.3552)\tAcc@1 86.719 (86.719)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 88.465 Acc@5 99.695\n",
      "Epoch Time: 4.25 sec\n",
      "Epoch: [4][0/313]\tTime 0.015 (0.015)\tData 0.008 (0.008)\tLoss 0.1416 (0.1416)\tAcc@1 96.875 (96.875)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 91.327 Acc@5 99.830\n",
      "Epoch Time: 4.24 sec\n",
      "Epoch: [5][0/313]\tTime 0.014 (0.014)\tData 0.008 (0.008)\tLoss 0.1272 (0.1272)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 94.235 Acc@5 99.910\n",
      "[5] test metrics:{\"loss\": 0.5239728908538819, \"error\": 0.1594}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 5.14 sec\n",
      "Epoch: [6][0/313]\tTime 0.024 (0.024)\tData 0.017 (0.017)\tLoss 0.1445 (0.1445)\tAcc@1 95.312 (95.312)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 96.740 Acc@5 99.960\n",
      "Epoch Time: 4.09 sec\n",
      "Epoch: [7][0/313]\tTime 0.018 (0.018)\tData 0.011 (0.011)\tLoss 0.1222 (0.1222)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 98.447 Acc@5 99.985\n",
      "Epoch Time: 4.08 sec\n",
      "Epoch: [8][0/313]\tTime 0.019 (0.019)\tData 0.013 (0.013)\tLoss 0.0580 (0.0580)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.365 Acc@5 99.995\n",
      "Epoch Time: 4.08 sec\n",
      "Epoch: [9][0/313]\tTime 0.015 (0.015)\tData 0.009 (0.009)\tLoss 0.0288 (0.0288)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.837 Acc@5 99.997\n",
      "Epoch Time: 4.05 sec\n",
      "Epoch: [10][0/313]\tTime 0.020 (0.020)\tData 0.014 (0.014)\tLoss 0.0219 (0.0219)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.932 Acc@5 100.000\n",
      "[10] test metrics:{\"loss\": 0.6609334421157836, \"error\": 0.1559}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 5.0 sec\n",
      "Epoch: [11][0/313]\tTime 0.020 (0.020)\tData 0.013 (0.013)\tLoss 0.0142 (0.0142)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.970 Acc@5 100.000\n",
      "Epoch Time: 4.16 sec\n",
      "Epoch: [12][0/313]\tTime 0.025 (0.025)\tData 0.019 (0.019)\tLoss 0.0116 (0.0116)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "Epoch Time: 4.2 sec\n",
      "Epoch: [13][0/313]\tTime 0.013 (0.013)\tData 0.008 (0.008)\tLoss 0.0094 (0.0094)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.985 Acc@5 100.000\n",
      "Epoch Time: 4.49 sec\n",
      "Epoch: [14][0/313]\tTime 0.015 (0.015)\tData 0.008 (0.008)\tLoss 0.0094 (0.0094)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "Epoch Time: 4.3 sec\n",
      "Epoch: [15][0/313]\tTime 0.016 (0.016)\tData 0.010 (0.010)\tLoss 0.0058 (0.0058)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.993 Acc@5 100.000\n",
      "[15] test metrics:{\"loss\": 0.7274604979515076, \"error\": 0.1579}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 5.45 sec\n",
      "Epoch: [16][0/313]\tTime 0.022 (0.022)\tData 0.016 (0.016)\tLoss 0.0091 (0.0091)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.51 sec\n",
      "Epoch: [17][0/313]\tTime 0.015 (0.015)\tData 0.009 (0.009)\tLoss 0.0070 (0.0070)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 4.32 sec\n",
      "Epoch: [18][0/313]\tTime 0.016 (0.016)\tData 0.009 (0.009)\tLoss 0.0182 (0.0182)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.37 sec\n",
      "Epoch: [19][0/313]\tTime 0.022 (0.022)\tData 0.016 (0.016)\tLoss 0.0053 (0.0053)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.36 sec\n",
      "Epoch: [20][0/313]\tTime 0.027 (0.027)\tData 0.020 (0.020)\tLoss 0.0054 (0.0054)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[20] test metrics:{\"loss\": 0.7736012166976929, \"error\": 0.1595}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 5.23 sec\n",
      "Epoch: [21][0/313]\tTime 0.030 (0.030)\tData 0.024 (0.024)\tLoss 0.0052 (0.0052)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.27 sec\n",
      "Epoch: [22][0/313]\tTime 0.014 (0.014)\tData 0.008 (0.008)\tLoss 0.0077 (0.0077)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.21 sec\n",
      "Epoch: [23][0/313]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0037 (0.0037)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.22 sec\n",
      "Epoch: [24][0/313]\tTime 0.025 (0.025)\tData 0.020 (0.020)\tLoss 0.0049 (0.0049)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.2 sec\n",
      "Epoch: [25][0/313]\tTime 0.014 (0.014)\tData 0.008 (0.008)\tLoss 0.0047 (0.0047)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[25] test metrics:{\"loss\": 0.8194447939872742, \"error\": 0.1599}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 5.17 sec\n",
      "Pure training time: 110.22 sec\n"
     ]
    }
   ],
   "source": [
    "%run main_merged.py --dataset cifar10 --model allcnn --dataroot=data/cifar-10-batches-py/ --filters 1.0 --lr 0.01 \\\n",
    "--resume checkpoints/cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 5e-4 --batch-size 128 --epochs 26 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar10_allcnn_1_0_forget_[5]_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar10_allcnn_1_0_forget_[5]_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Number of Classes: 10\n",
      "Epoch: [0][0/313]\tTime 0.021 (0.021)\tData 0.011 (0.011)\tLoss 2.3694 (2.3694)\tAcc@1 19.531 (19.531)\tAcc@5 55.469 (55.469)\n",
      " * Acc@1 72.082 Acc@5 97.962\n",
      "[0] test metrics:{\"loss\": 0.5740585956573486, \"error\": 0.2042222220632765}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 5.15 sec\n",
      "Epoch: [1][0/313]\tTime 0.014 (0.014)\tData 0.008 (0.008)\tLoss 0.5384 (0.5384)\tAcc@1 85.156 (85.156)\tAcc@5 98.438 (98.438)\n",
      " * Acc@1 84.283 Acc@5 99.603\n",
      "Epoch Time: 4.23 sec\n",
      "Epoch: [2][0/313]\tTime 0.015 (0.015)\tData 0.008 (0.008)\tLoss 0.3554 (0.3554)\tAcc@1 89.062 (89.062)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 88.745 Acc@5 99.787\n",
      "Epoch Time: 4.24 sec\n",
      "Epoch: [3][0/313]\tTime 0.017 (0.017)\tData 0.012 (0.012)\tLoss 0.2604 (0.2604)\tAcc@1 92.188 (92.188)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 92.095 Acc@5 99.880\n",
      "Epoch Time: 4.21 sec\n",
      "Epoch: [4][0/313]\tTime 0.022 (0.022)\tData 0.017 (0.017)\tLoss 0.0928 (0.0928)\tAcc@1 98.438 (98.438)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 94.772 Acc@5 99.923\n",
      "Epoch Time: 4.24 sec\n",
      "Epoch: [5][0/313]\tTime 0.016 (0.016)\tData 0.010 (0.010)\tLoss 0.0902 (0.0902)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 97.012 Acc@5 99.965\n",
      "[5] test metrics:{\"loss\": 0.45968301815456813, \"error\": 0.13466666661368476}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 5.01 sec\n",
      "Epoch: [6][0/313]\tTime 0.018 (0.018)\tData 0.012 (0.012)\tLoss 0.0561 (0.0561)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 98.645 Acc@5 99.978\n",
      "Epoch Time: 4.09 sec\n",
      "Epoch: [7][0/313]\tTime 0.015 (0.015)\tData 0.008 (0.008)\tLoss 0.0510 (0.0510)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.405 Acc@5 99.987\n",
      "Epoch Time: 3.99 sec\n",
      "Epoch: [8][0/313]\tTime 0.015 (0.015)\tData 0.010 (0.010)\tLoss 0.0358 (0.0358)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.762 Acc@5 99.997\n",
      "Epoch Time: 4.03 sec\n",
      "Epoch: [9][0/313]\tTime 0.024 (0.024)\tData 0.018 (0.018)\tLoss 0.0176 (0.0176)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.912 Acc@5 100.000\n",
      "Epoch Time: 4.48 sec\n",
      "Epoch: [10][0/313]\tTime 0.016 (0.016)\tData 0.010 (0.010)\tLoss 0.0162 (0.0162)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.965 Acc@5 100.000\n",
      "[10] test metrics:{\"loss\": 0.5541221812566122, \"error\": 0.131888888835907}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 5.44 sec\n",
      "Epoch: [11][0/313]\tTime 0.018 (0.018)\tData 0.011 (0.011)\tLoss 0.0099 (0.0099)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.980 Acc@5 100.000\n",
      "Epoch Time: 4.47 sec\n",
      "Epoch: [12][0/313]\tTime 0.026 (0.026)\tData 0.021 (0.021)\tLoss 0.0080 (0.0080)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.987 Acc@5 100.000\n",
      "Epoch Time: 4.53 sec\n",
      "Epoch: [13][0/313]\tTime 0.014 (0.014)\tData 0.007 (0.007)\tLoss 0.0052 (0.0052)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 4.55 sec\n",
      "Epoch: [14][0/313]\tTime 0.021 (0.021)\tData 0.015 (0.015)\tLoss 0.0072 (0.0072)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.37 sec\n",
      "Epoch: [15][0/313]\tTime 0.024 (0.024)\tData 0.018 (0.018)\tLoss 0.0047 (0.0047)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[15] test metrics:{\"loss\": 0.6114944379594591, \"error\": 0.13122222216924032}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 5.26 sec\n",
      "Epoch: [16][0/313]\tTime 0.023 (0.023)\tData 0.016 (0.016)\tLoss 0.0062 (0.0062)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.53 sec\n",
      "Epoch: [17][0/313]\tTime 0.026 (0.026)\tData 0.019 (0.019)\tLoss 0.0094 (0.0094)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.47 sec\n",
      "Epoch: [18][0/313]\tTime 0.026 (0.026)\tData 0.020 (0.020)\tLoss 0.0248 (0.0248)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 4.45 sec\n",
      "Epoch: [19][0/313]\tTime 0.015 (0.015)\tData 0.008 (0.008)\tLoss 0.0045 (0.0045)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.4 sec\n",
      "Epoch: [20][0/313]\tTime 0.014 (0.014)\tData 0.008 (0.008)\tLoss 0.0036 (0.0036)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[20] test metrics:{\"loss\": 0.65338273885515, \"error\": 0.13355555550257364}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 5.17 sec\n",
      "Epoch: [21][0/313]\tTime 0.031 (0.031)\tData 0.025 (0.025)\tLoss 0.0043 (0.0043)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.46 sec\n",
      "Epoch: [22][0/313]\tTime 0.014 (0.014)\tData 0.008 (0.008)\tLoss 0.0051 (0.0051)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.31 sec\n",
      "Epoch: [23][0/313]\tTime 0.016 (0.016)\tData 0.008 (0.008)\tLoss 0.0036 (0.0036)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 4.35 sec\n",
      "Epoch: [24][0/313]\tTime 0.014 (0.014)\tData 0.008 (0.008)\tLoss 0.0049 (0.0049)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 4.28 sec\n",
      "Epoch: [25][0/313]\tTime 0.025 (0.025)\tData 0.019 (0.019)\tLoss 0.0046 (0.0046)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[25] test metrics:{\"loss\": 0.7024074343575372, \"error\": 0.1321111110581292}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 5.13 sec\n",
      "Pure training time: 112.63 sec\n"
     ]
    }
   ],
   "source": [
    "%run main_merged.py --dataset cifar10 --model allcnn --dataroot=data/cifar-10-batches-py/ --filters 1 --lr 0.01 \\\n",
    "--resume checkpoints/cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 5e-4 --batch-size 128 --epochs 26 \\\n",
    "--forget-class 5 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 1620010\n"
     ]
    }
   ],
   "source": [
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model0 = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 4000#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model0.cuda()\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _  = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=512, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=256, seed=seed, shuffle=True)\n",
    "\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "36000\n",
      "10000\n",
      "40000\n",
      "{6: 4000, 9: 4000, 4: 4000, 1: 4000, 2: 4000, 8: 4000, 3: 4000, 7: 4000, 5: 4000, 0: 4000}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'sgd'\n",
    "args.gamma = 0.99\n",
    "args.alpha = 0.001\n",
    "args.beta = 0.95\n",
    "args.smoothing = 0.0\n",
    "args.msteps = 3\n",
    "args.clip = 0.2\n",
    "args.sstart = 4\n",
    "args.kd_T = 4\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_batch_size = 256\n",
    "args.del_batch_size = 512\n",
    "args.sgda_epochs = 4\n",
    "args.sgda_learning_rate = 5e-4\n",
    "args.lr_decay_epochs = [3,5,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> SCRUB unlearning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihao/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 100.000 \n",
      "maximize loss: -23.06\t minimize loss: 0.01\t train_acc: 100.0\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 99.392 \n",
      "maximize loss: -44.11\t minimize loss: 0.03\t train_acc: 99.39167022705078\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 98.150 \n",
      "maximize loss: -68.76\t minimize loss: 0.06\t train_acc: 98.1500015258789\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 99.708 \n",
      "maximize loss: 0.00\t minimize loss: 0.02\t train_acc: 99.70833587646484\n"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "acc_vs = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> SCRUB unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True, prefix=\"train_cifar10_allcnn\")\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True, prefix=\"forget_cifar10_allcnn\")\n",
    "    acc_v, acc5_v, loss_v = validate(valid_loader_full, model_s, criterion_cls, args, True)\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "    acc_vs.append(100-acc_v.item()) \n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\")\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    \n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n",
    "acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "acc_v, acc5_v, loss_v = validate(valid_loader_full, model_s, criterion_cls, args, True)\n",
    "acc_rs.append(100-acc_r.item())\n",
    "acc_fs.append(100-acc_f.item())\n",
    "acc_vs.append(100-acc_v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlQAAAHSCAYAAAAqmVVxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACDeElEQVR4nO3dd3wT9f8H8FfadO9FobSUPcreQ6BlCIggyBBRkSEOUBFkCF8RioqAgoKDH8MBypIpAgoyyt5Ly1baMgqF0pXulc/vj5CjadI2bdJe076emgeXu8/dvT+59PLO5z75nEIIIUBEREREJWYldwBERERElo4JFREREZGJmFARERERmYgJFREREZGJmFARERERmYgJFREREZGJmFARERERmYgJFREREZGJmFARERERmYgJFZEJoqKioFAooFAoEBUVJXc4FqOw183U15THBFL9Dx48WKL1w8PD8cILL6BatWpQKpVQKBRo0aKFWWMkqmiYUAEQQmDTpk14/vnnERgYCAcHBzg7O6NOnTro3Lkz3n//fWzbtg0qlarQ7WRmZuLHH3/E0KFDUbt2bbi4uMDOzg7VqlVDjx498OmnnyIyMlJvvYMHD0onwLwPpVIJLy8vdOrUCR9//DEePXpU4L7zbsOYk2hISAgUCgVCQkL0lo0aNcpgPAqFAs7OzmjcuDHGjRuHS5cuFbkfOUVFRSE0NBShoaFyh0JkMSIjI/HUU09h06ZNiImJgZubG3x9feHt7S13aGVm8eLFCA0NxcWLF+UOxewuXryI0NBQLF68WO5QKh5RySUkJIjg4GABQHoolUrh6ekplEqlzvyffvqpwO3s2LFDVK9eXae8nZ2dcHd3FwqFQppnbW0txo0bp7NuWFiYtNzDw0P4+voKX19f4e7urrM9Hx8fce7cOYP7z7uNsLCwIuutrXNwcLDespEjRwoAwsrKSorF19dXeHt769Xl+++/L3Jfcsn7mpSWu3fvigYNGogGDRqIu3fvltp+KprIyEjp2ERGRuosM/U1LWzblUVxzgX5ffDBBwKAqFu3rrhz5475g7MAgYGBRZ7zLdVPP/0kAIjAwEC5Q6lwKn0L1auvvopDhw7B2toakydPxo0bN5CZmYm4uDikp6fj77//xoIFC9C8efMCt7F8+XIMGDAA0dHRCAgIwHfffYfbt28jIyMDCQkJyMzMxOHDh/H2229DqVRi3bp1BW5r69atiImJQUxMDBISEpCYmIgvv/wStra2iI2NxdChQ5GdnV0aL4WegIAAKZaYmBjExsYiMzMTf/75J2rXro3c3FyMHz++0l5WAYDq1avj2rVruHbtGqpXry53OBUCX1N5hYeHAwAGDBgAf39/maMhshyVOqH6999/sWPHDgDAp59+ioULF6JevXqwstK8LEqlEs2aNcO0adNw8eJFDBs2TG8bx44dwzvvvAO1Wo2uXbsiPDwc48ePR0BAgFTGxsYGXbp0wbfffosbN26gc+fORsfo5uaGSZMmYebMmQCAiIgIhIWFmVJtk9jY2KBPnz5YvXo1ACArKwt//vmnbPEQkXmlpaUBAJydnWWOhMiyVOqEKu/18QEDBhRZ3sHBQW/e5MmTkZOTgypVqmDLli1wc3MrdBs1atTA77//XuxY+/TpI01fvny52OubW94OqikpKcVeX9tPa9SoURBC4Pvvv0fnzp3h5eUFhUKBVatW6ZSPiYnB9OnT0bx5c7i5ucHe3h61a9fG2LFjceXKFb3t16xZE926dZOe5+8LNmrUKGlZdnY29u7diwkTJqBNmzaoVq0abG1tUaVKFfTu3Rvr16+HEMJgPQrrAJ23XxsA/PfffxgzZgwCAgJgZ2cHf39/vP7664iOji7262cOEyZMgEKhQKtWrQotl5KSAicnJygUCqxZs0aab8rrVhhjOpVHR0fjzTff1HktR48ejf/++6/Y+yuJ8+fP4+OPP0bXrl0RGBgIe3t7uLu7o0OHDliwYEGhfxN5+zomJydj5syZaNiwIRwcHODl5YV+/frh1KlThe4/ISEBU6dORZ06dWBvb49q1aph6NChOHfuXInrVLNmTZ0+mHPmzNH5m8nfNzMmJgZTp05F48aN4ezsDCcnJzRu3BjTpk3DgwcPDO4j/7G9efMm3njjDdSqVQt2dnaoWbOmTvlbt27htddeg7+/v95xNuZ9kpubi1WrVqF3797w9fWFra0tfHx80Lt3b2zYsEHv/RkaGgqFQoFbt24BAEaPHq137jC39PR0LFy4EB07doSHhwdsbGzg4+ODoKAgjBw5Elu2bClw3Zs3b+Ldd99Fo0aN4OzsDEdHRzRq1AgTJ07E7du39corFAqMHj0agOa1zV+3kvQ3TUpKwty5c9G+fXt4eHjAzs4OAQEBGD58OE6ePGlwHWPfB/nPoRcuXMDLL78Mf39/2NjY6PUBLov3ZKHkveIor40bN0p9Df76669ir3/69Glp/U8++aTEcRjT/+nUqVNSmS+++KJE28jLmD5UhV1jP3r0qLS/7du3F7m/gvbx6quviiFDhkh9tjw8PISVlZVO34UdO3YIZ2dnaX82NjbCyclJem5raytWr16ts/02bdoIDw8PqUzevmC+vr5iwoQJUtm8rx0e933Luz8AYujQoSI3N1evHoX118m73QMHDkjbdHFx0emf5+fnJ0v/qzNnzkgxXLp0qcByq1atEgCEs7OzSElJkeaX1utWVB+oc+fO6RxbBwcHab+urq7i119/LfU+VHnraGVlpdffMSgoSDx48KDQddetWyfq1q0rAAh7e3vh6Oio8x7fvXu3wfUjIyOlPj7a97+rq6s0vX379hL1oWrTpo3w9fUVNjY2AoBwcnLS+Zs5duyYVPbgwYM6dXZ0dNT5m/Tw8BBHjhwxGLu2zNq1a6Xjpl0/7znn+PHjwsXFpcDjnPf8beg4x8TEiPbt2+scFzc3N53nzz33nMjMzJTW+eKLL4Svr6+wsrKS9pP/3GFOKpVKNG/eXIpHoVAId3d3nfNDQefhFStWSMdK+/fn4OAgPXd1ddX7XPP19ZXeK/n7yPr6+hr8bCnMyZMnha+vr06/2rzHTKFQiM8++0xvPWPfB3nPMZs3b5bq6+rqKuzt7XU+v8riPVmUSp1QRUZGSp2smzZtKq5fv16s9efNm2fUB1JRjEmG5syZI5XZsmVLibaRV0kTqqysLLFnzx7pgyAoKEhkZ2cXub+C9uHs7CyUSqVYuHChSEpKEkIIkZycLO7duyeE0CSStra2AoB48803xdWrV0VOTo4QQohbt26J8ePHC0DzQ4IzZ87o7MPYTuknT54UL730kti1a5eIiYkRarVaCCFEXFycWLJkiXQCWrJkid66xiZUHh4e4rnnnhNXr14VQgiRmZkpfv31V+nkM2LEiGK/huYQFBQkAIgPPvigwDI9evSQkt+8Sut1K2yZSqUSNWrUEABEjRo1xF9//SXt98SJE6Jx48Y6J9XSSqh69uwpfvzxR3Hr1i3p/Z+Wlia2bt0qGjRoIACI559/3uC6ed8TQUFB4sCBAyI3N1eo1Wpx+vRpaf3AwEC9ZDQnJ0e0adNGWn/jxo3S/i9fviy6dOmiU/+SdErXnhtmz55tcPnt27elfQQFBYmjR49Kyw4fPizF7+npqfdFIe+xdXZ2Fu3bt9f5u9WegxMSEkS1atUEAFG7dm1x4MAB6TifPn1aNG/eXCepzn+cMzMzRdu2bQUA0apVK7Fr1y6RmpoqhBAiJSVFrF69WlSpUkUAEBMnTtSrY1l1Sv/kk0+k12rLli0iIyNDCCFEbm6uiI6OFj///LN4/fXX9dbbtm2blHhPnz5dREVFCbVaLdRqtbh27ZoYOnSolHjcunVLZ11zdUqPjIyU3gdDhgwR586dk96LDx48EB999JGUGG7btk1vXWPeB3nPoc7OzqJv377SOVQIIW7cuCGEKJv3pDEqdUIlhBCvv/66TjbdsmVLMX78ePHDDz+I8PBw6Y/YkFdeeUX6ZmDoW7ixCkuGEhMTxVdffSUlFVWqVBHp6enF2oYhpv7Kz8fHR7z55psiLi6uRHXW7gOA+Prrrwsspz0pfvTRRwWWmTBhggAgBgwYoDPfXL/y27RpkwAg6tSpo7fM2ISqW7duBt8jX3/9tfTtuySJqam0Xwr8/f0Nxnf37l3p2/q+ffuKte2Svm6FLVuwYIEANC0xV65c0dvu/fv3C/2gLQt3794VdnZ2QqFQ6H2YCfEkofLx8THYivXPP/9IZfJ+MAghdFrfDB2P1NRUUadOnVJNqN566y0pobt//77e8jt37kjJ9Ntvv62zLO+xDQwMFMnJyQb3oU007O3txb///qu3PDY2Vnh7exd4nL/99lsBQDRu3FioVCqD+zh79qxQKBTC1tZW7ziUVUL1zDPPCAAGW3EKkpmZKf2i/Icffiiw3HPPPScAiPfee09nvrkSKu2VhcK+DH755ZcCgGjevLnOfGPfB3nPoe3atZO+TOdXFu9JY1T6hCo7O1t89NFHOk2DeR9VqlQRkyZNEjExMXrrav8YTG0GNnbYBBcXF3Hw4MEit2GuhKqwh729vXjxxRfFtWvXSlRn7T48PDx0mtzzunjxovQtLDExscBtnT17VgCaSxR5/+DMlVClpaVJ29G2nGkZm1Dt3bvX4LZv3bollTGUIJS2O3fuFJowaROYghKuwpT0dStsWcuWLQUA8fLLLxe43xkzZsiaUAkhRLt27QQAsX79er1l2tg+/PDDAtevVauWACCWLl2qM//5558XAMRTTz1V4LrLly8vtYRKrVYLT09PAUDMmDGjwG1MmzZNABBeXl468/Me28IuLzVr1kwAECNHjiywzEcffVTgcW7RooUAIL777rsC1xdCiCZNmggAYsOGDTrzyyqhGj58uAAg3n33XaPX+e2336TPncK+8G/evFkAEA0bNtSZb46EKi4uTjpvXL58ucByjx49ko5R3s9QY98Hec+hmzZtMlimrN6TxqjUndIBzS/5Pv74Y0RHR+OXX37B2LFj0bx5c9ja2gIAHj58iK+++gpNmjTB6dOnddYVjzs0mrOjYkJCAh48eIAHDx4gMTFRmt+iRQtcv34dwcHBZttXUQIDAyE0Sbf0SElJwfHjx6VOnR06dMCJEydKvI+2bdtKr3V+R48eBQCo1Wo0aNAAVatWNfjQdthPTU1FXFxcieJITk7GF198geDgYFSpUgW2trZSR0VHR0epXEk7kLdv397gfD8/P2k6Pj6+RNs2hb+/v9Sx85dfftFbrp338ssvS79+zau0X7e8srKypJ/0d+/evcByhS0zF7VajXXr1uG5555DjRo14ODgoNO5V3uuuHv3boHbKOg9ATx5X+R/T5w9exaAfPWPjIyUYurZs2eB5Z5++mkAQFxcnMHBjAHgqaeeMjg/KytL+uFNYec7Q4MSA5r35D///AMA+Oijjwo8b1StWhXXr18HAKkTelnr168fAODbb7/F8OHD8dtvvxU6gDPw5LyYkJCAatWqFVi3119/HUDp1O3EiRNQq9UANO+3gmJo3LixtE5BcRT0PjC2XFm8J42lNGntCsTNzQ2vvPIKXnnlFQBARkYGjh49iq+//ho7duzAo0ePMHjwYPz777+wt7cHAGnk4ISEBKjVaoMfOMUVFhYmnSji4+Nx4sQJadiGcePGYcuWLbC2tjZ5PyXl5OSEjh07YsuWLejYsSPOnDmDESNG4MaNGyWqf5UqVQpcdu/ePQCaX+oU9AuN/LQ/+S6OGzduoEePHjoffo6OjnB3d5fqpN1/ampqsbcPAC4uLgbnK5VP/gTzji/266+/4r333jO4ztatW9GpU6cSxWHIq6++igMHDmDLli1YunSplAhdvHhRGg3/1Vdf1VuvLF63vOLj45GTkwMAhY5PVdDYSeZ6TdPS0tCvXz+d4UtsbW3h6ekJGxsbKdbs7OxC613QewJ48r7IP+bcw4cPAZSs/uag3X9xYnj48CFq1aqlV6agv/34+Hjk5uYC0P3CkV9B+4+JiZE+7I39klLc88adO3fQtm1bg8umTJmCKVOmGLWdl156CadPn8Y333yDDRs2YMOGDQCAunXrolevXhgzZgxat26ts472vJiVlWXUeTE9Pd2oWIpDGwMAk8/NhX0GGFOuLN6Txqr0LVQFsbe3R8+ePfH7779j5MiRADTfNnfv3i2V0WbfmZmZuHr1qtlj8PT0xLPPPouwsDD4+vpi+/bt+OSTTwyWzTukgzF/QNo3t6GhIIxhbW0t/fz25s2beq13xdlOQbQn1YYNG+q1lBX0KNZPXB8bPXo07t69i5o1a2LTpk2Ii4tDamoqHj58iJiYGJ3WFW2rZGlLT0+XWirzP7Kyssy6r8GDB8PR0REpKSnYtm2bNF/bOtW6dWsEBQXprSfn61aSVmFzvaZz585FWFgYHBwc8NVXX+HWrVvIyMhAXFycNAiutvWptN4vhdW/NH7ab8p+CipX0N9+3tessH0U9NpqzxsAcPLkSaPOG8UdLkD7Jc/Qo7jDyCxevBjXr1/HZ599hmeeeQbu7u7477//sHTpUrRp0wYTJ040WL8+ffoYfV40N20MDg4ORsdQUIuisQ0ExpQrrfeksZhQGeGNN96QprVNxADQo0cPaTrvB5G5ValSBfPmzQMAzJ8/3+CYK3nvs2XM5RVtGR8fnxLHFRgYKE0X1IRqiqpVqwLQDGZqjhYOQ+7cuYPjx48DANavX48hQ4bA09NTp0xMTEyp7Lsw2vG5inNiKilnZ2c8//zzAJ4kUbm5uVi/fj0AYMSIEXrryPG6eXp6Sie8wi6lFfT+N9drqm1FmDVrFiZOnIgaNWronaBL6z2j/QZdWP0LW2au/QOa94AxMRT3HOPl5SUd57wtIfkVtMzX11ea1l4iNreaNWuaLTkDNC1SM2bMwB9//IG4uDicOHECAwcOBAAsWbJEZ+xC7XmxtOpmDG0M6enpZTb2W0HK4j1pLCZURsg7YrCdnZ003bZtW7Rr1w6A5hp4Ude+tbTN0cXx6quvok6dOsjMzMSsWbP0lteuXRseHh4AnlxjL0hkZKR0MsrfnFwced+gTk5OJd5OQbTXs7OyskqUsOa9BFnQt7S8f4AtW7Y0WGbfvn3F3rel0V7S27dvH2JiYrBv3z7cv38fSqUSw4cP1ysvx+tma2uLZs2aAUChdws4cOCAWfebn7buBdU7Kiqq1D5k2rRpA0C++teqVUtKnPfv319gOe2x9/LyMnhppTC2trZS639hN3ovaJmHh4fUoqpNfotLe+4oqxbp/Pvu0KEDNm/ejBo1agAA9u7dKy3Xnhejo6OLPNcXtH3AtLp16tRJ+hJR0tfYXMriPWmsSp1QRUZG4saNG0WW095mBYDeqNILFy6EtbU1Hjx4gMGDByMpKanQbd29e1f65lEc1tbW+OCDDwAAa9euxbVr13SWKxQKDB06FACwadMm3Lx5s8BtzZ8/H4Cmn4a2ZaK4hBBSCwZgWmJWkDZt2kgfWh9++CFiY2MLLZ+/v4Srq6s0nbeDf155R7b/+++/9ZYnJyfj008/NTZki9WzZ0/4+fkhNzcXa9eulVqq+vTpY7BfgVyvm/b2T5s2bdJpLdZ6+PAhli1bZvb95qWtu6F6A8D06dNLbd/a+h89etRgQpGeno4vvvii1PavUCikGJYvX26wJe7evXtYvnw5ABhMxo0xZMgQAMDGjRsRERGhtzwuLq7Q46y9qrB///4iP/AN9bPSnjsKOm+YS2ZmZoHLrK2tpR/s5L0U1b9/f1SrVg0A8N577xXZ/6ug86IpdatSpYp0d5EvvviiyM/R0vzBTVm9J41iwi8ELd6OHTuElZWV6Nu3r1i9erXOT2+zsrLE+fPnxahRo3TGwTD00/Fvv/1WGp+pRo0aYunSpTp3ac/KyhLHjh0T7733nnBwcBBubm466xs75EHe8UeGDRumt/zWrVvSGDwBAQFi06ZN0mB2QmgGKBs7dqy0rylTphjcT1EjpUdEROhsx1AsRdHuo7CfRQuhGdjTzs5OABC1atXSq9Pdu3fFL7/8Inr27CnGjh2rs25qaqo0ftfnn39u8CfGarVaGiiycePG4uzZs9Ky48ePi1atWgkvL68Cj4+xwyYUxphjX5i8w1yYYsqUKQKAaNSokTSMyK+//mqwbGm+boUtS0pKEv7+/gKAqFmzpti3b590XE+dOiWaNm1a6gN7asefc3FxEVu2bJHGD4uIiBDDhw8XCoVC+js0NPSAMce7oKELsrOzRatWrQSgGaRw8+bN0lAhV65cEcHBwTqjgZfGOFR37tyRXuPGjRvrjKB+9OhR0ahRIym+wgZRLOzYxMfHSyNw161bVxw8eFA6zmfOnBEtW7YsdLyxjIwMaZR0pVIpPvzwQ3H79m1peWpqqggLCxNvv/22cHd319v/yy+/LACITp06ifj4+CJesZJr3ry5ePfdd0VYWJjOXQiio6PFO++8I9Vvz549Outt27ZN+sxp0aKF2L17t87wMxEREWLZsmWibdu2enfx+Pfff6XtFvT3bYybN29Kf+M+Pj7ihx9+0BneJjY2VmzZskU8//zzolevXjrrGvs+MPYcWhbvSWNU6oRq9+7d0gupfdja2gpPT0/pzap9tGrVSkRHRxe4rd9++00a2Vf7sLe3Fx4eHjrbUiqVeiPzFmcMqa+++koAmkFI//nnH73lx44dE1WrVpW2Z2VlJTw9PXVuawFAvPbaawUOJFnQwJ6+vr46txUAIEJCQgocOK8wxiZUQgjx119/6Xw4W1tbCy8vL7065U+ohBDitddek5Y7OjqKGjVqiMDAQDF58mSpzI4dO3Ru9eDo6Cht29HRUezbt69SJFTh4eE6r6ebm5vBQWS1Sut1K+oEd+bMGb1bTOS9rU9p33omKipK53YbSqVSJ4n57LPPCk1KTEmohNB8kAUEBEjbsbOzk/Zvyq1njNm31sGDB3Xq7OTkpDOWn7u7uzh8+LDeesX58Dpy5IjOrYzyHmd3d3dp4FgABgdzjI2NFd27d9d5T7u6ugp3d3e9c3J+hw4dkspYW1uLatWqicDAQJMHw8wv7y2EtLedyT8m4qRJkwyuu2bNGp1zoFKpFF5eXtIXUO3j008/1VtXe/cD7d+Mtm5fffVVseI/f/68qFmzpk4dPDw89G5B1bNnT531zJ1QCVE278miVOqESghNtr5kyRIxdOhQ0ahRI+Hi4iKsrKyEk5OTqFevnnjhhRfEhg0bjBrUMD09XaxcuVIMGjRI1KxZUzg5OQlbW1vh6+srevToIebOnavzLUmrOAlVamqq8PHxEUDBt7ZITEwUCxcuFCEhIcLHx0colUrh7Ows6tevL0aNGmXwfkZ5FTawp52dnfD39xcDBgwQv/76a6EDyxmzD2MSKiE0t6KYN2+e6Ny5s/D09BTW1tbC2dlZBAUFiddee038/vvvBj/8MzIyRGhoqGjSpInOySf/fo8fPy6effZZ4e7uLmxtbUWNGjXE6NGjpYFLy3NC1bNnTwFAtG/fvkTr56UdEBGAwVte5Fcar5sxJ7jbt2+LsWPHiurVqwtbW1tRvXp1MXLkSPHvv/+a9QRZkDt37ojXXntN+Pn5CaVSKXx9fUW/fv2kloTSTKiE0Ays+P7774tatWpJ55ghQ4ZILYWlnVAJoRmVfvLkyaJRo0bCwcFBODo6ikaNGokpU6YYTHCEKP6HV0REhBg9erTw8/MTtra2wt/fX4wZM0ZERESI8+fPS9sqKPFXq9Vi+/btYsiQISIgIEDY2dlJ57BnnnlGfPvttwXeR/OPP/4QPXv2FJ6entIglqZ+acnvxIkTYs6cOaJHjx6idu3awtHRUdja2orAwEAxbNgwsX///kLXv3fvnpg5c6Zo06aNcHd3F9bW1sLNzU20aNFCvPPOO2Lfvn0GvzgnJCSISZMmifr16wt7e3upbkUdc0PS0tLEt99+K3r27Cl93jg6Oop69eqJl156SWzYsEHvS3dpJFRClM17sjAKIWTodUdEZpGVlQUPDw+kpaVh3759Or88JarIVq5ciTfeeAO1a9cutM8oUVmp1J3SiSzdyZMnkZaWhu7duzOZokojIyMDixcvBgDpTglEcmNCRWTBtD+f/+yzz2SOhMi8NmzYgJkzZ+LSpUvSwKs5OTk4fPgwunfvjitXrsDe3r7A0e+Jyhov+RERUbmzePFiTJo0CYDmp/EeHh5ISUmRkitbW1usXr0aL774opxhEkl4Lz8iIip3+vXrh9jYWBw8eBC3bt3Co0ePYGNjg9q1a6Nbt26YOHEi6tevL3eYRBK2UBERERGZiH2oiIiIiEzES37FoFarce/ePbi4uJTZHd2JiIjINEIIJCcnw8/PT+c+r+bEhKoY7t27h4CAALnDICIiohK4c+cO/P39S2XbTKiKwcXFBYDmgOS98S4RERGVXyqVCgEBAdLneGlgQlUM2st8rq6uTKiIiIgsTGl212GndCIiIiITMaEiIiIiMhETKiIiIiITMaEiIiIiMhETKiIiIiITMaEiIiIiMhGHTSgD2dnZyM3NlTsMqoSsra1hY2MjdxhERBUeE6pSpFKp8OjRI2RmZsodClVidnZ28Pb25thpRESlqNwlVMnJyfjkk09w8eJFXLhwAY8ePcLs2bMRGhqqV/b8+fOYNm0aTp48CaVSie7du2PhwoWoXbu2XtlvvvkG3333HSIjI+Hn54dRo0bhf//7X6l9e1epVIiOjoazszO8vb1hY2PD+/9RmRJCIDs7G0lJSYiOjgYAJlVERKWk3CVUcXFxWLFiBZo3b46BAwfi+++/N1ju2rVrCAkJQYsWLbBx40ZkZGRg1qxZ6NKlCy5evAgfHx+p7Ny5c/HRRx9h+vTp6NWrF86cOYOZM2ciOjoaK1asKJV6PHr0CM7OzvD392ciRbJxcHCAi4sL7t69i0ePHjGhoopLnQvcOg6kPACcfYHAToCVtdxRUXFY+DEsdwlVYGAgEhISoFAo8OjRowITqlmzZsHOzg47d+6UPiRat26NevXqYeHChViwYAEATYL26aef4vXXX8dnn30GAAgJCUF2djZmzpyJiRMnIigoyKx1yM7ORmZmJry9vZlMkewUCgXc3NwQHR2N7Oxs9qmiiufK78DuDwDVvSfzXP2APguAoOfki4uMVwGOYbn7lZ9CoSgyCcnJycHOnTsxePBgnW/cgYGB6NatG7Zt2ybN2717NzIyMjB69GidbYwePRpCCPz2229mjR+A1AGdH1xUXmjfi/xxBFU4V34HNr6q+0EMAKr7mvlXfpcnLjJeBTmG5a6Fyhg3b95Eeno6mjVrpresWbNm2Lt3LzIyMmBvb49Lly4BAJo2bapTrlq1avD29paWlwa2TlF5wfciVUjqXE2rBoSBhQKAAvhjKlC1qUVdOqpU1LnAH1NQ6DHcPR1o+Gy5P4YWmVDFxcUBADw9PfWWeXp6QgiBhIQEVKtWDXFxcbCzs4OTk5PBstptGZKZmanzCz2VSmWG6Imo3LHwvhvljhBATiaQkw5k53nkZADZaUD243+l58aUyXhcJs+8zFRAnVVYIEBKDPB1i7KqOZmdAFTRmr/PWl3kDqZQFplQaRX2rTvvMmPL5Tdv3jzMmTOnZMERkWWoAH03jCIEkJudJ8nJk6ToJT5FPDdYJl8iZLDFQSZWNkyQyyt1LqDOLrpcyoPSj8VEFplQeXl5AYDB1qX4+HgoFAq4u7tLZTMyMpCWlgZHR0e9sq1bty5wPzNmzMD7778vPVepVAgICDBDDUhONWvWBABERUXJGgeVA9q+G/k//LV9N174ufSTqtwcAy0x+VtvDLXWFNF6YyhZEjL0oVNYAzaOgI09YOMAKB00/2of0nN7TTnl43+NKfMgHNgytugYRmwr960blVbkEWB1v6LLOfuWfiwmssiEqk6dOnBwcEB4eLjesvDwcNStWxf29vYAnvSdCg8PR/v27aVyMTExePToEZo0aVLgfuzs7GBnZ2fm6Km4QkNDMWfOHISFhSEkJETucGSlUCgQHByMgwcPyh2K5TOm/82f0zT9b3KzjGiZKUbrTd556pwyrjg0ddNLWuzzJD6O+Z7nTWqMKZMnEbK2AUqrD593PWDvLE0CbPA4KjStjYGdSmf/ZLrATppjVAGOoUUmVEqlEv3798fWrVvx+eefw8XFBQBw+/ZthIWFYdKkSVLZPn36wN7eHqtWrdJJqFatWgWFQoGBAweWdfgks/3798sdApUHt47r/6pIhwCS75dt/xtjkxSDiZAxZR7/a21beklOWbKy1lya3fgqAAV0P5Af16/PfF7uK88q0DEslwnVn3/+idTUVCQnJwMArly5gs2bNwMA+vbtC0dHR8yZMwdt27ZFv379MH36dGlgT29vb0yePFnalqenJ2bOnImPPvoInp6e0sCeoaGhGDt2rNnHoKLyr06dOnKHQOWBsX0yrGwAO+eik5T8iZDevIKe59leRUhyylrQc5pLswb7wc2vWP3gKqqKcgxFORQYGCigSVP1HpGRkVK5s2fPih49eghHR0fh6uoqBg4cKP777z+D21yyZImoX7++sLW1FTVq1BCzZ88WWVlZxYorKSlJABBJSUmFlktPTxdXrlwR6enpxdp+Sfx9J0G8uPyE+PtOQqnvqzjCwsIEADF79mxx/Phx0atXL+Hm5ia0bzm1Wi1++OEH0alTJ+Hi4iIcHBxE69atxQ8//KCzneDgYIPvg8DAQKnMgQMHxOjRo0X9+vWFk5OTcHJyEq1btxbLly83GFtgYKDO+kIIMXv2bAFAhIWFiV9//VW0bNlS2Nvbi6pVq4p3331XpKWlFav+Bw4cEH369BHVqlUTtra2olq1aiI4OFisXLlSr2xERIR47bXXREBAgLC1tRVVq1YVI0eOFFFRUXqvp6HHTz/9VGQ8ZfmetBgRh4WY7Vr0I+Kw3JGSMXJzNMfqn02af3Nz5I6IiqsUj6Gxn9+mKJctVMZ2Fm7dujX27dtnVNkJEyZgwoQJJkRVPm09H40TEXHYej4azfzd5Q5Hz/Hjx/HZZ5+hW7dueOONN3D79m0IIfDKK69g3bp1qF+/Pl566SXY2tpi7969eO2113DlyhUsXLgQADBq1CgAwKFDhzBy5EipQ7n2RwcAsGDBAvz333/o0KEDnn/+eSQmJmL37t148803cf36dSxatMjoeL/77jv8+eefGDBgAEJCQrB792588803iIuLw9q1a43axq5du9C/f3+4u7tjwIABqFatGmJjY3Hx4kWsXbsWY8c+6UR76tQp9O7dG6mpqejfvz/q1q2LqKgorF27Fn/++SdOnDiB2rVro2bNmpg9ezbmzJmDwMBA6XUBgBYtWhhdP8pD6rtR0GU/y+m7QdBcEmLHc8tm6cew1FK1CsgcLVRqtVqkZmab9LjxQCVORz4SZyLjRMuP/xKBH+wULT/+S5yJjBOnIx+JGw9UJu9DrVab9FrlbVHJ3+q0YsUKAUC89tprIjs7W5qfmZkp+vfvLwCIs2fPSvPzth4ZEhERoTcvOztbPP3008La2lrcunVLZ1lhLVRubm7i2rVr0vy0tDRRv359oVAoRHR0tFF1HzRokAAg/v77b71ljx49kqazsrJEzZo1hYuLi7h48aJOuSNHjghra2vRr18/nfkARHBwsFFx5MUWqgJc3l5Ay5Sb5nF5u9wREpEZVNoWqoosPTsXQbP2mH278alZGLLshNm2d+Xj3nC0Nf3t0bJlS4wZM0Zn3rfffgsnJyd8++23UCqf7MPW1hZz587Fjh07sH79+kKHtMirVq1aevOUSiXeeust7N27F2FhYRg5cqRR23rvvffQoEED6bmDgwOGDx+OOXPm4Ny5c/Dz8zNqO9p189MO+QEAO3fuRFRUFD755BM0b95cp1znzp0xYMAA/Pbbb1CpVLypcWnxCDQ839L6bhCR7JhQUalq166dzvO0tDSEh4fDz88P8+fP1yufna0Z4O3atWtG7yM5ORkLFy7Eb7/9hps3byI1NVVn+b17hf2SS1erVq305vn7+wMAEhMTpXmhoaF65SZOnAh3d3e88MIL2Lp1K9q3b4/hw4eje/fu6NKlC6pUqaJT/uTJkwA0dTW0vZiYGKjVaty4cQNt2rQxug5UDIc+1/zbeDDQZjRHSieiEmNCVcYcbKxx5ePeJm/nyj2VwRapzW91RJCf6a0ZDjbm+TDx9dUdjC0hIQFCCERHRxc6Cn3+pKggWVlZCAkJwfnz59GyZUuMGDECXl5eUCqViIqKwurVq3VuH1QUNzc3vXnaVrS8NxY2FPuoUaPg7u6OYcOGwcbGBosXL8by5cuxdOlSKBQKhISE4Msvv5T6PMXHxwNAkX2zjH0tqJhiwoFrOwEogJAPAJ8GRa5CRFQQJlRlTKFQmOVSmv3jhEeh0NxRQvuvvY21WbZvLvlv7aO9dNW6dWucPXvW5O1v374d58+fx9ixY7Fy5UqdZRs2bMDq1atN3ochQhR+W41BgwZh0KBBUKlUOH78OLZu3YoffvgBvXv3xvXr1+Hu7i69Fjt27EC/fkaMFEzmdWiB5t/GzzOZIiKTWckdAJWMl7MtfJzt0LS6G+Y+3wRNq7vBx9kOXs62codWKBcXFzRq1AhXr17VuYRWGGtrTfKYt4VI6+bNmwCA557T7+ty5MiRkgdqJq6urujTpw9WrFiBUaNG4eHDhzh16hQASAPNnjhhfN83Kysrg68DFVPMJeDqDgAKIHia3NEQUQXAhMpCVXNzwNHp3bD97afwcvtAbH/7KRyd3g3V3PQ7Qpc3EyZMQFpaGl5//XWDl7MiIyN1hs7w9PQEANy9e1evbGCgplPx0aNHdeYfOnRIr8WqrOzfvx8ZGRl68x8+fAjgSWf1AQMGoEaNGvjyyy9x+PBhvfLZ2dl69fL09DT4OlAxHX7cdypoAFClkbyxEFGFUH6uDVGx2Smf9HNSKBQ6z8uzN998EydPnsTq1atx7Ngx9OzZE35+fnjw4AGuXbuGU6dOYd26ddKYU926dYNCocCHH36Ia9euwc3NDW5ubhg3bhz69++PmjVr4vPPP8elS5fQpEkTXL9+HTt37sTAgQOxZcuWMq/f5MmTcfv2bYSEhKBmzZpQKBQ4evQoTp8+jU6dOuGpp54CoLlX5ObNm/HMM88gODgYPXr0kO4tefv2bRw5cgReXl46HfS7d++OjRs3YsiQIWjZsiWsra3x7LPPSvesJCM8uAJc2a6ZZusUEZkJEyoqcwqFAqtWrULfvn2xcuVK7Ny5EykpKahSpQrq1auHhQsXomfPnlL5oKAg/PTTT1i0aBG++uorZGZmIjAwEOPGjYOzszMOHDiAqVOn4vDhwzh48CAaN26MtWvXwtfXV5aEasaMGdi6dSvOnTuHPXv2wMbGBrVq1cLnn3+O8ePHS5cwAaBt27b4+++/8cUXX+CPP/7A0aNHYWdnh+rVq2PgwIEYPny4zraXLFkCADhw4AC2bdsGtVqNqlWrMqEqDm3rVKPnAN/G8sZCRBWGQhTVu5YkKpUKbm5uSEpKKnRcoIyMDERGRqJWrVqwt7cvwwiJDON78rGH14ClHQAI4K1jQNUmckdERGXA2M9vU7APFRFVHoc/ByCAhv2YTBGRWTGhIqLKIfY6cGmrZjr4A3ljIaIKhwkVEVUOh78AIIAGzwLVmskdDRFVMEyoiKjie/QvcOnxDxT4yz4iKgVMqIio4jv8BSDUQP1nAL8WckdDRBUQEyoiqtge/QeEb9JMh7DvFBGVDiZURFSxHVmoaZ2q1xvwayl3NERUQTGhIqKKK+4m8M9GzTRbp4ioFDGhIqKK68giQOQCdZ8GqreWOxoiqsCYUBFRxRQfAfy9QTMdMl3eWIiowmNCRUQVk7Z1qk4PwL+N3NEQUQXHhIqIKp6EKLZOEVGZYkJFRBXPkUWAOgeo3Q0IaCd3NERUCTCholKTlZWFmTNnok6dOrC1tYVCocDBgwflDosquoRbwMV1mmm2ThFRGWFCRaVm4cKFmDt3LmrUqIFp06Zh9uzZqFmzptxhFcvBgwehUCgQGhoqdyiSVatWQaFQYNWqVXKHUj4d/VLTOlUrGKjRQe5oiKiSUModAFVcf/zxB5ydnfHXX3/BxsZG7nCoMki8A1xYq5lm6xQRlSEmVJZMnQvcOg6kPACcfYHAToCVtdxRSe7duwcvLy8mU1R2jn4JqLOBml00fw9ERGWEl/ws1ZXfgcVNgNX9gC2vaf5d3EQzX2ahoaFQKBSIjIzErVu3oFAooFAoEBISAgDIycnBV199hebNm8PBwQFubm7o1q0bdu3apbetvJe3du3ahS5dusDFxUXn0mFUVBSGDRsGT09PODs7Izg4GIcPH5biMNRv6/Dhw+jfvz+8vb1hZ2eHevXqYebMmUhLS9OpR7du3QAAc+bMkeqhUCgQFRVV5OuQkZGBRYsWoXnz5nBzc4OzszPq1KmD4cOHIzw8XK/89u3b0aNHD3h4eMDe3h5NmjTBwoULkZubK5UZNWoURo8eDQAYPXq0TkyVXtJd4Pwvmmm2ThFRGWMLlSW68juw8VUAQne+6r5m/gs/A0HPyRIaAClxWrx4MQBg4sSJAICaNWtCCIFhw4Zh69atqF+/Pt5++22kpqZi48aN6NevH5YsWYIJEybobXPTpk3466+/0K9fP4wfPx7JyckAgOjoaHTq1An3799H37590bx5c1y/fh29evWSkqH8li1bhvHjx8PDwwP9+/eHj48Pzpw5g7lz5yIsLAxhYWGwtbVFSEgIoqKisHr1agQHB0v1AgB3d/ciX4eRI0di48aNaNasGUaPHg07Ozvcvn0bYWFh6N27N5o2bSqV/d///od58+bB398fgwcPhqurKw4fPoypU6fi1KlT2LRJc3PfgQMHIjExEdu3b8eAAQPQokWLIuOoNI5+pWmdCuwM1OwsdzREVNkIMlpSUpIAIJKSkgotl56eLq5cuSLS09P1F6rVQmSmlPyRniTEwgZCzHYt4OEmxKKGmnKm7EetNvn1CgwMFIGBgTrzfv75ZwFABAcHi8zMTGn+nTt3RJUqVYSNjY2IiIiQ5v/0008CgFAoFGLv3r16+3jllVcEAPHFF1/ozNeuB0CEhYVJ8y9fviyUSqVo2bKliIuL01ln3rx5AoBYuHChNC8sLEwAELNnzy5W3RMTE4VCoRBt2rQROTk5OstycnJEQkKC9Pyvv/4SAMQzzzwjUlNTpflqtVq89dZbAoDYvHmzXt1++ukno+Mp9D1ZESTeFeJjb83fQMQhuaMhonLG2M9vU7CFqqxlpwGf+ZXiDgSgugfMDzBtM/+7B9g6mSekPLS/TPv8889ha2srzff398ekSZMwY8YMrF27FjNnztRZb+DAgejZs6fOvMzMTGzatAm+vr56rVojR47EggULcO3aNZ35y5cvR05ODr7++mt4enrqLJs2bRq+/PJLrF+/HpMnTzapngqFAkII2NnZwdpat1+btbW1TgvXt99+K8Xm6Oios4358+dj+fLlWL9+PQYPHmxSTBXascVAbhZQo5Om/xQRURljQkVl6sKFC3BwcEC7dvqDLWovqV28eFFvmaHy169fR2ZmJtq0aaOTnAGaZKRjx456CdXJkycBALt378a+ffv0tmljY6O3TkEuXryI3377TWdezZo1MWrUKLi6uqJPnz7YvXs3WrVqhSFDhqBLly5o3769XqwnT56Ek5MTfvjhB4P7cXBwMDqmSkl1Hzi3WjMd8gHA/mREJAMmVGXNxlHT+lNSt44Da4cUXe7lzab9ysnGsegyJaBSqRAQYLj1rGrVqgCApKQkvWW+vr4GtwUAPj4+BrdnaJ34+HgAwNy5c40LuBAXL17EnDlzdOYFBwdj1KhRAIDNmzfjs88+w/r16/Hhhx8CAFxcXDBmzBh89tlnUmtUfHw8cnJy9LaVV2pqqsnxVljHFgO5mUBAB83YU0REMmBCVdYUCtMupdXpDrj6ab6V5++UrtmBZnmd7uVqCAUtV1dXPHjwwOAy7XxXV1e9ZYZ+xaYtFxsbW+j2DK2jUqng4uJiXNAFGDVqlJQ8GeLk5IS5c+di7ty5iIyMRFhYGJYtW4YlS5YgPT0dy5cvl2JSKBR49OiRSfFUSskxwLlVmmm2ThGRjDhsgqWxsgb6LHj8JP+Hx+PnfeaXy2QKAFq2bIn09HScPn1ab9mhQ4cAwOhfrjVo0AB2dnY4d+4csrKydJYJIaTLe3m1b98eAAwuM0Tb/ynv0AUlUatWLYwZMwaHDh2Cs7Mzfv/9yfAW7du3R1xcHP79998yjalCOLYEyMkA/Ntp7ttHRCQTJlSWKOg5zdAIrtV057v6yT5kQlFGjhwJAJgxYways7Ol+dHR0fjyyy+hVCrx8ssvG7UtOzs7DBkyBDExMfj66691lv3888+4evWq3jrjx4+HUqnEu+++izt37ugtT0xMxIULF6Tn2o7rd+/eNSomrdjYWINJY0JCAjIzM+Hg4CDN03aoHzNmDOLi4vTWiYmJ0alLSWOqcJIfAGd/1EyzdYqIZMZLfpYq6Dmg4bPleqR0Q0aMGIGtW7di+/btaNasGfr16yeNQxUXF4dFixahdu3aRm9v3rx52LdvH6ZOnYqwsDC0aNEC169fx86dO6VO4VZWT743NGnSBEuXLsW4cePQoEED9O3bF3Xq1IFKpUJERAQOHTqEUaNGYdmyZQCAhg0bws/PDxs2bICjoyP8/f2hUCgwbtw4uLm5FRhXdHQ02rdvj8aNG6NVq1aoXr064uLisH37dmRnZ2PatGlS2T59+uCjjz7CJ598grp166JPnz4IDAxEXFwc/vvvPxw5cgSffvopGjVqBADo2LEjHBwcsHjxYqhUKqkP2fTplWwwy+Nfa1qnqrcB6vSQOxoiquxKbUCGCsgs41BVIobGoRJCiOzsbLFw4ULRtGlTYWdnJ1xcXERwcLDYvn27XlljxlyKiIgQQ4cOFW5ubsLR0VF06dJFHDp0SLzzzjsCgLhw4YLeOqdPnxYvvvii8PPzEzY2NsLb21u0atVKTJ8+XVy9elWn7MmTJ0VwcLBwcXGRxraKjIwstO4JCQkiNDRUdO3aVVSrVk3Y2toKPz8/0adPH7Fnzx6D6+zdu1f0799f+Pj4CBsbG1G1alXRsWNH8cknn4jbt2/rlN21a5do27atcHBwkGIqTIV7TyY/EOITX824Uzf+kjsaIirnymIcKoUQwlDPZjJApVLBzc0NSUlJBjtOa2VkZCAyMhK1atWCvb19GUZIeXXu3BknTpxAUlISnJ2d5Q5HVhXuPfnXTOD4N4BfK+D1A7zcR0SFMvbz2xTsQ0UW7/79+3rz1q5di2PHjqFnz56VPpmqcFJigTOPx+wKmc5kiojKBfahIovXpEkTtGzZEkFBQbC2tsbFixdx8OBBuLi4YOHChXKHR+Z24hvNHQf8WgL1eskdDRERACZUVAG89dZb2LFjB86ePYvU1FT4+PjgpZdewkcffYSGDRvKHR6ZU2occPp7zXQwf9lHROUHEyqyeNrBM6kSOPENkJ0KVGsO1O8jdzRERBL2oSIiy5AWD5xeqZlm6xQRlTNMqIjIMpz4FshKAao2BRr0lTsaIiIdTKiIqPxLiwdOrdBMs3WKiMohJlREVP6dXApkJQO+TYAGz8odDRGRHiZURFS+pScAp5ZrpoOnAVY8bRFR+cMzExGVbyf/D8hUAVWCgIb95Y6GiMggJlREVH6lJwInNTeqZusUEZVnPDsRUfl1ahmQmQT4NAIaDZA7GiKiAjGhIosTEhICRb5feR08eBAKhQKhoaEmbYfKkYwkTWd0AAieytYpIirXeIYikllJksFK4dRyTVLl3QAIGih3NEREheKtZ6hCaNeuHa5evQpvb2+5QyFzyFABJ77TTAdPA6ys5Y2HiKgIFt1CdeHCBQwcOBB+fn5wdHREw4YN8fHHHyMtLU2n3Pnz59GzZ084OzvD3d0dgwYNQkREhExRU2nQHn8mVBXE6eVARiLgXR9o/Lzc0RARFcliE6orV66gU6dOiIqKwuLFi7Fz5068+OKL+PjjjzF8+HCp3LVr1xASEoKsrCxs3LgRP/74I27cuIEuXbogNjZWxhpUXIcPH4ZCocBrr71mcPndu3dhbW2NHj16AADOnTuHd955B02aNIGbmxscHBzQtGlTzJ8/H9nZ2Ubts7DLZkePHkVwcDCcnJzg5eWFYcOG4c6dO8Wul1qtxvfff4927drB09MTjo6OqFmzJgYOHIjDhw/rlT98+DD69+8Pb29v2NnZoV69epg5c6ZOwh8aGopu3boBAObMmQOFQiE9oqKiih1jhZCZ/KR1qutUtk4RkUWw2Et+69atQ0ZGBrZs2YI6deoAALp374779+9jxYoVSEhIgIeHB2bNmgU7Ozvs3LkTrq6uAIDWrVujXr16WLhwIRYsWCBnNUwWkxqD26rbqOFaA1WdqsodDgCgS5cuqFmzJrZs2YLvvvsO9vb2OsvXrl0LtVqNESNGAABWrlyJHTt2oGvXrujbty/S0tJw8OBBzJgxA2fOnMGWLVtKHMv+/fvxzDPPwMrKCsOGDYOfnx/279+Pp556Ch4eHsXa1owZM/D555+jTp06eOmll+Di4oLo6GgcOXIEBw4cQNeuXaWyy5Ytw/jx4+Hh4YH+/fvDx8cHZ86cwdy5cxEWFoawsDDY2toiJCQEUVFRWL16NYKDgxESEiJtw93dvcT1tminV2gG8/SqCzQZLHc0RETGERYqNDRUABCxsbE686dNmyasrKxESkqKyM7OFg4ODuLNN9/UW79Xr16iXr16xdpnUlKSACCSkpIKLZeeni6uXLki0tPT9Zap1WqRmpVqlsf6q+tFs1XNRJNVTUSzVc3E+qvrzbZttVpdrNcmvw8//FAAEBs3btRb1rRpU+Hg4CBUKpUQQoioqCiRk5Oj9zqNGTNGABBHjx7VWRYcHCzyv3XDwsIEADF79mxpXm5urqhdu7ZQKBTiyJEjOtt+6aWXBAC97RTG09NTVK9eXaSmpurFGhcXJz2/fPmyUCqVomXLljrzhRBi3rx5AoBYuHBhobGbW2HvyXIlI1mI+TWFmO0qxMX1ckdDRBWEsZ/fprDYFqqRI0di8eLFGDduHBYsWAAfHx8cOnQIy5cvx9tvvw0nJydcv34d6enpaNasmd76zZo1w969e5GRkaHXglKa0nPS0X5de7NvVw015p6ai7mn5pple6deOgVHG8cSrz9ixAjMnTsXa9aswdChQ6X5f//9N8LDw/Hiiy/CxcUFABAYGKi3vkKhwNtvv40ff/wR+/btw1NPPVXsGI4ePYqIiAj0798fnTt31tn2Z599hl9//RW5ubnF2qatrS2USt0/G4VCAU9PT+n58uXLkZOTg6+//lpnPgBMmzYNX375JdavX4/JkycXu04V3pmVQHo84FkbaDJE7miIiIxmsQlVzZo1ceLECTz//PPSJT8AmDBhAhYvXgwAiIuLAwC9DzXtPCEEEhISUK1aNYP7yMzMRGZmpvRcpVKZsQYVW4MGDdCmTRv8+eefiI+Pl47BL7/8AgDS5T4AyMrKwrfffosNGzbg2rVrSElJgRBCWn7v3r0SxfD3338D0FyCzC8wMBABAQE6/ZSioqKwatUqnXLu7u6YOHEiAOCFF17AsmXL0KRJEwwbNgzBwcHo2LEjnJycdNY5efIkAGD37t3Yt2+f3r5tbGxw7dq1EtWpQstMAY5/o5nuOhWwttjTExFVQhZ7xoqKikL//v3h6+uLzZs3w8fHB6dOncKnn36KlJQU/PDDD1LZwgZvLGzZvHnzMGfOHLPG7aB0wKmXTpm8nQdpDzDwt4FQQy3Ns1JY4bcBv8HX0dfk7TsoHUzexogRI3D27Fls3LgRb731FtRqNdavX48qVaqgV69eUrkhQ4Zgx44dqF+/PoYNG4YqVarAxsYGiYmJWLJkiU5SWxxJSUkAgCpVqhhc7uvrq5dQ5T/egYGBUkL19ddfo3bt2li1ahU+/fRTfPrpp7C3t8cLL7yARYsWSb8wjI+PBwDMnWue1sJK4+wPQFoc4FELaPqC3NEQERWLxSZU06dPh0qlwsWLF6UWgq5du8Lb2xtjxozBq6++iqpVNZ20tS1VecXHx0OhUBTa8XfGjBl4//33pecqlQoBAQEmxa1QKEy6lKZVy60WZneajTkn5kAt1LBSWGF2x9mo5VbL5G2by4svvojJkydjzZo1eOutt3DgwAHcu3cP7733nnTZ7MyZM9ixYwd69+6NXbt2wdr6yS+6Tp48iSVLlpR4/25ubgCAhw8fGlz+4MEDnechISE6LWP52djYYOrUqZg6dSru3buHQ4cO4aeffsLPP/+MmJgY7NmzBwCkHz+oVCrpsiYVISsVOPa1ZrrrFLZOEZHFsdhhEy5evIigoCC9yy1t27YFAFy6dAl16tSBg4MDwsPD9dYPDw9H3bp1C+0/ZWdnB1dXV51HeTKo3iDsGbwHP/b+EXsG78GgeoPkDkmHtiXq+PHjiIyMxJo1awAAr7zyilTm5s2bAIBnn31WJ5kCgCNHjpi0/+bNmxe4nVu3bpVo6AQtPz8/DB8+HLt370a9evWwb98+pKenAwDat9f0kdNe+iuKtt7F7c9VoZz9EUh7BLgHAs2GyR0NEVGxWWxC5efnh8uXLyMlJUVn/okTJwAA/v7+UCqV6N+/P7Zu3Yrk5GSpzO3btxEWFoZBg8pXAlISVZ2qom3VtuVmyIT8RowYASEEvv/+e2zduhUNGzZEmzZtpOXaDulHjx7VWe/y5cuYN2+eSfvu3LkzatWqhZ07d+psXwiB//3vf8VKYDIzM3HgwAG9FqzU1FQkJyfDxsZGSozGjx8PpVKJd99912DSlpiYiAsXLkjPtf3L7t69W6z6VRhZacCxxy2RXacA1jbyxkNEVAIW264+ceJEDBw4EE8//TQmTZoEb29vnDx5EvPmzUNQUBCeeeYZAJrBEtu2bYt+/fph+vTpyMjIwKxZs+Dt7c1fWZWBAQMGwNXVFV988QWys7N1OqMDmlvGtGvXDhs3bsT9+/fRoUMH3L59G7///jueffZZbN68ucT7trKywooVK9C3b1/07NlTGofqwIEDuH//Ppo1a4Z//vnHqG2lp6ejR48eqF27Ntq3b48aNWogJSUFO3fuRExMDD744APY2toCAJo0aYKlS5di3LhxaNCgAfr27Ys6depApVIhIiIChw4dwqhRo7Bs2TIAQMOGDeHn54cNGzbA0dER/v7+UCgUGDdunHTZskI79xOQGgu41wCaDy+6PBFReVRqAzKUgQMHDohevXqJqlWrCgcHB1G/fn0xefJk8ejRI51yZ8+eFT169BCOjo7C1dVVDBw4UPz333/F3p85xqGqjEaPHi0ACIVCIaKiovSWP3z4UIwZM0b4+fkJe3t70bRpU/Hdd9+JiIgIAUCMHDlSp7yx41BpHT58WHTt2lU4ODgIT09PMXToUHHr1i2D2ylIVlaWWLBggejVq5fw9/cXtra2wtfXVwQHB4sNGzYYXOf06dPixRdfFH5+fsLGxkZ4e3uLVq1aienTp4urV6/qlD158qQIDg4WLi4u0vhYkZGRRsVmjHL7nsxKE+KLeppxp87+JHc0RFRBlcU4VAohCumFSzpUKhXc3NyQlJRUaH+qjIwMREZGolatWmU6xhVRQcrte/Lk/wG7pwNuAcC75wGlrdwREVEFZOzntykstg8VEVm47Azg6GLNdJf3mUwRkUVjQkVE8ji/GkiJAVz9gRavFF2eiKgcY0JFRGUvOwM4+pVmussktk4RkcVjQkVEZe/CL0DyfcC1OtByRNHliYjKOSZURFS2cjKftE51ngQo7eSNh4jIDJhQEVHZuvALoIoGXKqxdYqIKgwmVERUdnIygSN5WqdsytEQDkREJmBCVYo4xBeVF+XmvXhxLaC6CzhXBVqNlDsaIiKzYUJVCrT3dMvOzpY5EiIN7Xsx/w2oy1ROFnDkS81054lsnSKiCoUJVSmwsbGBnZ0dkpKSyk/LAFVaQggkJSXBzs4ONjYy3nj473VA0h3A2RdoPUq+OIiISoHF3hy5vPP29kZ0dDTu3r0LNzc32NjYQKFQyB0WVSJCCGRnZyMpKQkpKSmoXr26fMHkZgNHFmmmn3oPsHGQLxYiolLAhKqUaO8V9OjRI0RHR8scDVVmdnZ2qF69eqndv8oof68HEm8DTlWA1qPli4OIqJQwoSpFrq6ucHV1RXZ2NnJzc+UOhyoha2treS/zAZrWqcMLNdNPTQBsHeWNh4ioFDChKgM2Njbyf6gRyeWfX4HEW4CjN9BmjNzREBGVCnZKJ6LSk5uTr3XKSd54iIhKCRMqIio94RuBhEjA0QtoO1buaIiISg0TKiIqHbk5wOEvNNOd3mXrFBFVaEyoiKh0XNoMxEcADp5A29fljoaIqFQxoSIi81Pn5mmdegewc5Y3HiKiUsaEiojM79IWIO4/wMEDaPeG3NEQEZU6JlREZF7qXODQ55rpjm8Ddi7yxkNEVAaYUBGReV3eBsT9C9i7A+3elDsaIqIywYSKiMwnf+uUvYy3uyEiKkNMqIjIfK78Bjy6Dti7Ae3ZOkVElQcTKiIyD7UaOPT4l30dxmuSKiKiSoIJFRGZx9XtQOxVwM4NaP+W3NEQEZUpJlREZDq1+knfqQ5vAQ7usoZDRFTWmFARkemu7QAeXgHsXIEO4+SOhoiozDGhIiLT5G2dav+mZjBPIqJKhgkVEZnm+i7gwSXA1kXTGZ2IqBJiQkVEJScEcGiBZrr9G4Cjp7zxEBHJhAkVEZXc9T+AmHDA1hno+I7c0RARyYYJFRGVjBDAwfma6Xavs3WKiCo1JlREVDI3dgMx/wA2TkDHd+WOhohIVkyoiKj4dFqnxgJOXvLGQ0QkMyZURFR8//4F3L8I2DgCnSbIHQ0RkeyYUBFR8eRtnWr7GuDkLW88RETlABMqIiqe//YB984DSge2ThERPcaEioiMl791yrmKvPEQEZUTTKiIyHg39wPRZwGlPVuniIjyYEJFRMYRAjj4eFT0NmMAF1954yEiKkeYUBGRcSLCgLunNa1TT70ndzREROUKEyoiKlre1qnWowCXqrKGQ0RU3jChIqKiRR4C7pwErO2ApybKHQ0RUbnDhIqICqfTOjUScK0mbzxEROUQEyoiKlzUEeD2ccDalq1TREQFYEJFRIXTtk61ehVwqy5vLERE5RQTKiIqWNRR4NZRwMoG6DxJ7miIiMotJlREVDDtqOitXgXc/OWNhYioHGNCRUSG3Tqu6T/F1ikioiIxoSIiw7StUy1fAdwD5I2FiKicY0JFRPpun9SMPWVlA3R5X+5oiIjKPSZURKRP2zrV4iXAvYa8sRARWQAmVESk685pzX37rJRAl8lyR0NEZBGYUBGRLm3rVPPhgEegvLEQEVkIi0+ojh49ir59+8LDwwMODg6oV68ePvnkE50y58+fR8+ePeHs7Ax3d3cMGjQIERERMkVMVI7dPQvc3A8orNk6RURUDBadUK1btw7BwcFwc3PDzz//jD/++AMffPABhBBSmWvXriEkJARZWVnYuHEjfvzxR9y4cQNdunRBbGysjNETlUN5W6c8a8kbCxGRBVGIvNmHBYmOjkaDBg3w6quvYunSpQWWe+GFFxAWFoabN2/C1dUVAHDr1i3Uq1cPkyZNwoIFC4zep0qlgpubG5KSkqRtEVUYd88B33fXtE69exbwrC13REREZlEWn98W20L1/fffIzU1FR988EGBZXJycrBz504MHjxY5wUMDAxEt27dsG3btrIIlcgyHHr85aLZMCZTRETFZLEJ1eHDh+Hp6Ylr166hRYsWUCqVqFKlCt566y2oVCoAwM2bN5Geno5mzZrprd+sWTP8999/yMjIKHAfmZmZUKlUOg+iCin6PPDvHkBhBXSdInc0REQWx2ITqujoaKSlpWHo0KEYNmwY9u3bh6lTp+Lnn39G3759IYRAXFwcAMDT01NvfU9PTwghkJCQUOA+5s2bBzc3N+kREMDRoqmCOvS55t+mLwBedeSNhYjIAinlDqCk1Go1MjIyMHv2bEyfPh0AEBISAltbW0ycOBH79++Ho6MjAEChUBS4ncKWzZgxA++//2SUaJVKxaSKKp57F4Ebfz5unZoqdzRERBbJYluovLy8AAC9e/fWmf/MM88A0AyVoC2jbanKKz4+HgqFAu7u7gXuw87ODq6urjoPogpH2zrVZAjgXVfeWIiILJTFJlSG+kUBkIZMsLKyQp06deDg4IDw8HC9cuHh4ahbty7s7e1LNU6icu3+P8D1XQAUbJ0iIjKBxSZUgwcPBgD8+eefOvP/+OMPAECHDh2gVCrRv39/bN26FcnJyVKZ27dvIywsDIMGDSq7gInKI+0v+5oMBnzqyxsLEZEFs9hxqADgueeew19//YWZM2eiQ4cOOHv2LObMmYOePXtix44dADQDe7Zt2xatWrXC9OnTkZGRgVmzZiE+Ph4XL16Ej4+P0fvjOFRUocRcApY9BUABvH0K8Gkgd0RERKWC41AV4ddff8XEiROxYsUKPPPMM/i///s/TJo0CZs3b5bKNGzYEAcPHoSNjQ2GDBmCUaNGoW7dujh8+HCxkimiCkfbOtX4eSZTREQmsugWqrLGFiqqMB5cBv6vEwAFMP4EUKWR3BEREZUatlARUenQ/rIvaACTKSIiM2BCRVTZPLwKXNmumQ6eJm8sREQVBBMqosrm0OcABNDoOcC3sdzREBFVCEyoiCqTh9eAy49vCh5c8I3FiYioeJhQEVUmh78AIICG/YCqTeSOhoiowihxQvXxxx9jzZo15oyFiEpT7A3g0hbNNFuniIjMqsQJ1aeffmrwli5EVE5pW6caPAtUM3zrJiIiKpkSJ1SBgYGIj483ZyxEVFoe/QdcejzgLX/ZR0RkdiVOqIYPH449e/YgKSnJnPEQUWk4/AUg1ED9ZwC/FnJHQ0RU4ZQ4oZo5cyaaNWuG7t27Y9euXXj48KE54yIic4m7CYRv1EyHsO8UEVFpUJZ0RQcHBwCAEALPPfdcgeUUCgVycnJKuhsiMtXhhZrWqXq9Ab+WckdDRFQhlTih6tKlCxQKhTljISJzi48A/vlVM83WKSKiUlPihOrgwYNmDIOISsXhRYDIBeo+DVRvLXc0REQVFgf2JKqo4iOBv9drpkOmyxsLEVEFV+IWqryio6Px999/IykpCa6urmjRogWqV69ujk0TUUkdedw6VacH4N9G7miIiCo0kxKqiIgIvPXWW9i/f7/esh49emDp0qWoW7euKbsgopJIuMXWKSKiMlTihOru3bt46qmn8ODBAzRq1Ahdu3ZF1apV8eDBAxw5cgT79u1Dly5dcPr0aQQEBJgzZiIqypFFgDoHqN0NCGgndzRERBVeiROq0NBQPHjwACtWrMDYsWP1lv/www9444038PHHH2PlypUmBUlExZB4G7i4VjPN1ikiojJR4k7pe/bswXPPPWcwmQKA1157Df3798eff/5Z4uCIqASOfKlpnaoVDNToIHc0RESVQokTqocPH6Jx48aFlmncuDFiY2NLugsiKq7EO8CFNZpptk4REZWZEidUPj4+uHz5cqFlrly5Ah8fn5LugoiK6+hXgDobqNkFCOwkdzRERJVGiROq3r17Y8eOHfjhhx8MLv/xxx+xY8cO9OnTp8TBEVExJEUDF37RTLN1ioioTCmEEKIkK965cwdt2rTBo0ePEBQUhODgYPj6+uLBgwc4fPgwLl++DG9vb5w9e7bC/MpPpVLBzc1NGm+LqFzZNQU4sxII7AyM3iV3NERE5UZZfH6X+Fd+AQEBOHr0KN566y2EhYXpXf7r1q0b/u///q/CJFNE5ZrqHnB+tWaa9+wjIipzJg3sWa9ePezfvx93797FhQsXoFKppJHSmUgRlaGji4HcLKBGJ03/KSIiKlMlTqi6d++Ozp074+OPP4a/vz/8/f3NGRcRGUt1Hzi3SjMd8gGgUMgaDhFRZVTiTumnTp1CTk6OOWMhopI4tgTIzQQCOmjGniIiojJX4oSqUaNGiIqKMmMoRFRsyTHAuZ8002ydIiKSTYkTqnfffRe///47rly5Ys54iKg4jn0N5GQA/u009+0jIiJZlLgPVa1atRASEoIOHTrgzTffRNu2beHr6wuFgW/IXbt2NSlIIjIg5SFw9kfNNFuniIhkVeKEKiQkBAqFAkIILFq0yGAipZWbm1vS3RBRQY4tAXLSgeptgDo95I6GiKhSK3FCNWvWrEKTKCIqRSmxwJnHdykImc7WKSIimZU4oQoNDTVjGERULMe/1rRO+bUC6vaUOxoiokqvxJ3Sra2t8fLLL5szFiIyRuoj4Mz3mmm2ThERlQslTqhcXV05GjqRHI5/A2SnAX4tgXq95I6GiIhgQkLVrl07/P333+aMhYiKkhoHnF6pmQ7mL/uIiMqLEidUc+bMwYEDB7B69WpzxkNEhTnxLZCdClRrDtTvI3c0RET0WIk7pf/1118ICQnBmDFj8M0336Bdu3YGx6FSKBT46KOPTA6UqNJLiwdOr9BMs3WKiKhcUQghRElWtLIyrnFLoVBUmHGoVCoV3NzckJSUBFdXV7nDocpm/yfAkYVA1abAm0eYUBERGaksPr9L3EIVFhZmzjiIqDBp8cCp5Zpptk4REZU7JU6ogoN5V3uiMnPy/4CsZMC3CdDgWbmjISKifErcKR0AcnJy8NVXX6Fdu3ZwdXWFUvkkP7t48SLGjx+PGzdumBwkUaWWngCcWqaZDp4GGHm5nYiIyk6JW6jS09PRq1cvHD9+HN7e3nB1dUVqaqq0vFatWvjpp5/g6emJTz/91CzBElVKJ5cBmSqgShDQsL/c0RARkQEl/qr72Wef4dixY5g3bx5iYmIwduxYneVubm4IDg7Gnj17TA6SqNJKT9Rc7gPYOkVEVI6V+Oz866+/IiQkBNOmTYNCoTB4o+TatWvj9u3bJgVIVKmdWgZkJgE+jYBGA+SOhoiIClDihOr27dto27ZtoWVcXV2RlJRU0l0QVW4ZScDJpZrp4KlsnSIiKsdKfIZ2cXFBbGxsoWVu3rwJHx+fku6CqHI7tVyTVHk3AIIGyh0NEREVosQJVYcOHbBjx44CW6Du3r2LP/74A127di1xcESVVoYKOPGdZjp4GmBlLW88RERUqBInVFOnTkV8fDx69uyJ48ePIycnBwCQlpaG/fv3o1evXsjOzsb7779vtmCJKo3Ty4GMRMC7PtD4ebmjISKiIpR42ISuXbviu+++w4QJE9ClSxdpvouLCwDA2toaS5cuRevWrU2PkqgyyUx+0jrVdSpbp4iILECJEyoAeOuttxAcHIxly5bh1KlTiI+Ph6urK9q3b4/x48ejcePG5oqTqPI4vUIzmKdXXaDJYLmjISIiI5iUUAFAo0aNsGTJEnPEQkSZKcDxbzXTbJ0iIrIY/B02UXlyZiWQHg941gaaDJE7GiIiMhITKqLyIjMFOP6NZrrrVMDa5AZkIiIqIxUqofr++++hUCjg7Oyst+z8+fPo2bMnnJ2d4e7ujkGDBiEiIkKGKIkKcPYHIC0O8KgFNH1B7miIiKgYKkxCFR0djSlTpsDPz09v2bVr1xASEoKsrCxs3LgRP/74I27cuIEuXboUOTgpUZnISgWOfa2Z7jqFrVNERBamwiRUb731Frp27Yqnn35ab9msWbNgZ2eHnTt3om/fvhg0aBB27dqF2NhYLFy4UIZoifI5+yOQ9ghwDwSaDZM7GiIiKqYKkVCtWbMGhw4dwtKlS/WW5eTkYOfOnRg8eDBcXV2l+YGBgejWrRu2bdtWlqES6ctKA449/qVs1ymAtY288RARUbFZfEL18OFDTJw4EfPnz4e/v7/e8ps3byI9PR3NmjXTW9asWTP8999/yMjIKItQiQw79xOQGgu41wCaD5c7GiIiKgGL76gxfvx4NGjQAOPGjTO4PC4uDgDg6empt8zT0xNCCCQkJKBatWp6yzMzM5GZmSk9V6lUZoqa6LHs9CetU10ms3WKiMhCWXQL1ZYtW7Bjxw6sXLkSCoWi0LKFLS9o2bx58+Dm5iY9AgICTIqXSM+5VUDKA8AtAGj+ktzREBFRCVlsQpWSkoK3334b7777Lvz8/JCYmIjExERkZWUBABITE5GamgovLy8AT1qq8oqPj4dCoYC7u7vBfcyYMQNJSUnS486dO6VWH6qEsjOAo4s1013eB5S2soZDREQlZ7GX/B49eoQHDx5g0aJFWLRokd5yDw8PDBgwAJs3b4aDgwPCw8P1yoSHh6Nu3bqwt7c3uA87OzvY2dmZPXYiAMD51UBKDODqD7R4Re5oiIjIBBabUFWtWhVhYWF68+fPn49Dhw7hzz//hLe3N5RKJfr374+tW7fi888/h4uLCwDg9u3bCAsLw6RJk8o6dKLHrVNfaaa7TGLrFBGRhVMIIYTcQZjTqFGjsHnzZqSkpEjzrl27hrZt26JVq1aYPn06MjIyMGvWLMTHx+PixYvw8fExatsqlQpubm5ISkrSGYKBqNhOrwT+mAK4VgcmXACUbAklIiotZfH5bbF9qIqjYcOGOHjwIGxsbDBkyBCMGjUKdevWxeHDh41OpojMJifzSetU50lMpoiIKoAK10JVmthCRWZx5ntg12TApRow4SJgY7gPHxERmQdbqIgqmpxM4Eie1ikmU0REFQITKqKydHEtoLoLOFcFWo2UOxoiIjITJlREZSUnCzjypWa680S2ThERVSBMqIjKyt/rgKQ7gLMv0HqU3NEQEZEZMaEiKgu52cCRxwPQPvUeYOMgbzxERGRWTKiIysLf64HE24BTFaD1aLmjISIiM2NCRVTacrOBwws1009NAGwd5Y2HiIjMjgkVUWn751cg8Rbg6A20GSN3NEREVAqYUBGVptycfK1TTvLGQ0REpYIJFVFpCt8IJEQCjl5A27FyR0NERKWECRVRacnNAQ5/oZnu9C5bp4iIKjAmVESl5dJmID4CcPAE2r4udzRERFSKmFARlQZ1bp7WqXcAO2d54yEiolLFhIqoNFzaAsT9Bzh4AO3ekDsaIiIqZUyoiMxNnQsc+lwz3fFtwM5F3niIiKjUMaEiMrfL24C4fwF7d6Ddm3JHQ0REZYAJFZE55W+dsneVNx4iIioTSrkDILJ46lzg1nEg5QEQex14dB2wdwPas3WKiKiyYEJFZIorvwO7PwBU93Tn1+mhSaqIiKhS4CU/opK68juw8VX9ZArQ9KO68nvZx0RERLJgQkVUEupcTcsURMFldk/XlCMiogqPCRVRSdw6brhlSiIAVbSmHBERVXhMqIhKIuWBecsREZFFY0JFVBLOvuYtR0REFo0JFVFJBHYC7AobY0oBuFbXlCMiogqPCRVRcQkBhH0GZKoKKKDQ/NNnPmBlXWZhERGRfJhQERWHWg3smgwcWah53nQo4OqnW8bVD3jhZyDoubKPj4iIZMGBPYmMlZMF/PYWcGkLAAXw7CKg7Wu6I6U7+2ou87FlioioUmFCRWSMrDTNIJ7/7QWslMCgFUCTwZplVtZArS7yxkdERLJiQkVUlPREYN0w4M5JQOkADFsD1Ospd1RERFSOMKEiKkzyA2DNYOBBuObefC9tBGp0kDsqIiIqZ5hQERUkIQr4eSCQEAk4VQFGbAOqNpE7KiIiKoeYUBEZ8vAq8MvzQPJ9wD0QePU3wLO23FEREVE5xYSKKL+7Z4G1Q4D0BMCnkaZlyrWa3FEREVE5xoSKKK+bYcCGl4HsVMC/rabPlKOn3FEREVE5x4SKSOvKdmDLWCA3C6jdTfNrPjtnuaMiIiILwISKCADO/wLsmAAINRA0ABi0ElDayR0VERFZCN56hujY18Dv72iSqZYjgCE/MZkiIqJiYQsVVV5CAPs/Bo5+qXn+1HtAzzmAQiFvXEREZHGYUFHlpM7V3OT43E+a5z1Dgc6TZA2JiIgsFxMqqnxysoBtbwCXtwFQAP0XA61HyRwUERFZMiZUVLlkpQK/jgBu7gesbIDBK4HGz8sdFRERWTgmVFR5pCcAa18A7p4GbBw1wyLU7SF3VEREVAEwoaLKITkG+GUQ8PAyYO8OvLwJCGgnd1RERFRBMKGiii8+EvhloOZmx85VNbeS8Q2SOyoiIqpAmFBRxfbgsqZlKiUG8KgJjPgN8Kwld1RERFTBMKGiiuvOac1NjjOSgCqNgRFbAZeqckdFRKUkJjUGt1W3UcO1Bqo68W+dyhYTKqqY/tsP/PoKkJ0G+LcDXt4IOHjIHRURlZKt/27FnBNzoBZqWCmsMLvjbAyqN0jusKiYLDkpVgghhNxBWAqVSgU3NzckJSXB1dVV7nCoIJe3AVteB9TZQJ0ewLBfAFsnuaOics6ST+SlQS3UyFXnIludjVyRixx1jvSv9iE9F4+fqw08Fzn623k8X287j9fVbqeg+fm3n56djktxl/TqUN+9PmytbaFQKKDA4zsgKACF9r+88wHped75Cs0KOusUNb/QZQVN510n33ydfRYQp878POvk3Zex6xRa53z7L6jOBZUrLM7w2HD8desvCAizJ8Vl8fnNFiqqWM6tAnZMBCCAxoOA55cDSluZg6LyrritG0IIgx/sxiQI2eps/YShoIQkTwJiTGJj7PalcnkTnHz7VQt1GR6B0nEj8YbcIVAJqYUac07MQSe/ThbzBYcJFVUcR78C9oVqpluPBp5dBFhZyxoSlT/Z6mw8SH2Aeyn3EJ0SjX8T/sUvV3+RlquFGrOPz8a3F74FAIOJU0VINkpCAQWUVkrNQ6H519rKWvOvwho2VjawVljrzJfK5ZmvU+7xv9pH3vk2VjY62ze0X6VCiZSsFHx88mMIPLngYgUrfPzUx3C3c5fmCyGg/U/z/+P5EHrLDM3Pe0FHZ1lB8/PsN//+pXn5Yytofr7nBU0XWrfC4ixhnQuLM/86hcUfmx6LU/dP6bzf1EKNO8l3mFARlRkhgL2zgONfa553mQx0/4g3Oa6ksnOzEZMWg3sp96Sk6V7KPdxL1Tx/kPbAqIQoNj222PtWWinNmiwYm6QY2r6h+YaSkUKTHStrqYzSSgkrhVVJDkmZsLKy0mtlHFB3gNxhkZFiUmPQe0tvnb9NK4UVAlwCZIyqeJhQkWVT5wI73gMuPG5hePoT4KkJ8sZEpSorNwsxqTGITonG/dT7TxKmx8lTbHpskQmTrZUt/Jz94OfsB3c7d/wZ+ade68Y33b9BFacqUCoMJCAGEhJrtobKalC9Qejk1wl3ku8gwCXAYlo1SKOqU1XM7jhbLym2pOPIhIosV04msGUscPV3QGEF9F8CtHpV7qjIRFm5WQYTJe282LRYneTHEDtrO03C5OQnJU7VnatL87wcvHRaW9pXa693Iu8a0LW0q0pmVtWpqkV9AJMuS0+KLfZXfgcOHMCaNWtw/Phx3LlzB+7u7mjTpg1mzZqF1q1b65Q9f/48pk2bhpMnT0KpVKJ79+5YuHAhateuXax98ld+5UhmimZYhIgwwNoWGPwDEPSc3FGRETJzM3Ev5R7up9xHdGq07mW5lHtGXWqzt7bXSZSqOVV7kjA5+8HL3kvn10PGiEmNsdgTOREVjr/yK8T//d//IS4uDu+99x6CgoIQGxuLRYsWoUOHDtizZw+6d+8OALh27RpCQkLQokULbNy4ERkZGZg1axa6dOmCixcvwsfHR+aaULGlxQNrhwLRZwEbJ+DFtUCdbnJHRY9l5GRI/ZWk1qU8ydOj9EdFbsNB6WCwdUmbPHnaexY7YSoKWzeIyBQW20L18OFDVKlSRWdeSkoK6tatiyZNmmDfvn0AgBdeeAFhYWG4efOmlJXeunUL9erVw6RJk7BgwQKj98kWqnJAdR/45Xkg9qpmoM6XNwP+beSOqlJJz0nXJEiPW5W0iZL2EZcRV+Q2HJWOepfh8j53t3M3e8JERJUXW6gKkT+ZAgBnZ2cEBQXhzp07AICcnBzs3LkTr776qs4LGBgYiG7dumHbtm3FSqhIZnE3NTc5TrwNuFTT3OS4SiO5o6pw0rLTpF/FSa1LeX4pF58RX+Q2nGycNAmSU3XdS3PO1VDdqTrc7NyYMBFRhWKxCZUhSUlJOH/+vHS57+bNm0hPT0ezZs30yjZr1gx79+5FRkYG7O3tyzpUKq6YS5qWqdSHgGdtzU2OPQLljsoipWanFjikwL2Ue0jITChyG842zjp9lvyc/HSeu9q6MmEiokqlQiVUb7/9NlJTU/Hhhx8CAOLiNJcePD099cp6enpCCIGEhARUq1bN4PYyMzORmZkpPVepVKUQNRXp9klg7QtAZhLg21Rzk2Nn/RZK0kjJSjGYKEWnRONe6j0kZSYVuQ0XWxdNguSUr3XpcdLkastL3kREeVWYhOqjjz7C2rVr8c033+j9yq+wb8qFLZs3bx7mzJljthipBP7dp/k1X046ENABeOlXwMFd7qhkpcpS6fZhevyvdlgBVVbRib+bnZvUqiQlSnk6gbvYupRBTYiIKo4KkVDNmTMHn376KebOnYt33nlHmu/l5QXgSUtVXvHx8VAoFHB3dy9wuzNmzMD7778vPVepVAgIsJxRWy1e+GZg25uAOgeo+zTwws+AraPcUZUqIQRUWaonnbxT7+kNK5CcnVzkdtzt3J908s6TKGkvzznbOpdBbYiIKg+LT6jmzJmD0NBQhIaG4n//+5/Osjp16sDBwQHh4eF664WHh6Nu3bqF9p+ys7ODnZ2d2WMmI5z5Adg1GYAAmgwBBv5fub7JcUxqDG6rbqOGa41Cf3qvTZjyty7lvTSXkp1S5P487T3h5+Sncxkub/LkaFOxE08iovLGohOqTz75BKGhoZg5cyZmz56tt1ypVKJ///7YunUrPv/8c7i4aC5j3L59G2FhYZg0aVJZh0xFEQI4sgg48InmeduxwDNfAFbl9x5iW//dqjPK9pQ2U9CqSiu91iXtv2k5aUVu09PeU6eTd3WnJ5fmqjlVY8JERFTOWOw4VIsWLcKUKVPQp08fg8lUhw4dAGgG9mzbti1atWqF6dOnSwN7xsfHF3tgT45DVcqEAP6aCZz4VvO861Sg24ey3ORYLdRIy06DKkuF5KxkvX+1jwdpD7D31t5ib9/bwdvg+Et+zn6o5lQNDkqHUqgVEVHlVBaf3xabUIWEhODQoUMFLs9brXPnzuGDDz7AiRMndG49U6dOnWLtkwlVKcrN0dzk+OIazfPe84CO40u8OSEEMnIzoMp8nABlaxKgpMwknYQob4KkylJJz1OyU4q8wW5h3GzdUNOtpsFbo1RzqgZ7JYfqICIqK0yoyhkmVKUkOwPY8hpwbSegsAYGfAu0eAlZuVl6CU/+6fwtRnnn56hzTA7NxsoGrraucLVzhYutC1xsXeBq8+Q5APwQ/oPOzXqtFFbYM3gPb2NCRFROcKR0smg56hykZKVoEpxs1ZPWorwtQulxSP5vD5LTHkHlVxXJbv5IvvE9ki99hYzcDJNjsFZYa5Ig2ycJkfZ53nn5p7UJk5110T9KCHAJ0OlDNbvjbCZTRESVDBMqKpBaqJGanWrwkpjBy2WZKunSWnJWMlKzU43fmePjPkMZD/UWudg8TnS0rUQ2LrotRvmSo7zzHJQOpT5i96B6g9DJrxPuJN9BgEsAkykiokqICVU5YezP7otDCIH0nPQCL5MVdrnMHP2ItByUDvqJD6zhGnEILqnxcLG2h2v7cXD1baabFNm5wknpBGsrazO8GqWrqlNVJlJERJUYE6pyIP/P7md3nI1B9QYBgE4/IkMJkE4rUb4O2GXVjyj/ZbO8/zrbOsPGykZ3g3E3gZ8HAEl3ABc/4OXfAJ8GJsdJREQkF3ZKL4bS6NQWkxqDXpt76XRqBjTjEKVmpyIzN7OANY2n7UdksK+QgUtlOv/auRrVj8ho9/8B1gwCUmMBzzrAq78B7jXMt30iIqJ82Cm9Erituq2XTAFAfEa8znND/YjyPs+fCJV1PyKj3DoOrBsGZKqAqk2BV7YBzsaPA0ZERFReMaGSWQ3XGlBAofuze1jhux7foaZbTYvqR1SoG3uAja8CORlAjU7ASxsAeze5oyIiIjKL8ns/j0qiqlNVhHYKhZVCcyisFFaY3Wk2Ovt3hr+LP1xtXS0/mfpnE7DhJU0yVb8PMGIrkykiIqpQ2EJVDlTon92fXgn8MRWAAJq+AAxcCljbFLkaERGRJWFCVU5UuJ/dCwEc/gIIm6t53u5NoM/8cn2TYyIiopJiQkXmp1YDe/4HnPo/zfPg6UDIdFluckxERFQWmFCReeXmAL+/A/y9XvO8zwKgw1vyxkRERFTKmFCR+WRnAJtHA9f/0NzkeOBSoPmLckdFRERU6phQkXlkqDS/5Is6AljbAS+sBho8I3dUREREZYIJFZku9RGwZjBw/yJg66IZY6pmZ7mjIiIiKjNMqMg0SXeBnwcCcf8Cjt7AK1sAvxZyR0VERFSmmFBRyT36V5NMqe4Crv6a+/J515M7KiIiojLHhIpK5t5FzU2O0+IAr3rAiG2Ae4DcUREREcmCCRUVX9RRYN2LQFYyUK058MpWwMlb7qiIiIhkw4SKiufaH8CmUUBuJhDYGRi+HrB3lTsqIiIiWTGhIuP9vQH4bTwgcoEGfYEhPwI2DnJHRUREJDveWI2Mc3IZsO1NTTLVfDjwwi9MpoiIiB5jCxUVTgjg4Hzg0HzN8/bjgN6f8SbHREREeTChooKp1cDu6cDp5Zrn3T4Euk7lTY6JiIjyYUJFhuVmA9vfBv75VfO870Kg3evyxkRERFROMaEifdnpml/y3dgNWCmBgcuAZkPljoqIiKjcYkJFujKSgPXDgVvHAKU98MLPQP3eckdFRERUrjGhoidSYjWjn8f8A9i5Ai/9CgR2kjsqIiKico8JFWkk3gZ+eR6I+w9w8tHc5Lhac7mjIiIisghMqAiIva5JplTRgFsNzU2OverIHRUREZHFYEJV2UWfB9YMBtLjAe8Gmpscu1WXOyoiIiKLwoSqMos8rOmAnpUC+LUCXt4MOHnJHRUREZHFYUJVWV3dCWweo7nJca2uwIvrADsXuaMiIiKySLx/SGV0YS2wcYQmmWrYD3hpE5MpIiIiEzChqmxOfAdsHw8INdDiZWDoasDGXu6oiIiILBov+VUWQgAHPgWOLNQ87/gO8PQnvMkxERGRGTChqgzUauCPKcDZHzTPu38EdJnMmxwTERGZCROqii43G9j2FnBpMwAF8OxCoO1YuaMiIiKqUJhQVWRZacCmkcC/f2lucvz8cqDpELmjIiIiqnCYUFVU6YnA+heB2ycApQMw7Beg3tNyR0VERFQhMaGqiFIeAr8MAh6EA3ZuwMsbgRod5I6KiIiowmJCVdEk3AJ+GQjERwBOVYARW4GqTeWOioiIqEJjQlWRPLymSaaS7wPuNYARv/Emx0RERGWACVVFcfccsHYwkJ4A+DTStEy5+skdFRERUaXAhKoiuBkGbHgZyE4FqrcBXt4EOHrKHRUREVGlwYTK0l35HdjyGpCbBdQOAYatBeyc5Y6KiIioUuF9RyzZ+V8040zlZgGNngNe2shkioiISAZMqCzVsa+B39/R3OS45Qhg6CpAaSd3VERERJUSL/lZGiGA/R8DR7/UPO80AXj6Y96Xj4iISEZMqCyJOhfYNRk495Pmec9QoPMkWUMiIiIiJlSWIycL2PYGcHkbAAXQ7yugzWi5oyIiIiIwobIMWanAryOAm/sBKxtg0AqgySC5oyIiIqLHmFCVd+kJwNoXgLunARtHzU2O6/aUOyoiIiLKgwlVeZYco7nJ8cPLgL0b8PJmIKCd3FERERFRPpVm2ISUlBRMnDgRfn5+sLe3R4sWLbBhwwa5wypYfCTwY29NMuXsC4z+k8kUERFROVVpEqpBgwZh9erVmD17Nv7880+0bdsWw4cPx7p16+QOTfPrvcgjQPhmzb/3w4Ef+wAJUYBHTWDMHsC3sdxREhGVa//cTcTwFSfxz91EuUOhErLkY1gpLvn98ccf2Lt3L9atW4fhw4cDALp164Zbt25h6tSpGDZsGKytreUJ7srvwO4PANW9PDMVAARQJQgYsQ1wqSpPbGS0f+4mYt4f1zCjb0M083eXOxwqAR5Dy7f1fDRORMRh6/loHkMLZcnHsFK0UG3btg3Ozs4YOnSozvzRo0fj3r17OHXqlDyBXfkd2PhqvmQKAITmn04TmExZiLwnAbJMPIaW6W5CGsLvJuFSdBJ2/K05l+74+x4uRSch/G4S7iakyRwhFaWiHMNK0UJ16dIlNGrUCEqlbnWbNWsmLe/UqVPZBqXORc4f02ANAUNjnAsokPXXHMQG9AOsZGo9o0LFJGUgMT0bCgDbL2o+hLdfjEbnut4AADcHG1R1s4d4nB8LbaIMSPM08/NM51kgDJSFUdvIO1/ozzOwXH8/xpctrfgNbauwmAxNFhX/o5RMqDJyoACw5fxd4PG/gV4OUAvA2VYJT2c7CCGgFo+3IjTbFwJQC/F4+snrrBZ5yzzeq05ZTSxCPFmed1otdLenLavOM619fTRl82xPu89889SPJ3SXP4kVyLt/3ToCAmp1/n0UUAedfWmX69Yd0N9/3tdQmpdvG/nrqH48/fedROQXl5qFft8clZ43re6mszz/jSX0zsH5Chg6Rxe1DUWJtlH4RvX3YWCb+Urp7aOIfRpz042i6mY4roK3ceDaQ73y8fmOYdT8Z4sOTGaVIqGKi4tD7dq19eZ7enpKyw3JzMxEZmam9FylUpkvqFvHoUy5X+BiBQTs0u5jyqJlOKkOMt9+qVQlpGVj7M9n5Q6DTJCckYM5O67KHQaZUXh0ktwhUDFpv/4orRRYOLS5rLEYq1IkVIB+Rm3Msnnz5mHOnDmlE1DKA6OK+Vknwc6qUlyZtTi5aoEctYFmncdsrBVQWlnpfFvL+07L+75TGChQYFmFkdtC3m+K+usXVFZhVFn9vxmdsjrrKYzeVrFeh0L2lX9+QesnpWcjOjFdv+BjNTwd4elkCyuFZl3F4+0qoND8m2/aSqGta96ymvmaRQq9edpt4PH6edd7sh1Fnnma8prTgu58qzzloTfvyfb096XQWZ53nrbuMDQf2nppyljljVtvOl8d8tYtb6x5tqfzOknHVKF3PG7HpxpMgkP7ByHQ20l3psj/VHdG/pZagy23+Z/nK6S/XH8b+Uvp7beIbeSP27htFHy+MrS+of0U9foY2kNRrw+E5rLf1wf+01v3t7efQpN8LYzlVaVIqLy8vAy2QsXHxwN40lKV34wZM/D+++9Lz1UqFQICAswTlLOvUcW+HNMbX9bqYp59ktldik7SaZbW2vluZ4s5CVR2PIaW79LjFiiFQvMBr/23TU1PHkMLcSk6CV8f+E/vGFqSStH00bRpU1y9ehU5OTk688PDwwEATZo0MbienZ0dXF1ddR5mE9gJcPUroAeVpg8VXKtrylG59+RbtbxxUMnxGFouL2db+DjboWl1N8x9vgmaVneDj7MdvJxt5Q6NjFQRjmGlaKF6/vnnsXLlSmzZsgXDhg2T5q9evRp+fn5o37592QdlZQ30WQBsfBVq6Ga2ajy+nNFnPjukl3Pak0A1d3sMaxuAX8/cwf3EDIs6CVR2PIaWr5qbA45O7wZbaysoFAq81K4GsnLVsFPy/GkpKsIxVIiiLqpWEL169cLZs2exYMEC1K1bF+vXr8fKlSuxZs0avPzyy0ZtQ6VSwc3NDUlJSeZrrbryO8SfH0CR/GToBOFaHYo+84Gg58yzDypVmTm50klACGFxJwHiMSSq6Erl8zufSpNQpaSk4MMPP8TGjRsRHx+Phg0bYsaMGXjxxReN3kapHRB1LnDruKajurOv5jIfW6aIiIjMgglVOVMWB4SIiIjMqyw+vytFp3QiIiKi0sSEioiIiMhETKiIiIiITMSEioiIiMhETKiIiIiITMSEioiIiMhETKiIiIiITMSEioiIiMhETKiIiIiITFQpbo5sLtpB5VUqlcyREBERkbG0n9uleXMYJlTFkJycDAAICAiQORIiIiIqruTkZLi5uZXKtnkvv2JQq9W4d+8eXFxcoFAozLptlUqFgIAA3Llzp0LeJ5D1s3wVvY4VvX5Axa8j62f5SquOQggkJyfDz88PVlal09uJLVTFYGVlBX9//1Ldh6ura4X9QwFYv4qgotexotcPqPh1ZP0sX2nUsbRaprTYKZ2IiIjIREyoiIiIiEzEhKqcsLOzw+zZs2FnZyd3KKWC9bN8Fb2OFb1+QMWvI+tn+Sy5juyUTkRERGQitlARERERmYgJFREREZGJmFARERERmYgJVSlLSUnBxIkT4efnB3t7e7Ro0QIbNmwwat2HDx9i1KhR8Pb2hqOjIzp27Ij9+/eXcsTFU9L6rVq1CgqFwuAjJiamDCI3TnJyMqZNm4ZevXrBx8cHCoUCoaGhRq9f3o+hKfWzhGN44MABjBkzBg0bNoSTkxOqV6+OAQMG4Ny5c0atX96PH2BaHS3hGF68eBHPPvssatSoAQcHB3h6eqJjx45Ys2aNUeuX92NoSv0s4fgZ8v3330OhUMDZ2dmo8uX9GGpxYM9SNmjQIJw5cwbz589H/fr1sW7dOgwfPhxqtRovvfRSgetlZmaiR48eSExMxJIlS1ClShV899136NOnD/bt24fg4OAyrEXBSlo/rZ9++gkNGzbUmefl5VVa4RZbXFwcVqxYgebNm2PgwIH4/vvvjV7XEo6hKfXTKs/H8P/+7/8QFxeH9957D0FBQYiNjcWiRYvQoUMH7NmzB927dy9wXUs4foBpddQqz8cwMTERAQEBGD58OKpXr47U1FSsXbsWI0aMQFRUFGbOnFngupZwDE2pn1Z5Pn75RUdHY8qUKfDz80NSUlKR5S3hGEoElZpdu3YJAGLdunU6859++mnh5+cncnJyClz3u+++EwDE8ePHpXnZ2dkiKChItGvXrtRiLg5T6vfTTz8JAOLMmTOlHaZJ1Gq1UKvVQgghYmNjBQAxe/Zso9a1hGNoSv0s4Rg+ePBAb15ycrLw9fUVPXr0KHRdSzh+QphWR0s4hgVp3769CAgIKLSMpRxDQ4ypnyUev379+on+/fuLkSNHCicnpyLLW9Ix5CW/UrRt2zY4Oztj6NChOvNHjx6Ne/fu4dSpU4Wu26BBA3Ts2FGap1Qq8corr+D06dOIjo4utbiNZUr9LIW2+bwkLOEYmlI/S1ClShW9ec7OzggKCsKdO3cKXdcSjh9gWh0tmbe3N5TKwi+yWMoxNMSY+lmaNWvW4NChQ1i6dKnR61jSMWRCVYouXbqERo0a6f1RNGvWTFpe2LracobWvXz5shkjLRlT6qfVr18/WFtbw9PTE4MGDTJqHUthCcfQHCztGCYlJeH8+fNo3LhxoeUs+fgZW0ctSziGarUaOTk5iI2NxdKlS7Fnzx588MEHha5jScewJPXTsoTj9/DhQ0ycOBHz588v1j1xLekYVqz0t5yJi4tD7dq19eZ7enpKywtbV1uuuOuWFVPqV7VqVXz44Yfo0KEDXF1dER4ejvnz56NDhw44duwYmjdvXmpxlxVLOIamsNRj+PbbbyM1NRUffvhhoeUs+fgZW0dLOobjx4/H8uXLAQC2trb4+uuv8eabbxa6jiUdw5LUz9KOX4MGDTBu3LhirWdJx5AJVSkr7HJKUZdaTFm3rJQ0xj59+qBPnz7S865du+LZZ59F06ZNMWvWLGzfvt2sccrFEo5hSVniMfzoo4+wdu1afPPNN2jdunWR5S3x+BWnjpZ0DP/3v/9h7NixePjwIXbs2IF33nkHqampmDJlSqHrWcoxLEn9LOX4bdmyBTt27MCFCxdK9JpbyjFkQlWKvLy8DGbP8fHxAGAw6zbHumXF3DHWrFkTnTt3xsmTJ80Sn9ws4RiaW3k+hnPmzMGnn36KuXPn4p133imyvCUev+LW0ZDyegxr1KiBGjVqAAD69u0LAJgxYwZGjhwJHx8fg+tY0jEsSf0MKW/HLyUlBW+//Tbeffdd+Pn5ITExEQCQlZUFQPMrRxsbGzg5ORlc35KOIftQlaKmTZvi6tWryMnJ0ZkfHh4OAGjSpEmh62rLFXfdsmJK/QoihICVVcV4W1rCMSwN5fEYzpkzB6GhoQgNDcX//vc/o9axtONXkjoWpDwew/zatWuHnJwcREREFFjG0o5hXsbUryDl6fg9evQIDx48wKJFi+Dh4SE91q9fj9TUVHh4eODll18ucH2LOoby/siwYvvjjz8EALFhwwad+X369ClyWIGlS5cKAOLkyZPSvOzsbNG4cWPRvn37Uou5OEypnyERERHC2dlZDBw40Jxhmk1xhxWwhGOYV3HrZ0h5PIYff/yxACBmzpxZrPUs6fiVtI6GlMdjaMiIESOElZWVePjwYYFlLOkY5mdM/Qwpb8cvPT1dhIWF6T169+4t7O3tRVhYmAgPDy9wfUs6hkyoStnTTz8tPDw8xIoVK8SBAwfE66+/LgCINWvWSGXGjBkjrK2tRVRUlDQvIyNDNG7cWAQEBIi1a9eKvXv3iueff14olUpx8OBBOapiUEnr16NHDzFnzhyxbds2sX//frF48WLh5+cnXFxcCv3jksMff/whNm3aJH788UcBQAwdOlRs2rRJbNq0SaSmpgohLPsYlrR+lnAMFy5cKACIPn36iBMnTug9tCz5+JlSR0s4hq+//rqYPHmy+PXXX8XBgwfF5s2bxbBhwwQAMXXqVKmcpR5DU+pnCcevIIbGobLUY6jFhKqUJScniwkTJoiqVasKW1tb0axZM7F+/XqdMiNHjhQARGRkpM78mJgY8eqrrwpPT09hb28vOnToIPbu3VuG0RetpPWbOHGiCAoKEi4uLkKpVAo/Pz/xyiuviOvXr5dxDYoWGBgoABh8aOtkycewpPWzhGMYHBxcYN3yNtBb8vEzpY6WcAx//PFH0aVLF+Ht7S2USqVwd3cXwcHB4pdfftEpZ6nH0JT6WcLxK4ihhMpSj6GWQgghzHkJkYiIiKiyKR+91oiIiIgsGBMqIiIiIhMxoSIiIiIyERMqIiIiIhMxoSIiIiIyERMqIiIiIhMxoSIiIiIyERMqIiIzi4qKgkKhwKhRo+QOhYjKCBMqIiIiIhMxoSIiIiIyERMqIiIiIhMxoSKicu/w4cPo378/vL29YWdnh3r16mHmzJlIS0uTyhw8eBAKhQKhoaE4fPgwgoOD4ezsDE9PT7z00ku4e/euwW1fvnwZw4YNQ5UqVWBnZ4datWph0qRJiI+PN1j+4cOHmDJlCho0aAB7e3t4enqiQ4cOWLRokcHyERERGDJkCDw8PODk5ISePXvi77//Nv1FIaJyhTdHJqJybdmyZRg/fjw8PDzQv39/+Pj44MyZMzh06BA6deqEsLAw2Nra4uDBg+jWrRt69+6NsLAwPPvss2jYsCHOnz+PPXv2ICAgAGfOnIGvr6+07ePHj6NXr17IzMzEkCFDULNmTZw8eRIHDx5EvXr1cOLECXh5eUnl//33X3Tr1g3R0dHo3LkzOnXqhNTUVFy6dAn//POPlIRFRUWhVq1aCA4OxuXLlxEUFIQ2bdrg5s2b2L59Ozw8PHD16lWdWIjIwgkionLq8uXLQqlUipYtW4q4uDidZfPmzRMAxMKFC4UQQoSFhQkAAoD4/vvvdcrOmTNHABBjxoyR5uXm5op69eoJAGL37t065WfMmCEAiNdee01nfrt27QQAsWLFCr1Y79y5I01HRkZKscyfP1+n3MyZMwUAMW/evGK8EkRU3jGhIqJya8KECQKAOHLkiN6y3Nxc4ePjI1q3bi2EeJJQNWjQQKjVap2yaWlpwsfHRzg4OIjMzEwhhBCHDx8WAMQzzzyjt+2UlBTh5eWlU/706dMCgOjatWuRcWsTqlq1aonc3FyDywYNGmTci0BEFkFZ9m1iRETGOXnyJABg9+7d2Ldvn95yGxsbXLt2TWfeU089BYVCoTPPwcEBrVu3xu7du3Hjxg00adIEFy5cAACEhITobdfJyQlt2rTBnj17pPKnT58GAPTq1cvo+Js3bw4rK92uqv7+/gCAxMREo7dDROUfEyoiKre0fZLmzp1r9DpVqlQxOF/bXykpKQkAoFKpdObnV7VqVZ3y2gSoevXqRsfi5uamN0+p1Jx2c3Nzjd4OEZV//JUfEZVbrq6uADTJj9B0UTD4yOvhw4cGt/XgwQMAT5Ic7ba18wsqry3n7u4OAIiOjjahRkRUUTGhIqJyq3379gCeXPozxrFjx/SSrPT0dJw7dw4ODg6oX78+AKBly5YANMMt5JeWloazZ8/CwcEBDRo0AAC0a9cOAPDXX38Vux5EVPExoSKicmv8+PFQKpV49913cefOHb3liYmJUl8orevXr+PHH3/UmffFF18gNjYWw4cPh62tLQBNX6s6dergzz//1OufNW/ePDx69EinfNu2bdGuXTscPnwYK1eu1IuFLVdElRv7UBFRudWkSRMsXboU48aNQ4MGDdC3b1/UqVMHKpUKEREROHToEEaNGoVly5ZJ6/Tq1Qvjx4/Hrl279Mah+uyzz6RyVlZWWLVqFXr37o2+ffti6NChCAwMxKlTp3DgwAHUqVMH8+fP14lnzZo1CAkJwRtvvIFffvkFHTt2REZGBi5fvowLFy4gLi6uzF4bIipf2EJFROXa66+/jhMnTmDAgAE4ceIEvvrqK2zevBmPHj3CpEmTMHHiRJ3yHTt2xN69e/Ho0SMsWbIEp06dwosvvohjx47pdUDv3LkzTp48iQEDBuCvv/7CwoULcfPmTUyYMAEnT56Ej4+PTvl69erh/PnzeO+99xAdHY3FixdjzZo1SElJwcyZM0v7pSCicowjpRNRhaAdKX327NkIDQ2VOxwiqmTYQkVERERkIiZURERERCZiQkVERERkIvahIiIiIjIRW6iIiIiITMSEioiIiMhETKiIiIiITMSEioiIiMhETKiIiIiITMSEioiIiMhETKiIiIiITMSEioiIiMhETKiIiIiITPT/rrOkuH8boskAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.plot(indices, acc_vs, marker='.', alpha=1, label='valid-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('SCRUB retain-, valid- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Model_D_r -> Loss:0.003, Error:0.0\n",
      "Original_Model_D_f -> Loss:0.002, Error:0.0\n",
      "Original_Model_D_t -> Loss:0.594, Error:0.1598\n"
     ]
    }
   ],
   "source": [
    "m_D_r_activations,m_D_r_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(retain_loader),'Original_Model_D_r')\n",
    "m_D_f_activations,m_D_f_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(forget_loader),'Original_Model_D_f')\n",
    "m_D_t_activations,m_D_t_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(test_loader_full),'Original_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain_Model_D_r -> Loss:0.003, Error:0.0\n",
      "Retrain_Model_D_f -> Loss:12.57, Error:1.0\n",
      "Retrain_Model_D_t -> Loss:1.688, Error:0.2189\n"
     ]
    }
   ],
   "source": [
    "m0_D_r_activations,m0_D_r_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(retain_loader),'Retrain_Model_D_r')\n",
    "m0_D_f_activations,m0_D_f_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(forget_loader),'Retrain_Model_D_f')\n",
    "m0_D_t_activations,m0_D_t_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(test_loader_full),'Retrain_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRUB_D_r -> Loss:0.013, Error:0.0017222222222222222\n",
      "SCRUB_D_f -> Loss:22.13, Error:1.0\n",
      "SCRUB_D_t -> Loss:3.049, Error:0.2299\n"
     ]
    }
   ],
   "source": [
    "ntk_D_r_activations,ntk_D_r_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(retain_loader),'SCRUB_D_r')\n",
    "ntk_D_f_activations,ntk_D_f_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(forget_loader),'SCRUB_D_f')\n",
    "ntk_D_t_activations,ntk_D_t_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(test_loader_full),'SCRUB_D_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, modelf)\n",
    "#hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-7\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_D_r_activations,fisher_D_r_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(retain_loader),'Fisher_D_r')\n",
    "fisher_D_f_activations,fisher_D_f_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(forget_loader),'Fisher_D_f')\n",
    "fisher_D_t_activations,fisher_D_t_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(test_loader_full),'Fisher_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information in the Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_D_r_activations,finetune_D_r_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(retain_loader),'Finetune_D_r')\n",
    "finetune_D_f_activations,finetune_D_f_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(forget_loader),'Finetune_D_f')\n",
    "finetune_D_t_activations,finetune_D_t_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(test_loader_full),'Finetune_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(5,1,figsize=(9,20))\n",
    "ax = [0,0,0,0,0]\n",
    "plot_entropy_dist(copy.deepcopy(model_s),retain_loader,forget_loader,test_loader_full, 'SCRUB',ax[4])\n",
    "plot_entropy_dist(copy.deepcopy(model),retain_loader,forget_loader,test_loader_full, 'original', ax[0])\n",
    "plot_entropy_dist(copy.deepcopy(model0),retain_loader,forget_loader,test_loader_full, 'retrain', ax[1])\n",
    "plot_entropy_dist(copy.deepcopy(model_ft),retain_loader,forget_loader,test_loader_full, 'finetuned',ax[2])\n",
    "plot_entropy_dist(copy.deepcopy(modelf),retain_loader,forget_loader,test_loader_full, 'fisher',ax[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
