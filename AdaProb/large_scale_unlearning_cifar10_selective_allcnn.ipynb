{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_858/3351334044.py:20: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import variational\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activations along the path\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datasets\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def entropy(p, dim = -1, keepdim = False):\n",
    "    return -torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim=dim, keepdim=keepdim)\n",
    "    #return (p * p.log()).sum(dim=dim, keepdim=keepdim)\n",
    "\n",
    "def collect_prob(data_loader, model):\n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=False, num_samples=4000)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, sampler=sampler, shuffle=False)\n",
    "    prob = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(data_loader_small, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            data, target = batch\n",
    "            output = model(data)\n",
    "            prob.append(F.softmax(output, dim=-1).data)\n",
    "    return torch.cat(prob)\n",
    "\n",
    "def get_membership_attack_data(retain_loader, forget_loader, test_loader, model):    \n",
    "    retain_prob = collect_prob(retain_loader, model)\n",
    "    forget_prob = collect_prob(forget_loader, model)\n",
    "    test_prob = collect_prob(test_loader, model)\n",
    "    \n",
    "    X_r = torch.cat([entropy(retain_prob), entropy(test_prob)]).cpu().numpy().reshape(-1, 1)\n",
    "    Y_r = np.concatenate([np.ones(len(retain_prob)), np.zeros(len(test_prob))])\n",
    "    \n",
    "    X_f = entropy(forget_prob).cpu().numpy().reshape(-1, 1)\n",
    "    Y_f = np.concatenate([np.ones(len(forget_prob))])    \n",
    "    return X_f, Y_f, X_r, Y_r\n",
    "\n",
    "def get_membership_attack_prob(retain_loader, forget_loader, test_loader, model):\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader, forget_loader, test_loader, model)\n",
    "    #clf = SVC(C=3,gamma='auto',kernel='rbf')\n",
    "    clf = LogisticRegression(class_weight='balanced',solver='lbfgs',multi_class='multinomial')\n",
    "    clf.fit(X_r, Y_r)\n",
    "    results = clf.predict(X_f)\n",
    "    results1 = clf.predict(X_r)\n",
    "    acc = accuracy_score(results, Y_f)\n",
    "    train_ac = accuracy_score(results1, Y_r)\n",
    "    TP, FP, TN, FN = perf_measure(Y_r, results1)\n",
    "    FPR = FP/(FP+TN)\n",
    "    FNR = FN/(FN+TP)\n",
    "    \n",
    "    print (f\"TP:{TP}, FP{FP}, TN{TN}, FN{FN}\")\n",
    "    print (f\"false negative rate: {FN/(FN+TP)}\")\n",
    "    print (f\"false positive rate: {FP/(FP+TN)}\")\n",
    "    return acc, train_acc, FPR, FNR #results.mean(), results1.mean()\n",
    "    \n",
    "def plot_entropy_dist(model,retain_loader,forget_loader,test_loader_full, title, ax):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from matplotlib.ticker import FuncFormatter\n",
    "    #train_loader_full, valid_loader_full, test_loader_full = datasets.get_loaders(dataset, batch_size=100, seed=0, augment=False, shuffle=False)\n",
    "    #indexes = np.flatnonzero(np.array(train_loader_full.dataset.targets) == class_to_forget)\n",
    "    #replaced = np.random.RandomState(0).choice(indexes, size=100 if num_to_forget==100 else len(indexes), replace=False)\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader,forget_loader,test_loader_full, model)\n",
    "    #np.savetxt('retain_prob.txt', X_r[Y_r==1])\n",
    "    #np.savetxt('forget_prob.txt', X_f)\n",
    "    #np.savetxt('test_prob.txt', X_r[Y_r==0])\n",
    "    sns.distplot(np.log(X_r[Y_r==1]).reshape(-1), kde=False, norm_hist=True, rug=False, label='retain', ax=plt)\n",
    "    sns.distplot(np.log(X_r[Y_r==0]).reshape(-1), kde=False, norm_hist=True, rug=False, label='test', ax=plt)\n",
    "    sns.distplot(np.log(X_f).reshape(-1), kde=False, norm_hist=True, rug=False, label='forget', ax=plt)\n",
    "    plt.legend(prop={'size': 14})\n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.title(title,size=18)\n",
    "    plt.xlabel('Log of Entropy',size=14)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def membership_attack(retain_loader,forget_loader,test_loader,model, name):\n",
    "    prob, train_acc, FPR, FNR = get_membership_attack_prob(retain_loader,forget_loader,test_loader,model)\n",
    "    print(\"Attack prob: \", prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=512, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=512, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics=get_metrics(model,dataloader,criterion,False)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "    return _,_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        #run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.01, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    #print(f\"{name} ->\"\n",
    "    #      f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_activations(model_scrubf, modelf0, delta_w_s, delta_w_m0, data_loader, \\\n",
    "                    loss_fn=nn.CrossEntropyLoss(),\\\n",
    "                    optimizer=torch.optim.SGD, \\\n",
    "                    seed=1,quiet=False):\n",
    "\n",
    "    model_scrubf.eval()\n",
    "    modelf0.eval()\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    \n",
    "    metrics = AverageMeter()    \n",
    "    num_classes = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "        batch = [tensor.to(next(model_scrubf.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        \n",
    "        output_sf = model_scrubf(input)\n",
    "        G_sf = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=True)\n",
    "            grads = torch.cat([g.view(-1) for g in grads])\n",
    "            G_sf.append(grads)\n",
    "\n",
    "        grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_sf = torch.stack(G_sf)#.pow(2)\n",
    "        delta_f_sf_update = torch.matmul(G_sf,delta_w_s.sqrt()*torch.empty_like(delta_w_s).normal_())\n",
    "        G_sf = G_sf.pow(2)\n",
    "        delta_f_sf = torch.matmul(G_sf,delta_w_s)\n",
    "\n",
    "        output_m0 = modelf0(input)\n",
    "        G_m0 = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=True)\n",
    "            grad_m0 = torch.cat([g.view(-1) for g in grads])\n",
    "            G_m0.append(grad_m0)\n",
    "\n",
    "        grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_m0 = torch.stack(G_m0).pow(2)\n",
    "        delta_f_m0 = torch.matmul(G_m0,delta_w_m0)\n",
    "        \n",
    "        kl = ((output_m0 - output_sf).pow(2)/delta_f_m0 + delta_f_sf/delta_f_m0 - torch.log(delta_f_sf/delta_f_m0) - 1).sum()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        output_sf += delta_f_sf_update#delta_f_sf.sqrt()*torch.empty_like(delta_f_sf).normal_()\n",
    "        \n",
    "        loss = loss_fn(output_sf, target)\n",
    "        metrics.update(n=input.size(0), loss=loss.item(), error=get_error(output_sf, target), kl=kl.item())\n",
    "    \n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 100\n",
      "[0] train metrics:{\"loss\": 3.9548908950805663, \"error\": 0.9099}\n",
      "Learning Rate : 0.1\n",
      "[0] dry_run metrics:{\"loss\": 3.6569058528900147, \"error\": 0.864325}\n",
      "Learning Rate : 0.1\n",
      "[0] test metrics:{\"loss\": 3.669260646820068, \"error\": 0.8685}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 8.76 sec\n",
      "[1] train metrics:{\"loss\": 3.4458515224456785, \"error\": 0.82415}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.14 sec\n",
      "[2] train metrics:{\"loss\": 3.016041721725464, \"error\": 0.729925}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.21 sec\n",
      "[3] train metrics:{\"loss\": 2.711107554626465, \"error\": 0.65535}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.02 sec\n",
      "[4] train metrics:{\"loss\": 2.496060283279419, \"error\": 0.593225}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.04 sec\n",
      "[5] train metrics:{\"loss\": 2.329194931793213, \"error\": 0.538525}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.13 sec\n",
      "[6] train metrics:{\"loss\": 2.2086135860443115, \"error\": 0.49755}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.0 sec\n",
      "[7] train metrics:{\"loss\": 2.123908861541748, \"error\": 0.462825}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.97 sec\n",
      "[8] train metrics:{\"loss\": 2.049863201332092, \"error\": 0.431325}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.02 sec\n",
      "[9] train metrics:{\"loss\": 1.9988761100769044, \"error\": 0.40755}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.06 sec\n",
      "[10] train metrics:{\"loss\": 1.9432452823638915, \"error\": 0.379025}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.06 sec\n",
      "[11] train metrics:{\"loss\": 1.9131594074249267, \"error\": 0.35665}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.0 sec\n",
      "[12] train metrics:{\"loss\": 1.8789236625671386, \"error\": 0.332075}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.01 sec\n",
      "[13] train metrics:{\"loss\": 1.8584274074554443, \"error\": 0.31195}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.01 sec\n",
      "[14] train metrics:{\"loss\": 1.835866969680786, \"error\": 0.29505}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.1 sec\n",
      "[15] train metrics:{\"loss\": 1.8279486743927003, \"error\": 0.2791}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.01 sec\n",
      "[16] train metrics:{\"loss\": 1.8283887172698974, \"error\": 0.26425}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.98 sec\n",
      "[17] train metrics:{\"loss\": 1.7811426517486573, \"error\": 0.240475}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.13 sec\n",
      "[18] train metrics:{\"loss\": 1.7966106941223146, \"error\": 0.23225}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.02 sec\n",
      "[19] train metrics:{\"loss\": 1.791147449493408, \"error\": 0.21845}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.07 sec\n",
      "[20] train metrics:{\"loss\": 1.7986805698394774, \"error\": 0.20865}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.0 sec\n",
      "[21] train metrics:{\"loss\": 1.802754467010498, \"error\": 0.20475}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.91 sec\n",
      "[22] train metrics:{\"loss\": 1.793176968383789, \"error\": 0.19225}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.15 sec\n",
      "[23] train metrics:{\"loss\": 1.8183617504119873, \"error\": 0.19235}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.14 sec\n",
      "[24] train metrics:{\"loss\": 1.7976149871826173, \"error\": 0.17805}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.07 sec\n",
      "[25] train metrics:{\"loss\": 1.8099366149902343, \"error\": 0.178875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.19 sec\n",
      "[26] train metrics:{\"loss\": 1.791230718421936, \"error\": 0.168925}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.29 sec\n",
      "[27] train metrics:{\"loss\": 1.8296453895568847, \"error\": 0.176575}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.26 sec\n",
      "[28] train metrics:{\"loss\": 1.79987815284729, \"error\": 0.16505}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.39 sec\n",
      "[29] train metrics:{\"loss\": 1.8186018115997316, \"error\": 0.16935}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.27 sec\n",
      "[30] train metrics:{\"loss\": 1.8211508485794068, \"error\": 0.1652}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 4.29 sec\n",
      "Pure training time: 127.45999999999998 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset cifar100 --dataroot=data/cifar-100-python --model allcnn --filters 1.0 --lr 0.1 --lossfn ce --num-classes 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar10_allcnn_1_0_forget_None_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar10_allcnn_1_0_forget_None_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Number of Classes: 10\n",
      "Epoch: [0][0/313]\tTime 0.066 (0.066)\tData 0.016 (0.016)\tLoss 2.3841 (2.3841)\tAcc@1 16.406 (16.406)\tAcc@5 53.125 (53.125)\n",
      " * Acc@1 68.640 Acc@5 97.112\n",
      "[0] test metrics:{\"loss\": 0.6585341669082642, \"error\": 0.2298}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 4.3 sec\n",
      "Epoch: [1][0/313]\tTime 0.013 (0.013)\tData 0.008 (0.008)\tLoss 0.5724 (0.5724)\tAcc@1 81.250 (81.250)\tAcc@5 99.219 (99.219)\n",
      " * Acc@1 80.620 Acc@5 99.275\n",
      "Epoch Time: 3.53 sec\n",
      "Epoch: [2][0/313]\tTime 0.011 (0.011)\tData 0.007 (0.007)\tLoss 0.4610 (0.4610)\tAcc@1 85.938 (85.938)\tAcc@5 99.219 (99.219)\n",
      " * Acc@1 85.045 Acc@5 99.567\n",
      "Epoch Time: 3.5 sec\n",
      "Epoch: [3][0/313]\tTime 0.014 (0.014)\tData 0.009 (0.009)\tLoss 0.3552 (0.3552)\tAcc@1 86.719 (86.719)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 88.465 Acc@5 99.695\n",
      "Epoch Time: 3.51 sec\n",
      "Epoch: [4][0/313]\tTime 0.020 (0.020)\tData 0.015 (0.015)\tLoss 0.1416 (0.1416)\tAcc@1 96.875 (96.875)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 91.327 Acc@5 99.830\n",
      "Epoch Time: 3.56 sec\n",
      "Epoch: [5][0/313]\tTime 0.022 (0.022)\tData 0.017 (0.017)\tLoss 0.1272 (0.1272)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 94.235 Acc@5 99.910\n",
      "[5] test metrics:{\"loss\": 0.5239728908538819, \"error\": 0.1594}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 4.31 sec\n",
      "Epoch: [6][0/313]\tTime 0.030 (0.030)\tData 0.025 (0.025)\tLoss 0.1445 (0.1445)\tAcc@1 95.312 (95.312)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 96.740 Acc@5 99.960\n",
      "Epoch Time: 3.54 sec\n",
      "Epoch: [7][0/313]\tTime 0.022 (0.022)\tData 0.017 (0.017)\tLoss 0.1222 (0.1222)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 98.447 Acc@5 99.985\n",
      "Epoch Time: 3.55 sec\n",
      "Epoch: [8][0/313]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0580 (0.0580)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.365 Acc@5 99.995\n",
      "Epoch Time: 3.53 sec\n",
      "Epoch: [9][0/313]\tTime 0.023 (0.023)\tData 0.017 (0.017)\tLoss 0.0288 (0.0288)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.837 Acc@5 99.997\n",
      "Epoch Time: 3.68 sec\n",
      "Epoch: [10][0/313]\tTime 0.023 (0.023)\tData 0.017 (0.017)\tLoss 0.0219 (0.0219)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.932 Acc@5 100.000\n",
      "[10] test metrics:{\"loss\": 0.6609334421157836, \"error\": 0.1559}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 4.5 sec\n",
      "Epoch: [11][0/313]\tTime 0.029 (0.029)\tData 0.023 (0.023)\tLoss 0.0142 (0.0142)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.970 Acc@5 100.000\n",
      "Epoch Time: 3.51 sec\n",
      "Epoch: [12][0/313]\tTime 0.023 (0.023)\tData 0.016 (0.016)\tLoss 0.0116 (0.0116)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "Epoch Time: 3.55 sec\n",
      "Epoch: [13][0/313]\tTime 0.011 (0.011)\tData 0.007 (0.007)\tLoss 0.0094 (0.0094)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.985 Acc@5 100.000\n",
      "Epoch Time: 3.58 sec\n",
      "Epoch: [14][0/313]\tTime 0.022 (0.022)\tData 0.017 (0.017)\tLoss 0.0094 (0.0094)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "Epoch Time: 3.59 sec\n",
      "Epoch: [15][0/313]\tTime 0.015 (0.015)\tData 0.011 (0.011)\tLoss 0.0058 (0.0058)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.993 Acc@5 100.000\n",
      "[15] test metrics:{\"loss\": 0.7274604979515076, \"error\": 0.1579}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 4.36 sec\n",
      "Epoch: [16][0/313]\tTime 0.020 (0.020)\tData 0.016 (0.016)\tLoss 0.0091 (0.0091)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 3.58 sec\n",
      "Epoch: [17][0/313]\tTime 0.024 (0.024)\tData 0.019 (0.019)\tLoss 0.0070 (0.0070)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 3.42 sec\n",
      "Epoch: [18][0/313]\tTime 0.022 (0.022)\tData 0.016 (0.016)\tLoss 0.0182 (0.0182)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 3.34 sec\n",
      "Epoch: [19][0/313]\tTime 0.022 (0.022)\tData 0.018 (0.018)\tLoss 0.0053 (0.0053)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 3.3 sec\n",
      "Epoch: [20][0/313]\tTime 0.024 (0.024)\tData 0.019 (0.019)\tLoss 0.0054 (0.0054)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[20] test metrics:{\"loss\": 0.7736012166976929, \"error\": 0.1595}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 4.09 sec\n",
      "Epoch: [21][0/313]\tTime 0.027 (0.027)\tData 0.022 (0.022)\tLoss 0.0052 (0.0052)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 3.34 sec\n",
      "Epoch: [22][0/313]\tTime 0.011 (0.011)\tData 0.007 (0.007)\tLoss 0.0077 (0.0077)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 3.51 sec\n",
      "Epoch: [23][0/313]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0037 (0.0037)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 3.55 sec\n",
      "Epoch: [24][0/313]\tTime 0.023 (0.023)\tData 0.017 (0.017)\tLoss 0.0049 (0.0049)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 3.65 sec\n",
      "Epoch: [25][0/313]\tTime 0.022 (0.022)\tData 0.017 (0.017)\tLoss 0.0047 (0.0047)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[25] test metrics:{\"loss\": 0.8194447939872742, \"error\": 0.1599}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 4.34 sec\n",
      "Pure training time: 91.4 sec\n"
     ]
    }
   ],
   "source": [
    "%run main_merged.py --dataset cifar10 --model allcnn --dataroot=data/cifar-10-batches-py/ --filters 1.0 --lr 0.01 \\\n",
    "--resume checkpoints/cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 5e-4 --batch-size 128 --epochs 26 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar10_allcnn_1_0_forget_[5]_num_100_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar10_allcnn_1_0_forget_[5]_num_100_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [22644  6987 36335 23839  1559  5648 18421  4310 12085 10658 22192  2404\n",
      " 28077 23062 33361 18035 28713 24244 36749 30401 34869 14428  9997 10500\n",
      "  9429  5670 31993 23103 36858 37665 18393   364 10553 33173 11641 18201\n",
      "  5622 34816  9309 26329 22289 20137 25274 32709 29643 36973 38557 16388\n",
      " 26011 18122 13663  3617 33612 19687 13240 11287  6150 38579 21265 25977\n",
      "  2308 39186 16293 31764  6557 31314  9730  5069 15650 14230 20863 17632\n",
      "  3979 30893 11810 32154 25290 37680  9940 31707 12928 15234 22597 35439\n",
      " 33722 14472  9259 15288  1960  3135 22250 26104 26497 32327   386 19083\n",
      " 37138 13509 39902  7136]\n",
      "Number of Classes: 10\n",
      "Epoch: [0][0/313]\tTime 0.031 (0.031)\tData 0.024 (0.024)\tLoss 2.3841 (2.3841)\tAcc@1 16.406 (16.406)\tAcc@5 53.125 (53.125)\n",
      " * Acc@1 68.692 Acc@5 97.122\n",
      "[0] test metrics:{\"loss\": 0.6584638257980346, \"error\": 0.2295}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 4.26 sec\n",
      "Epoch: [1][0/313]\tTime 0.022 (0.022)\tData 0.017 (0.017)\tLoss 0.5720 (0.5720)\tAcc@1 81.250 (81.250)\tAcc@5 99.219 (99.219)\n",
      " * Acc@1 80.728 Acc@5 99.290\n",
      "Epoch Time: 3.33 sec\n",
      "Epoch: [2][0/313]\tTime 0.012 (0.012)\tData 0.008 (0.008)\tLoss 0.4603 (0.4603)\tAcc@1 85.938 (85.938)\tAcc@5 99.219 (99.219)\n",
      " * Acc@1 85.127 Acc@5 99.575\n",
      "Epoch Time: 3.33 sec\n",
      "Epoch: [3][0/313]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.3523 (0.3523)\tAcc@1 86.719 (86.719)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 88.452 Acc@5 99.688\n",
      "Epoch Time: 3.32 sec\n",
      "Epoch: [4][0/313]\tTime 0.013 (0.013)\tData 0.008 (0.008)\tLoss 0.1418 (0.1418)\tAcc@1 96.875 (96.875)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 91.342 Acc@5 99.825\n",
      "Epoch Time: 3.35 sec\n",
      "Epoch: [5][0/313]\tTime 0.019 (0.019)\tData 0.014 (0.014)\tLoss 0.1249 (0.1249)\tAcc@1 96.875 (96.875)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 94.267 Acc@5 99.908\n",
      "[5] test metrics:{\"loss\": 0.5247040887832641, \"error\": 0.1594}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 4.14 sec\n",
      "Epoch: [6][0/313]\tTime 0.030 (0.030)\tData 0.024 (0.024)\tLoss 0.1454 (0.1454)\tAcc@1 95.312 (95.312)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 96.802 Acc@5 99.962\n",
      "Epoch Time: 3.36 sec\n",
      "Epoch: [7][0/313]\tTime 0.013 (0.013)\tData 0.008 (0.008)\tLoss 0.1169 (0.1169)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 98.480 Acc@5 99.982\n",
      "Epoch Time: 3.33 sec\n",
      "Epoch: [8][0/313]\tTime 0.016 (0.016)\tData 0.011 (0.011)\tLoss 0.0554 (0.0554)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.382 Acc@5 99.995\n",
      "Epoch Time: 3.4 sec\n",
      "Epoch: [9][0/313]\tTime 0.011 (0.011)\tData 0.007 (0.007)\tLoss 0.0289 (0.0289)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.832 Acc@5 99.997\n",
      "Epoch Time: 3.42 sec\n",
      "Epoch: [10][0/313]\tTime 0.018 (0.018)\tData 0.013 (0.013)\tLoss 0.0221 (0.0221)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.935 Acc@5 100.000\n",
      "[10] test metrics:{\"loss\": 0.6618290606498718, \"error\": 0.1566}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 4.23 sec\n",
      "Epoch: [11][0/313]\tTime 0.017 (0.017)\tData 0.012 (0.012)\tLoss 0.0152 (0.0152)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.975 Acc@5 100.000\n",
      "Epoch Time: 3.42 sec\n",
      "Epoch: [12][0/313]\tTime 0.023 (0.023)\tData 0.019 (0.019)\tLoss 0.0114 (0.0114)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.995 Acc@5 100.000\n",
      "Epoch Time: 3.41 sec\n",
      "Epoch: [13][0/313]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0093 (0.0093)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.987 Acc@5 100.000\n",
      "Epoch Time: 3.42 sec\n",
      "Epoch: [14][0/313]\tTime 0.016 (0.016)\tData 0.011 (0.011)\tLoss 0.0099 (0.0099)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 3.44 sec\n",
      "Epoch: [15][0/313]\tTime 0.024 (0.024)\tData 0.019 (0.019)\tLoss 0.0062 (0.0062)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.990 Acc@5 100.000\n",
      "[15] test metrics:{\"loss\": 0.7294946759223938, \"error\": 0.1582}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 4.17 sec\n",
      "Epoch: [16][0/313]\tTime 0.025 (0.025)\tData 0.019 (0.019)\tLoss 0.0090 (0.0090)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 3.41 sec\n",
      "Epoch: [17][0/313]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0066 (0.0066)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.997 Acc@5 100.000\n",
      "Epoch Time: 3.39 sec\n",
      "Epoch: [18][0/313]\tTime 0.017 (0.017)\tData 0.013 (0.013)\tLoss 0.0193 (0.0193)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 3.36 sec\n",
      "Epoch: [19][0/313]\tTime 0.013 (0.013)\tData 0.008 (0.008)\tLoss 0.0054 (0.0054)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 3.36 sec\n",
      "Epoch: [20][0/313]\tTime 0.023 (0.023)\tData 0.019 (0.019)\tLoss 0.0055 (0.0055)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[20] test metrics:{\"loss\": 0.7751746067047119, \"error\": 0.1586}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 4.16 sec\n",
      "Epoch: [21][0/313]\tTime 0.013 (0.013)\tData 0.008 (0.008)\tLoss 0.0050 (0.0050)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 3.5 sec\n",
      "Epoch: [22][0/313]\tTime 0.013 (0.013)\tData 0.008 (0.008)\tLoss 0.0068 (0.0068)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 3.37 sec\n",
      "Epoch: [23][0/313]\tTime 0.014 (0.014)\tData 0.009 (0.009)\tLoss 0.0038 (0.0038)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 3.55 sec\n",
      "Epoch: [24][0/313]\tTime 0.023 (0.023)\tData 0.019 (0.019)\tLoss 0.0049 (0.0049)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 3.41 sec\n",
      "Epoch: [25][0/313]\tTime 0.014 (0.014)\tData 0.010 (0.010)\tLoss 0.0047 (0.0047)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[25] test metrics:{\"loss\": 0.8211732999801635, \"error\": 0.16}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 4.09 sec\n",
      "Pure training time: 88.13 sec\n"
     ]
    }
   ],
   "source": [
    "%run main_merged.py --dataset cifar10 --model allcnn --dataroot=data/cifar-10-batches-py/ --filters 1 --lr 0.01 \\\n",
    "--resume checkpoints/cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 5e-4 --batch-size 128 --epochs 26 \\\n",
    "--forget-class 5 --num-to-forget 100 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 1620010\n"
     ]
    }
   ],
   "source": [
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model0 = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 4000#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model0.cuda()\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [22644  6987 36335 23839  1559  5648 18421  4310 12085 10658 22192  2404\n",
      " 28077 23062 33361 18035 28713 24244 36749 30401 34869 14428  9997 10500\n",
      "  9429  5670 31993 23103 36858 37665 18393   364 10553 33173 11641 18201\n",
      "  5622 34816  9309 26329 22289 20137 25274 32709 29643 36973 38557 16388\n",
      " 26011 18122 13663  3617 33612 19687 13240 11287  6150 38579 21265 25977\n",
      "  2308 39186 16293 31764  6557 31314  9730  5069 15650 14230 20863 17632\n",
      "  3979 30893 11810 32154 25290 37680  9940 31707 12928 15234 22597 35439\n",
      " 33722 14472  9259 15288  1960  3135 22250 26104 26497 32327   386 19083\n",
      " 37138 13509 39902  7136]\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _  = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=512, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=256, seed=seed, shuffle=True)\n",
    "\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "39900\n",
      "10000\n",
      "40000\n",
      "{6: 4000, 9: 4000, 4: 4000, 1: 4000, 2: 4000, 8: 4000, 3: 4000, 7: 4000, 5: 4000, 0: 4000}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'sgd'\n",
    "args.gamma = 0.99\n",
    "args.alpha = 0.001\n",
    "args.beta = 0.9\n",
    "args.smoothing = 0.0\n",
    "args.msteps = 5\n",
    "args.clip = 0.2\n",
    "args.sstart = 5\n",
    "args.kd_T = 4\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_batch_size = 64\n",
    "args.del_batch_size = 16\n",
    "args.sgda_epochs = 3\n",
    "args.sgda_learning_rate = 0.0025\n",
    "args.lr_decay_epochs = [3,5,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> SCRUB unlearning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihao/anaconda3/envs/zihao/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 99.950 \n",
      "maximize loss: -21.79\t minimize loss: 0.01\t train_acc: 99.94987487792969\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 99.704 \n",
      "maximize loss: -35.24\t minimize loss: 0.02\t train_acc: 99.70426177978516\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 99.639 \n",
      "maximize loss: -50.00\t minimize loss: 0.02\t train_acc: 99.63909912109375\n"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "acc_vs = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> SCRUB unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True, prefix=\"train_cifar10_selective_allcnn\")\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True, prefix=\"forget_cifar10_selective_allcnn\")\n",
    "    acc_v, acc5_v, loss_v = validate(valid_loader_full, model_s, criterion_cls, args, True)\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "    acc_vs.append(100-acc_v.item()) \n",
    "    \n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\")\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    \n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n",
    "acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "acc_v, acc5_v, loss_v = validate(valid_loader_full, model_s, criterion_cls, args, True)\n",
    "acc_rs.append(100-acc_r.item())\n",
    "acc_fs.append(100-acc_f.item())\n",
    "acc_vs.append(100-acc_v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.plot(indices, acc_vs, marker='.', alpha=1, label='valid-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('SCRUB retain-, valid- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Model_D_r -> Loss:0.003, Error:0.0\n",
      "Original_Model_D_f -> Loss:0.002, Error:0.0\n",
      "Original_Model_D_t -> Loss:0.594, Error:0.1598\n"
     ]
    }
   ],
   "source": [
    "m_D_r_activations,m_D_r_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(retain_loader),'Original_Model_D_r')\n",
    "m_D_f_activations,m_D_f_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(forget_loader),'Original_Model_D_f')\n",
    "m_D_t_activations,m_D_t_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(test_loader_full),'Original_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain_Model_D_r -> Loss:0.003, Error:0.0\n",
      "Retrain_Model_D_f -> Loss:0.856, Error:0.22\n",
      "Retrain_Model_D_t -> Loss:0.596, Error:0.16\n"
     ]
    }
   ],
   "source": [
    "m0_D_r_activations,m0_D_r_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(retain_loader),'Retrain_Model_D_r')\n",
    "m0_D_f_activations,m0_D_f_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(forget_loader),'Retrain_Model_D_f')\n",
    "m0_D_t_activations,m0_D_t_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(test_loader_full),'Retrain_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRUB_D_r -> Loss:0.005, Error:0.0\n",
      "SCRUB_D_f -> Loss:2.385, Error:0.27\n",
      "SCRUB_D_t -> Loss:0.68, Error:0.1662\n"
     ]
    }
   ],
   "source": [
    "ntk_D_r_activations,ntk_D_r_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(retain_loader),'SCRUB_D_r')\n",
    "ntk_D_f_activations,ntk_D_f_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(forget_loader),'SCRUB_D_f')\n",
    "ntk_D_t_activations,ntk_D_t_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(test_loader_full),'SCRUB_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, modelf)\n",
    "#hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-7\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_D_r_activations,fisher_D_r_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(retain_loader),'Fisher_D_r')\n",
    "fisher_D_f_activations,fisher_D_f_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(forget_loader),'Fisher_D_f')\n",
    "fisher_D_t_activations,fisher_D_t_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(test_loader_full),'Fisher_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information in the Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_D_r_activations,finetune_D_r_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(retain_loader),'Finetune_D_r')\n",
    "finetune_D_f_activations,finetune_D_f_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(forget_loader),'Finetune_D_f')\n",
    "finetune_D_t_activations,finetune_D_t_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(test_loader_full),'Finetune_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(5,1,figsize=(9,20))\n",
    "ax = [0,0,0,0,0]\n",
    "plot_entropy_dist(copy.deepcopy(model_s),retain_loader,forget_loader,test_loader_full, 'SCRUB',ax[4])\n",
    "plot_entropy_dist(copy.deepcopy(model),retain_loader,forget_loader,test_loader_full, 'original', ax[0])\n",
    "plot_entropy_dist(copy.deepcopy(model0),retain_loader,forget_loader,test_loader_full, 'retrain', ax[1])\n",
    "plot_entropy_dist(copy.deepcopy(model_ft),retain_loader,forget_loader,test_loader_full, 'finetuned',ax[2])\n",
    "plot_entropy_dist(copy.deepcopy(modelf),retain_loader,forget_loader,test_loader_full, 'fisher',ax[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
