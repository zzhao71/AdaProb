{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393416/3351334044.py:20: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import variational\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activations along the path\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datasets\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def entropy(p, dim = -1, keepdim = False):\n",
    "    return -torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim=dim, keepdim=keepdim)\n",
    "    #return (p * p.log()).sum(dim=dim, keepdim=keepdim)\n",
    "\n",
    "def collect_prob(data_loader, model):\n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=False, num_samples=4000)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, sampler=sampler, shuffle=False)\n",
    "    prob = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(data_loader_small, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            data, target = batch\n",
    "            output = model(data)\n",
    "            prob.append(F.softmax(output, dim=-1).data)\n",
    "    return torch.cat(prob)\n",
    "\n",
    "def get_membership_attack_data(retain_loader, forget_loader, test_loader, model):    \n",
    "    retain_prob = collect_prob(retain_loader, model)\n",
    "    forget_prob = collect_prob(forget_loader, model)\n",
    "    test_prob = collect_prob(test_loader, model)\n",
    "    \n",
    "    X_r = torch.cat([entropy(retain_prob), entropy(test_prob)]).cpu().numpy().reshape(-1, 1)\n",
    "    Y_r = np.concatenate([np.ones(len(retain_prob)), np.zeros(len(test_prob))])\n",
    "    \n",
    "    X_f = entropy(forget_prob).cpu().numpy().reshape(-1, 1)\n",
    "    Y_f = np.concatenate([np.ones(len(forget_prob))])    \n",
    "    return X_f, Y_f, X_r, Y_r\n",
    "\n",
    "def get_membership_attack_prob(retain_loader, forget_loader, test_loader, model):\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader, forget_loader, test_loader, model)\n",
    "    #clf = SVC(C=3,gamma='auto',kernel='rbf')\n",
    "    clf = LogisticRegression(class_weight='balanced',solver='lbfgs',multi_class='multinomial')\n",
    "    clf.fit(X_r, Y_r)\n",
    "    results = clf.predict(X_f)\n",
    "    results1 = clf.predict(X_r)\n",
    "    acc = accuracy_score(results, Y_f)\n",
    "    train_ac = accuracy_score(results1, Y_r)\n",
    "    TP, FP, TN, FN = perf_measure(Y_r, results1)\n",
    "    FPR = FP/(FP+TN)\n",
    "    FNR = FN/(FN+TP)\n",
    "    \n",
    "    print (f\"TP:{TP}, FP{FP}, TN{TN}, FN{FN}\")\n",
    "    print (f\"false negative rate: {FN/(FN+TP)}\")\n",
    "    print (f\"false positive rate: {FP/(FP+TN)}\")\n",
    "    return acc, train_acc, FPR, FNR #results.mean(), results1.mean()\n",
    "    \n",
    "def plot_entropy_dist(model,retain_loader,forget_loader,test_loader_full, title, ax):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from matplotlib.ticker import FuncFormatter\n",
    "    #train_loader_full, valid_loader_full, test_loader_full = datasets.get_loaders(dataset, batch_size=100, seed=0, augment=False, shuffle=False)\n",
    "    #indexes = np.flatnonzero(np.array(train_loader_full.dataset.targets) == class_to_forget)\n",
    "    #replaced = np.random.RandomState(0).choice(indexes, size=100 if num_to_forget==100 else len(indexes), replace=False)\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader,forget_loader,test_loader_full, model)\n",
    "    #np.savetxt('retain_prob.txt', X_r[Y_r==1])\n",
    "    #np.savetxt('forget_prob.txt', X_f)\n",
    "    #np.savetxt('test_prob.txt', X_r[Y_r==0])\n",
    "    sns.distplot(np.log(X_r[Y_r==1]).reshape(-1), kde=False, norm_hist=True, rug=False, label='retain', ax=plt)\n",
    "    sns.distplot(np.log(X_r[Y_r==0]).reshape(-1), kde=False, norm_hist=True, rug=False, label='test', ax=plt)\n",
    "    sns.distplot(np.log(X_f).reshape(-1), kde=False, norm_hist=True, rug=False, label='forget', ax=plt)\n",
    "    plt.legend(prop={'size': 14})\n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.title(title,size=18)\n",
    "    plt.xlabel('Log of Entropy',size=14)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def membership_attack(retain_loader,forget_loader,test_loader,model, name):\n",
    "    prob, train_acc, FPR, FNR = get_membership_attack_prob(retain_loader,forget_loader,test_loader,model)\n",
    "    print(\"Attack prob: \", prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=512, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=512, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics=get_metrics(model,dataloader,criterion,False)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "    return _,_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        #run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.01, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    #print(f\"{name} ->\"\n",
    "    #      f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_activations(model_scrubf, modelf0, delta_w_s, delta_w_m0, data_loader, \\\n",
    "                    loss_fn=nn.CrossEntropyLoss(),\\\n",
    "                    optimizer=torch.optim.SGD, \\\n",
    "                    seed=1,quiet=False):\n",
    "\n",
    "    model_scrubf.eval()\n",
    "    modelf0.eval()\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    \n",
    "    metrics = AverageMeter()    \n",
    "    num_classes = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "        batch = [tensor.to(next(model_scrubf.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        \n",
    "        output_sf = model_scrubf(input)\n",
    "        G_sf = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=True)\n",
    "            grads = torch.cat([g.view(-1) for g in grads])\n",
    "            G_sf.append(grads)\n",
    "\n",
    "        grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_sf = torch.stack(G_sf)#.pow(2)\n",
    "        delta_f_sf_update = torch.matmul(G_sf,delta_w_s.sqrt()*torch.empty_like(delta_w_s).normal_())\n",
    "        G_sf = G_sf.pow(2)\n",
    "        delta_f_sf = torch.matmul(G_sf,delta_w_s)\n",
    "\n",
    "        output_m0 = modelf0(input)\n",
    "        G_m0 = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=True)\n",
    "            grad_m0 = torch.cat([g.view(-1) for g in grads])\n",
    "            G_m0.append(grad_m0)\n",
    "\n",
    "        grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_m0 = torch.stack(G_m0).pow(2)\n",
    "        delta_f_m0 = torch.matmul(G_m0,delta_w_m0)\n",
    "        \n",
    "        kl = ((output_m0 - output_sf).pow(2)/delta_f_m0 + delta_f_sf/delta_f_m0 - torch.log(delta_f_sf/delta_f_m0) - 1).sum()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        output_sf += delta_f_sf_update#delta_f_sf.sqrt()*torch.empty_like(delta_f_sf).normal_()\n",
    "        \n",
    "        loss = loss_fn(output_sf, target)\n",
    "        metrics.update(n=input.size(0), loss=loss.item(), error=get_error(output_sf, target), kl=kl.item())\n",
    "    \n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: lacuna100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in lacuna100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 100\n",
      "[0] train metrics:{\"loss\": 3.8233655672073366, \"error\": 0.91084375}\n",
      "Learning Rate : 0.1\n",
      "[0] dry_run metrics:{\"loss\": 3.3233438425064086, \"error\": 0.8385625}\n",
      "Learning Rate : 0.1\n",
      "[0] test metrics:{\"loss\": 3.3385062034606934, \"error\": 0.8347}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 7.63 sec\n",
      "[1] train metrics:{\"loss\": 3.040908597946167, \"error\": 0.77615625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.75 sec\n",
      "[2] train metrics:{\"loss\": 2.4334757966995237, \"error\": 0.61228125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.43 sec\n",
      "[3] train metrics:{\"loss\": 1.9808078465461731, \"error\": 0.47328125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.44 sec\n",
      "[4] train metrics:{\"loss\": 1.7013714575767518, \"error\": 0.372}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.45 sec\n",
      "[5] train metrics:{\"loss\": 1.4868793172836303, \"error\": 0.29221875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.44 sec\n",
      "[6] train metrics:{\"loss\": 1.3205368399620057, \"error\": 0.22421875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.46 sec\n",
      "[7] train metrics:{\"loss\": 1.2272959723472596, \"error\": 0.18584375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.43 sec\n",
      "[8] train metrics:{\"loss\": 1.133659630537033, \"error\": 0.1455625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.45 sec\n",
      "[9] train metrics:{\"loss\": 1.058427065372467, \"error\": 0.1150625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.4 sec\n",
      "[10] train metrics:{\"loss\": 1.0026730167865754, \"error\": 0.09853125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.38 sec\n",
      "[11] train metrics:{\"loss\": 0.9732379763126373, \"error\": 0.0879375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.39 sec\n",
      "[12] train metrics:{\"loss\": 0.9377931756973267, \"error\": 0.078125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.41 sec\n",
      "[13] train metrics:{\"loss\": 0.9080658020973206, \"error\": 0.07284375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.44 sec\n",
      "[14] train metrics:{\"loss\": 0.9161779451370239, \"error\": 0.07359375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.42 sec\n",
      "[15] train metrics:{\"loss\": 0.8948067147731781, \"error\": 0.06978125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.42 sec\n",
      "[16] train metrics:{\"loss\": 0.9115999133586884, \"error\": 0.07090625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.41 sec\n",
      "[17] train metrics:{\"loss\": 0.8692510938644409, \"error\": 0.06215625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.45 sec\n",
      "[18] train metrics:{\"loss\": 0.850548889875412, \"error\": 0.05765625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.45 sec\n",
      "[19] train metrics:{\"loss\": 0.8475376484394074, \"error\": 0.0595625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.35 sec\n",
      "[20] train metrics:{\"loss\": 0.8996174812316895, \"error\": 0.0716875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.29 sec\n",
      "[21] train metrics:{\"loss\": 0.9307970433235169, \"error\": 0.07428125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.3 sec\n",
      "[22] train metrics:{\"loss\": 0.8617413666248321, \"error\": 0.053}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.32 sec\n",
      "[23] train metrics:{\"loss\": 0.7489767985343934, \"error\": 0.03721875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.25 sec\n",
      "[24] train metrics:{\"loss\": 0.6187533376216888, \"error\": 0.0264375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.22 sec\n",
      "[25] train metrics:{\"loss\": 0.5147787270545959, \"error\": 0.02403125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.25 sec\n",
      "[26] train metrics:{\"loss\": 0.4404430638551712, \"error\": 0.02259375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.27 sec\n",
      "[27] train metrics:{\"loss\": 0.5160071790218353, \"error\": 0.0500625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.31 sec\n",
      "[28] train metrics:{\"loss\": 1.382216766834259, \"error\": 0.2360625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.39 sec\n",
      "[29] train metrics:{\"loss\": 1.0175488970279694, \"error\": 0.0950625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.68 sec\n",
      "[30] train metrics:{\"loss\": 0.8608677124977112, \"error\": 0.05125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.81 sec\n",
      "Pure training time: 106.26 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset lacuna100 --dataroot=data/lacuna100 --model allcnn --filters 1.0 --lr 0.1 --lossfn ce --num-classes 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: lacuna10_allcnn_1_0_forget_None_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in lacuna10_allcnn_1_0_forget_None_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Number of Classes: 10\n",
      "Epoch: [0][0/25]\tTime 0.041 (0.041)\tData 0.007 (0.007)\tLoss 1.7051 (1.7051)\tAcc@1 57.031 (57.031)\tAcc@5 86.719 (86.719)\n",
      " * Acc@1 86.750 Acc@5 97.969\n",
      "[0] test metrics:{\"loss\": 0.2643024432659149, \"error\": 0.050999993324279784}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.45 sec\n",
      "Epoch: [1][0/25]\tTime 0.012 (0.012)\tData 0.006 (0.006)\tLoss 0.2843 (0.2843)\tAcc@1 94.531 (94.531)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 95.562 Acc@5 99.781\n",
      "Epoch Time: 0.32 sec\n",
      "Epoch: [2][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.1189 (0.1189)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 97.906 Acc@5 99.906\n",
      "Epoch Time: 0.31 sec\n",
      "Epoch: [3][0/25]\tTime 0.012 (0.012)\tData 0.006 (0.006)\tLoss 0.0765 (0.0765)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.031 Acc@5 99.906\n",
      "Epoch Time: 0.33 sec\n",
      "Epoch: [4][0/25]\tTime 0.012 (0.012)\tData 0.006 (0.006)\tLoss 0.0227 (0.0227)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.531 Acc@5 99.906\n",
      "Epoch Time: 0.32 sec\n",
      "Epoch: [5][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0466 (0.0466)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.812 Acc@5 100.000\n",
      "[5] test metrics:{\"loss\": 0.08815042459964752, \"error\": 0.019999999046325684}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.41 sec\n",
      "Epoch: [6][0/25]\tTime 0.012 (0.012)\tData 0.006 (0.006)\tLoss 0.0434 (0.0434)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.969 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [7][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0221 (0.0221)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [8][0/25]\tTime 0.012 (0.012)\tData 0.006 (0.006)\tLoss 0.0285 (0.0285)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [9][0/25]\tTime 0.013 (0.013)\tData 0.007 (0.007)\tLoss 0.0129 (0.0129)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [10][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0126 (0.0126)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[10] test metrics:{\"loss\": 0.0742943023443222, \"error\": 0.018999999046325683}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.4 sec\n",
      "Epoch: [11][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0126 (0.0126)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [12][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0098 (0.0098)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [13][0/25]\tTime 0.015 (0.015)\tData 0.009 (0.009)\tLoss 0.0068 (0.0068)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [14][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0084 (0.0084)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [15][0/25]\tTime 0.012 (0.012)\tData 0.006 (0.006)\tLoss 0.0055 (0.0055)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[15] test metrics:{\"loss\": 0.07153536677360535, \"error\": 0.018999999046325683}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.4 sec\n",
      "Epoch: [16][0/25]\tTime 0.012 (0.012)\tData 0.006 (0.006)\tLoss 0.0069 (0.0069)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [17][0/25]\tTime 0.012 (0.012)\tData 0.006 (0.006)\tLoss 0.0046 (0.0046)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.31 sec\n",
      "Epoch: [18][0/25]\tTime 0.012 (0.012)\tData 0.006 (0.006)\tLoss 0.0055 (0.0055)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.32 sec\n",
      "Epoch: [19][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0058 (0.0058)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.31 sec\n",
      "Epoch: [20][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0051 (0.0051)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[20] test metrics:{\"loss\": 0.07155667555332183, \"error\": 0.018999999046325683}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.41 sec\n",
      "Epoch: [21][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0072 (0.0072)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.31 sec\n",
      "Epoch: [22][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0040 (0.0040)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.31 sec\n",
      "Epoch: [23][0/25]\tTime 0.014 (0.014)\tData 0.008 (0.008)\tLoss 0.0038 (0.0038)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [24][0/25]\tTime 0.012 (0.012)\tData 0.006 (0.006)\tLoss 0.0050 (0.0050)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [25][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0041 (0.0041)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[25] test metrics:{\"loss\": 0.07072802913188934, \"error\": 0.018999999046325683}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.4 sec\n",
      "Pure training time: 8.009999999999998 sec\n"
     ]
    }
   ],
   "source": [
    "%run main_merged.py --dataset lacuna10 --model allcnn --dataroot=data/lacuna10/ --filters 1.0 --lr 0.01 \\\n",
    "--resume checkpoints/lacuna100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 5e-4 --batch-size 128 --epochs 26 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: lacuna10_allcnn_1_0_forget_[5]_num_100_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in lacuna10_allcnn_1_0_forget_[5]_num_100_lr_0_01_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [1781 1858 1615 1665 1816 1663 1840 1828 1767 1756 1664 1703 1724 1915\n",
      " 1910 1660 1722 1871 1666 1626 1886 1758 1736 1909 1764 1800 1872 1607\n",
      " 1606 1830 1854 1622 1752 1621 1655 1757 1612 1708 1668 1744 1770 1878\n",
      " 1659 1734 1831 1692 1775 1867 1674 1681 1888 1859 1864 1652 1733 1776\n",
      " 1656 1690 1848 1617 1601 1608 1750 1806 1809 1740 1817 1794 1605 1633\n",
      " 1880 1634 1812 1815 1863 1737 1645 1701 1866 1726 1798 1838 1799 1822\n",
      " 1720 1845 1735 1673 1913 1904 1716 1855 1629 1697 1620 1646 1813 1862\n",
      " 1912 1689]\n",
      "Number of Classes: 10\n",
      "Epoch: [0][0/25]\tTime 0.136 (0.136)\tData 0.007 (0.007)\tLoss 1.7059 (1.7059)\tAcc@1 58.594 (58.594)\tAcc@5 86.719 (86.719)\n",
      " * Acc@1 86.688 Acc@5 98.000\n",
      "[0] test metrics:{\"loss\": 0.26762058663368227, \"error\": 0.04799999332427979}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.68 sec\n",
      "Epoch: [1][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.2866 (0.2866)\tAcc@1 94.531 (94.531)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 95.656 Acc@5 99.781\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [2][0/25]\tTime 0.012 (0.012)\tData 0.006 (0.006)\tLoss 0.1144 (0.1144)\tAcc@1 97.656 (97.656)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 97.906 Acc@5 99.906\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [3][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0788 (0.0788)\tAcc@1 99.219 (99.219)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.094 Acc@5 99.906\n",
      "Epoch Time: 0.29 sec\n",
      "Epoch: [4][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0235 (0.0235)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.562 Acc@5 99.906\n",
      "Epoch Time: 0.31 sec\n",
      "Epoch: [5][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0468 (0.0468)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.844 Acc@5 99.969\n",
      "[5] test metrics:{\"loss\": 0.08906168484687806, \"error\": 0.019999999046325684}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.39 sec\n",
      "Epoch: [6][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0412 (0.0412)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 99.969 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [7][0/25]\tTime 0.012 (0.012)\tData 0.006 (0.006)\tLoss 0.0220 (0.0220)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.31 sec\n",
      "Epoch: [8][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0278 (0.0278)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [9][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0137 (0.0137)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [10][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0124 (0.0124)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[10] test metrics:{\"loss\": 0.07493651735782624, \"error\": 0.019999999046325684}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.39 sec\n",
      "Epoch: [11][0/25]\tTime 0.012 (0.012)\tData 0.006 (0.006)\tLoss 0.0126 (0.0126)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.29 sec\n",
      "Epoch: [12][0/25]\tTime 0.011 (0.011)\tData 0.007 (0.007)\tLoss 0.0098 (0.0098)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [13][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0072 (0.0072)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.29 sec\n",
      "Epoch: [14][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0080 (0.0080)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [15][0/25]\tTime 0.013 (0.013)\tData 0.008 (0.008)\tLoss 0.0060 (0.0060)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[15] test metrics:{\"loss\": 0.07198784399032593, \"error\": 0.019999999046325684}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.4 sec\n",
      "Epoch: [16][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0072 (0.0072)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [17][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0041 (0.0041)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [18][0/25]\tTime 0.013 (0.013)\tData 0.008 (0.008)\tLoss 0.0050 (0.0050)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.31 sec\n",
      "Epoch: [19][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0059 (0.0059)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [20][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0046 (0.0046)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[20] test metrics:{\"loss\": 0.07289170265197754, \"error\": 0.019999999046325684}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.4 sec\n",
      "Epoch: [21][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0063 (0.0063)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.31 sec\n",
      "Epoch: [22][0/25]\tTime 0.011 (0.011)\tData 0.006 (0.006)\tLoss 0.0040 (0.0040)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [23][0/25]\tTime 0.012 (0.012)\tData 0.007 (0.007)\tLoss 0.0036 (0.0036)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.29 sec\n",
      "Epoch: [24][0/25]\tTime 0.011 (0.011)\tData 0.007 (0.007)\tLoss 0.0050 (0.0050)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "Epoch Time: 0.3 sec\n",
      "Epoch: [25][0/25]\tTime 0.013 (0.013)\tData 0.008 (0.008)\tLoss 0.0042 (0.0042)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\n",
      " * Acc@1 100.000 Acc@5 100.000\n",
      "[25] test metrics:{\"loss\": 0.07140162253379821, \"error\": 0.018999999046325683}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.4 sec\n",
      "Pure training time: 8.099999999999998 sec\n"
     ]
    }
   ],
   "source": [
    "%run main_merged.py --dataset lacuna10 --model allcnn --dataroot=data/lacuna10/ --filters 1 --lr 0.01 \\\n",
    "--resume checkpoints/lacuna100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 5e-4 --batch-size 128 --epochs 26 \\\n",
    "--forget-class 5 --num-to-forget 100 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 1620010\n"
     ]
    }
   ],
   "source": [
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model0 = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 4000#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model0.cuda()\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confuse mode: False\n",
      "split mode: train\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [1781 1858 1615 1665 1816 1663 1840 1828 1767 1756 1664 1703 1724 1915\n",
      " 1910 1660 1722 1871 1666 1626 1886 1758 1736 1909 1764 1800 1872 1607\n",
      " 1606 1830 1854 1622 1752 1621 1655 1757 1612 1708 1668 1744 1770 1878\n",
      " 1659 1734 1831 1692 1775 1867 1674 1681 1888 1859 1864 1652 1733 1776\n",
      " 1656 1690 1848 1617 1601 1608 1750 1806 1809 1740 1817 1794 1605 1633\n",
      " 1880 1634 1812 1815 1863 1737 1645 1701 1866 1726 1798 1838 1799 1822\n",
      " 1720 1845 1735 1673 1913 1904 1716 1855 1629 1697 1620 1646 1813 1862\n",
      " 1912 1689]\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _  = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=512, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=256, seed=seed, shuffle=True)\n",
    "\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "3100\n",
      "1000\n",
      "3200\n",
      "{0: 320, 1: 320, 2: 320, 3: 320, 4: 320, 5: 320, 6: 320, 7: 320, 8: 320, 9: 320}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'sgd'\n",
    "args.gamma = 0.9999\n",
    "args.alpha = 0.001\n",
    "args.beta = 0.9\n",
    "args.smoothing = 0.0\n",
    "args.msteps = 2\n",
    "args.clip = 0.2\n",
    "args.sstart = 4\n",
    "args.kd_T = 4\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_batch_size = 32\n",
    "args.del_batch_size = 8\n",
    "args.sgda_epochs = 5\n",
    "args.sgda_learning_rate = 0.0001\n",
    "args.lr_decay_epochs = [3,5,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> SCRUB unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: -13.29\t minimize loss: 0.00\t train_acc: 100.00000762939453\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: -13.67\t minimize loss: 0.00\t train_acc: 100.00000762939453\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: 0.00\t minimize loss: 0.00\t train_acc: 100.00000762939453\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: 0.00\t minimize loss: 0.00\t train_acc: 100.00000762939453\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 100.000 \n",
      "maximize loss: 0.00\t minimize loss: 0.00\t train_acc: 100.00000762939453\n"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "acc_vs = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> SCRUB unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "    acc_v, acc5_v, loss_v = validate(valid_loader_full, model_s, criterion_cls, args, True)\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "    acc_vs.append(100-acc_v.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\")\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    \n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n",
    "acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "acc_v, acc5_v, loss_v = validate(valid_loader_full, model_s, criterion_cls, args, True)\n",
    "acc_rs.append(100-acc_r.item())\n",
    "acc_fs.append(100-acc_f.item())\n",
    "acc_vs.append(100-acc_v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAHSCAYAAAAaFURQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB1C0lEQVR4nO3dd1xV9f8H8Ndl7ymgDBkOcM/EDY7cOxW15WpoaVppmZqYOSrNUfm1raUJuDItRyhucVtOXICIAgICMmTdz+8Pfpy4XsDDPIzX8/G4D+XM9zn3cu6Lcz7nc1RCCAEiIiIieiYdpQsgIiIiqi4YnIiIiIhkYnAiIiIikonBiYiIiEgmBiciIiIimRiciIiIiGRicCIiIiKSicGJiIiISCYGJyIiIiKZGJyIZIqIiIBKpYJKpUJERITS5VQbxe23su5TvieQtv/QoUOlmv/SpUsYPXo06tWrBz09PahUKrRu3bpcaySqSWpdcBJCYMuWLRg+fDhcXV1hbGwMMzMzNGjQAF27dsW7776LHTt2ICUlpdjlZGZm4qeffsKoUaPg4eEBc3NzGBoaol69eujVqxc+/fRThIeHa8136NAh6UBX8KWnpwdbW1t07twZn3zyCeLj44tcd8FlyDlY+vr6QqVSwdfXV2vc+PHjC61HpVLBzMwMzZo1w5QpU3D58uVnrkdJERER8Pf3h7+/v9KlEFUb4eHh6NKlC7Zs2YKYmBhYWlrCwcEBderUUbq0SrNq1Sr4+/vj4sWLSpdS7i5evAh/f3+sWrVK6VJqFlGLPHr0SPj4+AgA0ktPT0/Y2NgIPT09jeE///xzkcvZtWuXcHJy0pje0NBQWFlZCZVKJQ3T1dUVU6ZM0Zg3JCREGm9tbS0cHByEg4ODsLKy0lienZ2dOHfuXKHrL7iMkJCQZ253/jb7+PhojXv11VcFAKGjoyPV4uDgIOrUqaO1LT/88MMz16WUgvukoty7d094enoKT09Pce/evQpbT00THh4uvTfh4eEa48q6T4tbdm1RkmPB0z744AMBQDRs2FBERUWVf3HVgKur6zOP+dXVzz//LAAIV1dXpUupUWrVGadXXnkFhw8fhq6uLt577z3cuHEDmZmZSEhIQEZGBv755x989tlnaNWqVZHL+PbbbzF06FBER0fDxcUF33zzDe7evYsnT57g0aNHyMzMxJEjR/DWW29BT08Pv/32W5HL2r59O2JiYhATE4NHjx4hKSkJX375JQwMDPDw4UOMGjUK2dnZFbErtLi4uEi1xMTE4OHDh8jMzMSePXvg4eGB3NxcTJ06tdZeDgEAJycnXL9+HdevX4eTk5PS5dQI3KfKunTpEgBg6NChcHZ2Vrgaouqh1gSnmzdvYteuXQCATz/9FMuXL0ejRo2go5O3C/T09NCyZUvMnj0bFy9ehJ+fn9Yyjh8/jrfffhtqtRrdu3fHpUuXMHXqVLi4uEjT6Ovro1u3bvj6669x48YNdO3aVXaNlpaWmDlzJubNmwcAuHPnDkJCQsqy2WWir6+Pfv36YcOGDQCArKws7NmzR7F6iKh8paenAwDMzMwUroSo+qg1wang9euhQ4c+c3pjY2OtYe+99x5ycnJgb2+Pbdu2wdLSsthl1K9fH3/88UeJa+3Xr5/0/ytXrpR4/vJWsKFoampqiefPb0c1fvx4CCHwww8/oGvXrrC1tYVKpcL69es1po+JicGHH36IVq1awdLSEkZGRvDw8MDkyZNx9epVreW7ubmhR48e0s9Pt9UaP368NC47Oxt///03pk+fjvbt26NevXowMDCAvb09+vbti82bN0MIUeh2FNcQuWC7MwC4desWJk6cCBcXFxgaGsLZ2RmvvfYaoqOjS7z/ysP06dOhUqnQtm3bYqdLTU2FqakpVCoVNm7cKA0vy34rjpzG3dHR0XjjjTc09uWECRNw69atEq+vNM6fP49PPvkE3bt3h6urK4yMjGBlZYWOHTvis88+K/Z3omBbxMePH2PevHnw8vKCsbExbG1tMWjQIJw6darY9T969AizZs1CgwYNYGRkhHr16mHUqFE4d+5cqbfJzc1No43kwoULNX5nnm47GRMTg1mzZqFZs2YwMzODqakpmjVrhtmzZyM2NrbQdTz93t6+fRuvv/463N3dYWhoCDc3N43pIyMjMWnSJDg7O2u9z3I+J7m5uVi/fj369u0LBwcHGBgYwM7ODn379kVAQIDW59Pf3x8qlQqRkZEAgAkTJmgdO8pbRkYGli9fjk6dOsHa2hr6+vqws7ND06ZN8eqrr2Lbtm1Fznv79m1MmzYNTZo0gZmZGUxMTNCkSRPMmDEDd+/e1ZpepVJhwoQJAPL27dPbVpr2oMnJyVi8eDG8vb1hbW0NQ0NDuLi4YOzYsQgNDS10Hrmfg6ePoRcuXMCLL74IZ2dn6Ovra7XRrYzPZJGUvVJYeYKCgqS2APv37y/x/KdPn5bmX7RoUanrkNM+6dSpU9I0X3zxRamWUZCcNk7FXQM/duyYtL6dO3c+c31FreOVV14RI0eOlNpUWVtbCx0dHY22Bbt27RJmZmbS+vT19YWpqan0s4GBgdiwYYPG8tu3by+sra2laQq21XJwcBDTp0+Xpi247/D/bdMKrg+AGDVqlMjNzdXajuLa0xRc7sGDB6Vlmpuba7Sfc3R0VKR91JkzZ6QaLl++XOR069evFwCEmZmZSE1NlYZX1H57Vhulc+fOaby3xsbG0notLCxEYGBghbdxKriNOjo6Wu0RmzZtKmJjY4ud97fffhMNGzYUAISRkZEwMTHR+Izv3bu30PnDw8OlNjj5n38LCwvp/zt37ixVG6f27dsLBwcHoa+vLwAIU1NTjd+Z48ePS9MeOnRIY5tNTEw0fietra3F0aNHC609f5pNmzZJ71v+/AWPOSdOnBDm5uZFvs8Fj9+Fvc8xMTHC29tb432xtLTU+HnIkCEiMzNTmueLL74QDg4OQkdHR1rP08eO8pSSkiJatWol1aNSqYSVlZXG8aGo4/B3330nvVf5v3/GxsbSzxYWFlrfaw4ODtJn5ek2rA4ODoV+txQnNDRUODg4aLR7LfieqVQqsWTJEq355H4OCh5jtm7dKm2vhYWFMDIy0vj+qozPZHFqTXAKDw+XGju3aNFChIWFlWj+pUuXyvrieRY5oWfhwoXSNNu2bSvVMgoqbXDKysoS+/btkw74TZs2FdnZ2c9cX1HrMDMzE3p6emL58uUiOTlZCCHE48ePxf3794UQeYHRwMBAABBvvPGGuHbtmsjJyRFCCBEZGSmmTp0qgLwG/WfOnNFYh9zG4aGhoWLcuHHizz//FDExMUKtVgshhEhISBCrV6+WDjSrV6/WmlducLK2thZDhgwR165dE0IIkZmZKQIDA6WDzMsvv1zifVgemjZtKgCIDz74oMhpevXqJYXcgipqvxU3LiUlRdSvX18AEPXr1xf79++X1nvy5EnRrFkzjYNnRQWn3r17i59++klERkZKn//09HSxfft24enpKQCI4cOHFzpvwc9E06ZNxcGDB0Vubq5Qq9Xi9OnT0vyurq5aoTMnJ0e0b99emj8oKEha/5UrV0S3bt00tr80jcPzjw0LFiwodPzdu3eldTRt2lQcO3ZMGnfkyBGpfhsbG60/CAq+t2ZmZsLb21vj9zb/GPzo0SNRr149AUB4eHiIgwcPSu/z6dOnRatWrTTC89Pvc2ZmpnjuuecEANG2bVvx559/irS0NCGEEKmpqWLDhg3C3t5eABAzZszQ2sbKahy+aNEiaV9t27ZNPHnyRAghRG5uroiOjha//PKLeO2117Tm27FjhxSwP/zwQxERESHUarVQq9Xi+vXrYtSoUVLAiIyM1Ji3vBqHh4eHS5+DkSNHinPnzkmfxdjYWDF//nwpAO7YsUNrXjmfg4LHUDMzMzFgwADpGCqEEDdu3BBCVM5n8llqTXASQojXXntNIx23adNGTJ06Vfz444/i0qVL0i9rYV566SUp6Rf2V7VcxYWepKQksXLlSik82Nvbi4yMjBItozBlvavOzs5OvPHGGyIhIaFU25y/DgBizZo1RU6Xf/CbP39+kdNMnz5dABBDhw7VGF5ed9Vt2bJFABANGjTQGic3OPXo0aPQz8iaNWukv6ZLE0DLKj/8Ozs7F1rfvXv3pL++g4ODS7Ts0u634sZ99tlnAsg7s3L16lWt5T548KDYL9TKcO/ePWFoaChUKpXWl5YQ/wUnOzu7Qs9K/fvvv9I0Bb8AhBAaZ9MKez/S0tJEgwYNKjQ4vfnmm1Jwe/Dggdb4qKgoKTS/9dZbGuMKvreurq7i8ePHha4jP1AYGRmJmzdvao1/+PChqFOnTpHv89dffy0AiGbNmomUlJRC13H27FmhUqmEgYGB1vtQWcGpf//+AkChZ2WKkpmZKd3B/eOPPxY53ZAhQwQA8c4772gML6/glH+loLg/+r788ksBQLRq1UpjuNzPQcFjaIcOHaQ/mp9WGZ/JZ6lVwSk7O1vMnz9f45RewZe9vb2YOXOmiImJ0Zo3/0Nf1tO3crsjMDc3F4cOHXrmMsorOBX3MjIyEmPGjBHXr18v1Tbnr8Pa2lrjVHlBFy9elP6qSkpKKnJZZ8+eFUDepYWCv1jlFZzS09Ol5eSfCcsnNzj9/fffhS47MjJSmqawIFDRoqKiig1G+UGlqGBVnNLut+LGtWnTRgAQL774YpHrnTNnjqLBSQghOnToIACIzZs3a43Lr23u3LlFzu/u7i4AiLVr12oMHz58uAAgunTpUuS83377bYUFJ7VaLWxsbAQAMWfOnCKXMXv2bAFA2Nraagwv+N4Wd1moZcuWAoB49dVXi5xm/vz5Rb7PrVu3FgDEN998U+T8QgjRvHlzAUAEBARoDK+s4DR27FgBQEybNk32PL///rv0vVPcH/Zbt24VAISXl5fG8PIITgkJCdJx48qVK0VOFx8fL71HBb9D5X4OCh5Dt2zZUug0lfWZfJZa0zgcyLtz7pNPPkF0dDR+/fVXTJ48Ga1atYKBgQEAIC4uDitXrkTz5s1x+vRpjXnF/zcsLM8Gg48ePUJsbCxiY2ORlJQkDW/dujXCwsLg4+NTbut6FldXV4i8IC29UlNTceLECalxZceOHXHy5MlSr+O5556T9vXTjh07BgBQq9Xw9PRE3bp1C33lN5xPS0tDQkJCqep4/PgxvvjiC/j4+MDe3h4GBgZSg0ETExNputI25Pb29i50uKOjo/T/xMTEUi27LJydnaUGlr/++qvW+PxhL774onS3aUEVvd8KysrKkm6V79mzZ5HTFTeuvKjVavz2228YMmQI6tevD2NjY41GtvnHinv37hW5jKI+E8B/n4unPxNnz54FoNz2h4eHSzX17t27yOmef/55AEBCQkKhnf4CQJcuXQodnpWVJd0AU9zxrrDOe4G8z+S///4LAJg/f36Rx426desiLCwMAKTG4JVt0KBBAICvv/4aY8eOxe+//15sR8fAf8fFR48eoV69ekVu22uvvQagYrbt5MmTUKvVAPI+b0XV0KxZM2meouoo6nMgd7rK+EzKoVfqOasxS0tLvPTSS3jppZcAAE+ePMGxY8ewZs0a7Nq1C/Hx8XjhhRdw8+ZNGBkZAYDUk+6jR4+gVqsL/WIpqZCQEOmAkJiYiJMnT0rdIUyZMgXbtm2Drq5umddTWqampujUqRO2bduGTp064cyZM3j55Zdx48aNUm2/vb19kePu378PIO/OmKLuiHha/q3UJXHjxg306tVL40vOxMQEVlZW0jblrz8tLa3EywcAc3PzQofr6f3361awf67AwEC88847hc6zfft2dO7cuVR1FOaVV17BwYMHsW3bNqxdu1YKPBcvXpR6h3/llVe05quM/VZQYmIicnJyAKDY/p2K6nuovPZpeno6Bg0apNEtiIGBAWxsbKCvry/Vmp2dXex2F/WZAP77XDzdZ1tcXByA0m1/echff0lqiIuLg7u7u9Y0Rf3uJyYmIjc3F4DmHxZPK2r9MTEx0pe63D9GSnrciIqKwnPPPVfouPfffx/vv/++rOWMGzcOp0+fxldffYWAgAAEBAQAABo2bIg+ffpg4sSJaNeuncY8+cfFrKwsWcfFjIwMWbWURH4NAMp8bC7uO0DOdJXxmZSjVp1xKoqRkRF69+6NP/74A6+++iqAvL8e9+7dK02Tn6YzMzNx7dq1cq/BxsYGAwcOREhICBwcHLBz504sWrSo0GkLdpUg5xcl/0NcWBcLcujq6kq3td6+fVvrbFxJllOU/IOnl5eX1pmvol6ybx0tYMKECbh37x7c3NywZcsWJCQkIC0tDXFxcYiJidE4W5J/lrGiZWRkSGcen35lZWWV67peeOEFmJiYIDU1FTt27JCG559tateuHZo2bao1n5L7rTRnectrny5evBghISEwNjbGypUrERkZiSdPniAhIUHqLDb/bFJFfV6K2/6KuGW+LOsparqifvcL7rPi1lHUvs0/bgBAaGiorONGSW/Dz/9jrrBXSbtnWbVqFcLCwrBkyRL0798fVlZWuHXrFtauXYv27dtjxowZhW5fv379ZB8Xy1t+DcbGxrJrKOoModwTAXKmq6jPpBwMTk95/fXXpf/nn9oFgF69ekn/L/iFU97s7e2xdOlSAMCyZcsK7bOk4HOk5FwWyZ/Gzs6u1HW5urpK/y/q1GdZ1K1bF0Bep5/lccaiMFFRUThx4gQAYPPmzRg5ciRsbGw0pomJiamQdRcnv3+rkhyASsvMzAzDhw8H8F9Yys3NxebNmwEAL7/8stY8Suw3Gxsb6cBW3CWwoj7/5bVP888KfPzxx5gxYwbq16+vdSCuqM9M/l/ExW1/cePKa/1A3mdATg0lPcbY2tpK73PBMxtPK2qcg4OD9P/8S7vlzc3NrdxCGJB3hmnOnDn466+/kJCQgJMnT2LYsGEAgNWrV2v0/Zd/XKyobZMjv4aMjIxK6zutKJXxmZSDwekpBXvQNTQ0lP7/3HPPoUOHDgDyrlE/69p0vvzTyCXxyiuvoEGDBsjMzMTHH3+sNd7DwwPW1tYA/rsGXpTw8HDpoPP0aeCSKPhBNDU1LfVyipJ/vTkrK6tUwbTgpcOi/uoq+IvWpk2bQqcJDg4u8bqrm/xLccHBwYiJiUFwcDAePHgAPT09jB07Vmt6JfabgYEBWrZsCQDF9p5/8ODBcl3v0/K3vajtjoiIqLAvk/bt2wNQbvvd3d2lgHzgwIEip8t/721tbQu9JFIcAwMD6Wx+cQ8sL2qctbW1dIY0P+SWVP6xo7LOMD+97o4dO2Lr1q2oX78+AODvv/+WxucfF6Ojo595rC9q+UDZtq1z587SHwul3cflpTI+k3LUmuAUHh6OGzduPHO6/MeLANDqZXn58uXQ1dVFbGwsXnjhBSQnJxe7rHv37kl/SZSErq4uPvjgAwDApk2bcP36dY3xKpUKo0aNAgBs2bIFt2/fLnJZy5YtA5DXjiL/TENJCSGkMxJA2QJYUdq3by99Oc2dOxcPHz4sdvqn2zNYWFhI/y/Y0L6ggj29//PPP1rjHz9+jE8//VRuydVW79694ejoiNzcXGzatEk689SvX79Cr/srtd/yH3u0ZcsWjbO/+eLi4rBu3bpyX29B+dte2HYDwIcfflhh687f/mPHjhUaHDIyMvDFF19U2PpVKpVUw7ffflvombX79+/j22+/BYBCQ7ccI0eOBAAEBQXhzp07WuMTEhKKfZ/zrxIcOHDgmV/shbWDyj92FHXcKC+ZmZlFjtPV1ZVunCl4CWnw4MGoV68eAOCdd955Zvusoo6LZdk2e3t76WkbX3zxxTO/RyvyxpfK+kw+U6nvx6tmdu3aJXR0dMSAAQPEhg0bNG5pzcrKEufPnxfjx4/X6EeisFuyv/76a6l/o/r164u1a9dqPFU8KytLHD9+XLzzzjvC2NhYWFpaaswvtyuBgv13+Pn5aY2PjIyU+rBxcXERW7ZskTp9EyKvI6/JkydL63r//fcLXc+zeg6/c+eOxnIKq+VZ8tdR3O3GQuR1gGloaCgACHd3d61tunfvnvj1119F7969xeTJkzXmTUtLk/q/+vzzzwu9dVetVksdKjZr1kycPXtWGnfixAnRtm1bYWtrW+T7I7c7guLIee+LU7D7iLJ4//33BQDRpEkTqXuOwMDAQqetyP1W3Ljk5GTh7OwsAAg3NzcRHBwsva+nTp0SLVq0qPAOMPP7bzM3Nxfbtm2T+t+6c+eOGDt2rFCpVNLvYWG39Mt5v4vqEiA7O1u0bdtWAHmd+W3dulXqguPq1avCx8dHo3fsiujHKSoqStrHzZo10+hR/NixY6JJkyZSfcV1Nljce5OYmCj1SN2wYUNx6NAh6X0+c+aMaNOmTbH9dT158kTqNVxPT0/MnTtX3L17VxqflpYmQkJCxFtvvSWsrKy01v/iiy8KAKJz584iMTHxGXus9Fq1aiWmTZsmQkJCNHrlj46OFm+//ba0ffv27dOYb8eOHdJ3TuvWrcXevXs1unW5c+eOWLdunXjuuee0nmpx8+ZNablF/X7Lcfv2bel33M7OTvz4448a3cY8fPhQbNu2TQwfPlz06dNHY165nwO5x9DK+Ew+S60JTnv37pV2WP7LwMBA2NjYSB/K/Ffbtm1FdHR0kcv6/fffpZ5u819GRkbC2tpaY1l6enpaPdWWpA+mlStXCiCvs85///1Xa/zx48dF3bp1peXp6OgIGxsbjcc5ABCTJk0qssPFojrAdHBw0OhOH4Dw9fUtsoO54sgNTkIIsX//fo0vYV1dXWFra6u1TU8HJyGEmDRpkjTexMRE1K9fX7i6uor33ntPmmbXrl0ajzgwMTGRlm1iYiKCg4NrRXC6dOmSxv60tLQstLPVfBW13551IDtz5ozWoxUKPs6moh+5EhERofGYCT09PY2wsmTJkmLDR1mCkxB5X1guLi7ScgwNDaX1l+WRK3LWne/QoUMa22xqaqrRF56VlZU4cuSI1nwl+ZI6evSoxiN8Cr7PVlZWUgerAArt9PDhw4eiZ8+eGp9pCwsLYWVlpXVMftrhw4elaXR1dUW9evWEq6trmTuNfFrBR+fkP27l6T4FZ86cWei8Gzdu1DgG6unpCVtbW+kPzfzXp59+qjVv/tMA8n9n8rdt5cqVJar//Pnzws3NTWMbrK2ttR691Lt3b435yjs4CVE5n8ni1JrgJERe+l69erUYNWqUaNKkiTA3Nxc6OjrC1NRUNGrUSIwePVoEBATI6vwvIyNDfP/992LEiBHCzc1NmJqaCgMDA+Hg4CB69eolFi9erPFXT76SBKe0tDRhZ2cngKIf6ZCUlCSWL18ufH19hZ2dndDT0xNmZmaicePGYvz48YU+r6eg4jrANDQ0FM7OzmLo0KEiMDCw2A7Y5KxDTnASIu8RDEuXLhVdu3YVNjY2QldXV5iZmYmmTZuKSZMmiT/++KPQL/knT54If39/0bx5c42DzNPrPXHihBg4cKCwsrISBgYGon79+mLChAlSB59VOTj17t1bABDe3t6lmr+g/I4DART6qIenVcR+k3Mgu3v3rpg8ebJwcnISBgYGwsnJSbz66qvi5s2b5XYgLE5UVJSYNGmScHR0FHp6esLBwUEMGjRIOjNQkcFJiLwOCN99913h7u4uHWNGjhwpnfmr6OAkRF4v7e+9955o0qSJMDY2FiYmJqJJkybi/fffLzTICFHyL6k7d+6ICRMmCEdHR2FgYCCcnZ3FxIkTxZ07d8T58+elZRUV8NVqtdi5c6cYOXKkcHFxEYaGhtIxrH///uLrr78u8jmRf/31l+jdu7ewsbGROnss6x8nTzt58qRYuHCh6NWrl/Dw8BAmJibCwMBAuLq6Cj8/P3HgwIFi579//76YN2+eaN++vbCyshK6urrC0tJStG7dWrz99tsiODi40D+QHz16JGbOnCkaN24sjIyMpG171ntemPT0dPH111+L3r17S983JiYmolGjRmLcuHEiICBA64/righOQlTOZ7IoKiEUaBFHRCWWlZUFa2trpKenIzg4WONOT6Ka7Pvvv8frr78ODw+PYtt0ElWGWtM4nKi6Cw0NRXp6Onr27MnQRLXGkydPsGrVKgCQnhxApCQGJ6JqIv+29CVLlihcCVH5CggIwLx583D58mWpg9KcnBwcOXIEPXv2xNWrV2FkZFRkb/BElYmX6oiISFGrVq3CzJkzAeTdcm5tbY3U1FQpRBkYGGDDhg0YM2aMkmUSAailz6ojIqKqY9CgQXj48CEOHTqEyMhIxMfHQ19fHx4eHujRowdmzJiBxo0bK10mEQCecSIiIiKSjW2ciIiIiGTipboiqNVq3L9/H+bm5pX2BHIiIiIqGyEEHj9+DEdHR43nmJYXBqci3L9/Hy4uLkqXQURERKUQFRUFZ2fncl8ug1MRzM3NAeTt+IIPkCUiIqKqKyUlBS4uLtL3eHljcCpC/uU5CwsLBiciIqJqpqKa2bBxOBEREZFMDE5EREREMjE4EREREcnE4EREREQkE4MTERERkUwMTkREREQyMTgRERERycTgRERERCQTgxMRERGRTAxORERERDIxOBERERHJxOBERET0/2LSYnD6wWnEpMUoXQpVUXzILxEREYDtN7dj4YmFUEMNHZUOFnRagBGNRihdFlUxPONERES1XkxaDPxP+EMNNQBALdRYeHIhzzyRFgYnIiKq9bbf3A4BoTFMLdR44+83sOv2LmTmZipUGVU1KiGEePZktU9KSgosLS2RnJwMCwsLpcshIqIKIITAun/XYe3FtcVOZ2VoheGNhmNU41FwMXeppOqoNCr6+5vBqQgMTkRENVtGTgbmH5+PfRH7AAAd63XE6ZjTUIu8Nk7vtnsXWblZCLoRJF2yU0GFrk5dMcZrDLo4doGujq6Sm0CFYHBSCIMTEVHNFZMWg+kHp+Na4jXo6ehhfsf5GNFoBGLSYhD1OAou5i6oa1oXAJCjzsHRe0cRGBaI4/ePS8twMnPCqMajMLzRcNgY2Si1KfQUBieFMDgREdVM/z78F++EvIP4jHhYG1pjZY+VaOfQTta8kSmR2BK2BTtu7UBKVgoAQF9HH33d+sLP0w+t7FpBpVJVZPn0DAxOCmFwIiKqeXbf2Y0FxxcgS52FRtaN8FXPr+Bk5lTi5TzJeYK9EXsReD0QlxMuS8O9bLzg5+mHAe4DYKJvUp6lk0wMTgphcCIiqjnUQo0159fgx8s/AgB8XXyxrNsymOqblnnZl+MvIzAsEHvC90h335npm2FIgyHw8/KDh6VHmddB8jE4KYTBiYioZkjLTsOHRz/EoahDAIDJLSZjWptp0FGVb488yZnJ+P3W7wgKC8Ldx3el4d51veHn5QdfF1/o6+iX6zpJG4OTQhiciIiqv+jUaEw7OA03H92EgY4BFnZZiEEegyp0nWqhRuj9UASEBeDwvcNQi7xONe2N7fFC4xfwQqMX4GDqUKE11GYMTgphcCIiqt7OxZ7DzJCZeJT5CHWM62B1j9VoadeyUmt4kPoAW25swbab25D4JBEAoKvSRc/6PeHn6YcOdTuwMXk5Y3BSCIMTEVH1tf3mdiwKXYQcdQ6a2jbF6h6rpe4FlJCdm43gu8EIuB6A83HnpeHulu7w8/TD4AaDYWHA75rywOCkEAYnIqLqJ0edgxVnV2DjtY0AgL5ufbGoyyIY6xkrXNl/bjy6gaCwIOy6vQvpOekAAGM9YwxwH4AxXmPgZeOlcIXVG4OTQhiciIiql5SsFMw+PFvqpPKt1m/hjZZvVNlLYalZqdh9ZzcCwwJxK+mWNLyVXSv4efqhj1sfGOoaKlhh9cTgpBAGJyKi6iMyJRJvH3gbESkRMNI1wuKui9HHrY/SZckihMC52HMIDAtEcGQwckQOAMDa0Fp6Pp6zubPCVVYfDE4KYXAiIqoeTt4/ifcPv4+UrBTUNa2LNT3WoIltE6XLKpX4jHhsu7ENW25sQWx6LIC85+N1c+4GP08/Ph9PBgYnhTA4ERFVfZuvb8Znpz9DrshFS7uWWN1jNeoY11G6rDLLUefgyL0jCLgegJMPTkrDncycMNpzNIY3HA5rI2sFK6y6GJwUwuBERFR1ZauzsezUMgTdCAIADGkwBB93+rhGtgmKTIlEUFgQdtzagcdZjwEABjoGec/H8/JDyzotq2w7LiUwOCmEwYmIqGpKepKEdw+/izMxZ6CCCjPbzcT4ZuNrfHjIyMnA3vC9CAgLwNWEq9LwJjZN4Ofph/7u/fl8PDA4KYbBiYio6rmddBtvH3gb91LvwUTPBJ93/xw+Lj5Kl1XpLsdfRsD1AOwJ34MsdRYAwFzfHEMbDsVoz9Fwt3RXuELlMDgphMGJiKhqOXLvCGYfmY207DQ4mTnhq55foZF1I6XLUlTSkyTsvL0TgWGBiHocJQ33rueNMZ5j4OviCz0dPQUrrHwMTgphcCIiqhqEEPjl6i9YcXYFBATaO7THl75fsnF0AWqhxsn7JxEQFoAj947893w8E3uMbDwSLzR6AfYm9gpXWTkYnBTC4EREpLys3Cx8cvIT7Ly9EwDwQqMXMNd7LvR19RWurOq6n3ofW29s1Xg+np5KDz3r98QYrzFo79C+RrcHY3BSCIMTEZGy4jPiMTNkJi4+vAgdlQ5mPzcb47zG1egv/fKUlZuF4MhgBIYFajwfz8PSA6M9R2NIgyEwNzBXsMKKweCkEAYnIiLlhCWGYdrBaXiQ9gDm+uZY7rMcnZ06K11WtRWWGJb3fLw7u5CRkwEg7/l4Az0GYoznGHjaeCpcYflhcFIIgxMRkTIORB7AnGNzkJGTATcLN6zpuaZW3yVWnlKzUrHrzi4EXg/E7eTb0vA29m3g5+mH512fh4GugYIVlh2Dk0IYnIiIKpcQAt9f+h5fXfgKANDZsTM+7/45LA0tFa6s5hFC4GzsWQSGBeJA5AHp+Xg2RjYY3nA4RnmOgpOZk8JVlg6Dk0IYnIiIKs+TnCf4+PjH2BOxBwDwYpMX8X7792vdrfRKeJj+ENtu5j0fLy49DkDe8/G6O3fPez6eUxfoqHQUrlI+BieFMDgREVWO2LRYvBPyDq4kXIGeSg8fdfwIoxqPUrqsWidHnYPDUYcREBaA0Aeh0nBnM2eM9hyNYQ2HVYsuIBicFMLgRERU8S7HX8b0g9PxMOMhrAyt8KXvl3iu7nNKl1XrhSeHIygsCDtv7cTj7P+ej9fPvR/8PP3Qok6LKnt3I4OTQhiciIgq1l93/sLHJz5GZm4mGlo1xJqea+Bi7qJ0WVRAenY69kbsRcD1AFxLvCYNb2LTBGO8xqC/e38Y6xkrWKE2BieFMDgREVUMtVDjm4vf4Lt/vwMA+Dj7YFm3ZTAzMFO4MiqKEAKX4i8hMCwQe8P3/vd8PANzDG0wFH6efnCzdFO2yP9X0d/fVa6118GDBzFx4kR4eXnB1NQUTk5OGDp0KM6dOydr/ri4OIwfPx516tSBiYkJOnXqhAMHDlRw1UREJEd6djrePfSuFJomNp+I1T1WMzRVcSqVCi3tWmJx18UIHhWMd9u9CyczJzzOeoyN1zZi8O+D8dr+1/Lu0FPnKF1uhapyZ5xGjRqFhIQEjBo1Ck2bNsXDhw+xYsUKnD17Fvv27UPPnj2LnDczMxPt27dHUlISli1bBnt7e3zzzTf4888/ERwcDB8f+U/Q5hknIqLydT/1PqYdnIYbj25AX0cfCzsvxOAGg5Uui0pJLdQ4Hn0cgWGBOHLvCATy4oS9iT1GNR6FFxq9ADsTu0qvq9ZdqouLi4O9veaDCFNTU9GwYUM0b94cwcHBRc67du1avPXWWzhx4gQ6deoEAMjJyUGrVq1gZmaGU6dOya6DwYmIqPxciLuAGSEzkPgkEbZGtljVYxVa27dWuiwqJ9Gp0dgStgXbb27Ho8xHAPKej9fLtRf8PP0q9fl4tS44FaVnz56Ijo5GWFhYkdM8//zziIqKwvXr1zWGL126FB999BHu3bsHJyd5HXoxOBHJE5MWg7spd1Hfoj7qmtZVuhyqgn6/9TsWnlyIHHUOmtg0wZqea/hZqaGycrOwP3I/gsKCcCHugjS8gWUD+Hn5YbDHYJgZmFXocaOiv7+rRc9iycnJOH/+fLGX6QDg8uXL6Natm9bwli1bAgCuXLkiOzgR0bNtv7kdC08uhFqooaPSwYJOCzCi0Qily6IqIledi5XnVmLD1Q0AgOddn8enXT6Fib6JwpVRRTHQNcAgj0EY5DEIYYlhCAwLxO47u3E7+TaWnFqCledWopltM5yLPQcBUS2PG1WucXhh3nrrLaSlpWHu3LnFTpeQkAAbGxut4fnDEhISipw3MzMTKSkpGi8iKlpESgT8T/hDLdQA8to7LDy5EDFpMQpXRlXB46zHmHZwmhSaprSaguU+yxmaahFPG0983OljHBh1AHM6zIGHpQcycjJwNvas1B6qOh43qnxwmj9/PjZt2oSVK1eiXbt2z5y+uGuoxY1bunQpLC0tpZeLC/sSISpMRHIEPj/zOUbvGi0d/PKphRq77+xGrjpXoeqoKribchcv/fUSjkYfhZGuEb7w+QJTW0+tVo/toPJjbmCOcU3G4fehv2NW+1la49VCjajHUQpUVjpV+lO8cOFCfPrpp1i8eDHefvvtZ05va2tb6FmlxMREACj0bFS+OXPmIDk5WXpFRVWfN5GoouWoc3Dg7gG8vv91DP59MH69+isycjIKnXb1+dUYuGMgfrz0IxKfJFZypaS00w9OY9xf43An+Q7sTeyxvv969HPrp3RZVAWoVCr0ceujFaB1VDrVquPTKtvGaeHChfD394e/vz8++ugjWfO0aNECly5d0hqeP6x58+ZFzmtoaAhDQ8PSFUtUQ8VnxGPbjbyHf8amxwLQfPhnbHosFoUukto4dazbEZcTLiM6NRqrzq/CNxe/QR+3PhjjOQat7FpV2Uc0UPkICgvC0lNLkSNy0KJOC6zusVqR29Gp6qprWhcLOi3QahtZnW4WqJJ31S1atAgff/wx5s2bh0WLFsme73//+x+mTp2K0NBQeHt7A8jrjqB169YwMzNDaGjoM5bwH95VR7WVEALnYs8hMCwQwZHByBF5ndlZG1pjRKMRGOU5Ck5m/91kEZMWg6jHUXAxd0Fd07p4kvMEeyP2IvB6IC4nXJam87T2hJ+XHwa6D2Q7lxomW52Nz09/joCwAADAQI+BWNh5IQx1+ccoFe7p40Z5qnXdEaxYsQLvv/8++vXrhwULFmiN79ixIwBg0qRJ2LBhA27fvg1XV1cAeQ2827Vrh5SUFKkDzLVr12LXrl3sAJPoGVKzUrH7zm4EhgXiVtItaXhru9bw8/JDH9c+MNA1KNEyL8dfRmBYIPaE70FmbiYAwEzfDEMaDIGfpx88rDzKdRuo8iVnJuO9w+/h1INTUEGF6W2nY1LzSTy7SIqpdcHJ19cXhw8fLnJ8frnjx4/Hhg0bEB4eDjc3N2l8bGwsZs+ejd27dyM9PR2tW7fGokWL0Lt37xLVweBEtcWNRzcQFBaEXbd3IT0nHQBgrGeMgR4D4efpBy8brzKvIzkzGb/f+h1BYUG4+/iuNLxD3Q7w8/RDj/o9oK+jX+b1UOW6k3QH0w5Ow93Hd2GsZ4xl3ZahZ/3iu40hqmi1LjhVFQxOVJNl52bj78i/ERgWiPNx56Xh7pbu8PP0w5AGQ2BuYF7u61ULNULvhyIgLACH7x2WujKwM7bDyMYj8UKjF+Bg6lDu66Xydyz6GGYdnoXU7FQ4mjpiTc818LTxVLosIgYnpTA4UU30IPUBttzYgm03t0l3vOmqdNGzfk+M8RyD5+o+V2mXWIqrxc/TDx3qduDlnipICIGN1zZi+dnlUAs12tq3xcoeK2FjVPRdy0SVicFJIQxOVFOohRon759EQFgAjtw7Ip3lsTe2zzvL0/gF2JvYP2MpFSc7NxvBd4MRcD2g0LNfgxsMhoUBfwergqzcLHwa+il23NoBABjRaATmec+Dvi4vs1LVweCkEAYnqu7y2xUFhgVqdC7nXdcbfl5+8HXxrXLtim4+uonAsECt9lYD3AdgjNeYcmlvRaWTkJGAdw+9i/Nx56Gj0sGs9rPwYpMXeVaQqhwGJ4UwOFF1dTn+MgKuB2BvxF6NO9mGNhyK0Y1HV4s72dKy07D79m4EhAVo3OHXyq4V/Dz90MetD291r0RhiWGYfnA67qfdh7m+Ob7w+QJdnLooXRZRoRicFMLgRNVJRk4G9obvRWBYIK4kXJGGe9l4wc/TDwPcB1TLvpOEEDgfdx6B1wPxd+TfGn1KDW80HKMaj4KzubPCVdZsB+8exIdHP0RGTgbqm9fHV72+godl1Q/fVHsxOCmEwYmqg8iUSASFBeH3W78jJSvvwdT6Ovro69YXfp5+Naq37viMeGy/uR1BYUEavZh3c+4GP08/dHHsAl0dXYWrrDmEEPjx8o9Yc34NBAS863ljhc8KWBpaKl0aUbEYnBTC4ERVVY46B0fuHUFgWCBO3D8hDXcyc8Joz9EY1nBYjb7D6VnbP7zhcFgbWStYYfX3JOcJFpxYgL/C/wIAjPUai1nPzapybeKICsPgpBAGJ6pq8s+4bLmxBTFpMQDyzrh0deqKMV5jauUZl/wzbjtu7cDjrMcAAAMdg7wzbl5+aFmnZY0541ZZHqY/xDsh7+BS/CXoqfQwx3sORnuOVrosItkYnBTC4ERVgUYbn7t/I0ed18bHytAq77lxbOMD4L82XgFhAbiacFUa3sSmCfw8/dDfvX+1bONV2a4kXMH0g9MRlx4HS0NLfOnzJTrU66B0WUQlwuCkEAYnUhLvKiu9wu4qNNc3z7ur0HM03C3dFa6watobsRfzj83Hk9wn8LD0wNc9v4aLhYvSZRGVGIOTQhicSAnF9WPk5+mHJrZNFK6w+kh6koSdt3dq92NVzxtjPMfA18UXejp6ClZYNaiFGv/7539Y9886AEA3p274rPtnFfLIHaLKwOCkEAYnqizZudk4cPcAAsICcC72nDTczcINY7zGsOfsMiq253TPvOfjKdlzupLSs9Mx7/g8/B35NwBgfLPxmNF2Rq1rK0c1C4OTQhicqKLFpMXkPavtxjYkPEkAwGe1VbT7qfex9cZWjefj6an08p7V5zUG7R3a15p9/iD1AaaHTMf1xOvQ19HHx50+xrCGw5Qui6jMGJwUwuBEFUEt1Ah9EIrA64E4dO+QdPbDztgu77lxjV6Ag6mDwlXWfFm5WQiODEZgWKDG8/E8LD0w2nM0hjQYUqMvVV2Mu4gZITOQ8CQBNkY2WNVjFdrYt1G6LKJyweCkEAYnKk/JmcnYeWsngm4EITIlUhreoW4H+Hn6oUf9HuwjRyFhiWEICgvCrju7kJGTASCvXdlAj4EY4zkGnjaeCldYvv64/Qf8T/gjW50NT2tPrOm5Bo5mjkqXRVRuGJwUwuBE5eFKwhUEXA/AnvA9Gs+NG9JgCEZ7jkYDqwYKV0j5UrNSsevOLgReD8Tt5NvS8Db2beDn6YfnXZ+Hga6BghWWTa46F6svrMbPl38GAPSq3wtLui5hNw1U4zA4KYTBiUrrSc4T7IvYh8CwQFyKvyQNb2zdGGO8xmCg+0B+WVVhQgicjT2LwLBAHIg8ID0fz8bIBsMbDscoz1FwMnNSuMqSSc1KxQdHP8CRe0cAAG+0fANTW0+FjkpH4cqIyh+Dk0IYnKik7qbczXtu3O3fkZyZDCDvuXF93PpgjOeYGvXcuNriYfpDbLu5DVtubEFcehyAvN7auzt3z3s+nlOXKh8+oh5HYdqBabidfBuGuoZY1GUR+rv3V7osogrD4KQQBieSI1edKz037fj949JwR1NHjPIcheENh8PW2FbBCqk85KhzcDjqMALCAhD6IFQa7mzmLD0fsCo+H+9MzBm8e+hdJGUmwc7YDmt6rkHzOs2VLouoQjE4KYTBiYoTnxGPHTd3YMuNLXiQ9gBA3pmILk5dMMZzDLo6dWVfODVUeHI4gsKCsPPWTjzO/u/5eP3c+8HP0w8t6rSoEmcWt9zYgiWhS5AjctDctjlW91xda/urotqFwUkhDE70NCEELsRdQEBYAP6O/O+5cZaGlhjRMO+5cXxERe2RkZOBPeF7EHA9ANcSr0nDm9g0wRivMejv3h/GesaVXleOOgdfnPkCv13/DQDQ370/Pun8CYz0jCq9FiIlMDgphMGJ8qVlp+HPO38iICwANx/dlIa3rNMSfl5+6OvWl8+Nq8WEELgUfwmBYYHYG74XWeosAIC5gTmGNhgKP08/uFm6VUotyZnJmHV4Fk4+OAkAmN5mOia3mFwlzoARVRYGJ4UwONGtR7fynht3ZxfSstMAAEa6RhjoMRCjPUejqW1ThSukqubRk0f4/dbvCAoLwr3Ue9LwjvU6YoznGPi4+FTY8/HCk8Mx7eA0RKZEwljPGEu7LkUv114Vsi6iqozBSSEMTrVTdm42DkQdQOD1QJyNPSsNd7Nwg5+nHwY3GAxLQ0sFK6TqQC3UOHH/BAKvB+LwvcMQyDvM2pvYY1TjUXih0QuwM7Ert/WdiD6B9w+/j8fZj1HPtB6+6vlVjeu4k0guBieFMDjVLjFpMdIzzOIz4gEAOiod9HDpgTFeY+Bd15uXO6hUolOjsfXGVmy/uV3j+Xi9XHvBz9OvTM/HE0Lgt+u/4fMzn0Mt1Ghj3wYrfVfyTk6q1RicFMLgVPOphRqnHpxCYFggDkUdQq7IBQDUMa4jPTeurmldZYukGiMrNwt/R/6NwLBAXIi7IA1vYNkAfl5+GOwxGGYGZrKXl52bjcWnFmPbzW0AgGENh2F+x/nVundzovLA4KQQBqeaKzkzGX/c/gNBYUGISImQhj9X9zn4efqhZ/2efG4cVaiwxDAEhgVi953dGs/HG+wxGKM9Rz/zMtujJ48w89BMnIs9Bx2VDt5t9y5eafoKz4oSgcFJMQxONc/VhKsIDAvEX3f+wpPcJwAAU33TvOfGNR6NhtYNFa6QapvHWY+x6/YuBIYF4k7yHWl4W/u28PP0Q2/X3lpnkG4+uolpB6chOjUaZvpm+Kz7Z+ju3L2ySyeqshicFFJROz4mLQZ3U+6ivkV9XgaqYDFpMbiddBt3ku9gb/he/Bv/rzSukXUjjPEcg0Eeg/jcOFJc/vPxAq4H4ODdgxrPx3uh0QsY1XgUVCoV/rj9B77/93s8yX0CF3MXfNXzKz4omugpDE4KqYgdv/3mdvif8IeAgAoqPO/6PFrUaVEuyyZNl+IvYX/kfo1hejp66OPaB2O8xqC1XWte1qAqKS49DttubsPWsK2Iy/jv+Xj5d+YBgLuFO37p/wusjKwUqpKo6mJwUkh57/iYtBj03dYXaqEuh+qopFRQIXBQIJrYNlG6FCJZstXZOBx1GBuubsDFuIsa43RUOtj3wj6etSYqREUHp4rpiY203E25W2ho6uzYGXWM6yhQUc0VnxGPE/dPaAwTEEjNTlWoIqKS09fRR2/X3rAwsMCk/ZM0xqmFGlGPoxiciBTA4FRJ6lvUh45KRyM86ah0sLDzQh78yllhZ/d0VDpwMedz5Kj6KerYwc8zkTJ0lC6gtqhrWhcLOi2Ajipvl+uodLCg0wKGpgrAfU01CT/PRFUL2zgVoSLvqot6HAUXcxce+CoY9zXVJPw8E8nDxuEKYT9ORERE1U9Ff3/zUh0RERGRTAxORERERDIxOBERERHJxOBEREREJBODExEREZFMDE5EREREMjE4EREREcnE4EREREQkE4MTERERkUwMTkREREQyMTgRERERycTgRERERCQTgxMRERGRTAxORERERDIxOBERERHJxOBEREREJBODExEREZFMDE5EREREMjE4EREREcnE4EREREQkE4MTERERkUwMTkREREQyMTgRERERycTgRERERCQTgxMRERGRTAxORERERDJVueD0+PFjzJ49G3369IGdnR1UKhX8/f1lzbt+/XqoVKpCXzExMRVbOBEREdV4ekoX8LSEhAR89913aNWqFYYNG4YffvihxMv4+eef4eXlpTHM1ta2vEokIiKiWqrKBSdXV1c8evQIKpUK8fHxpQpOzZs3R/v27SugOiIiIqrNqlxwUqlUSpdAREREVKgq18apPAwaNAi6urqwsbHBiBEjcPnyZaVLIiIiohqgyp1xKou6deti7ty56NixIywsLHDp0iUsW7YMHTt2xPHjx9GqVasi583MzERmZqb0c0pKSmWUTERERNVIjQpO/fr1Q79+/aSfu3fvjoEDB6JFixb4+OOPsXPnziLnXbp0KRYuXFgZZRIREVE1VSMv1RXk5uaGrl27IjQ0tNjp5syZg+TkZOkVFRVVSRUSERFRdVGjzjgVRQgBHZ3iM6KhoSEMDQ0rqSIiIiKqjmr8Gafw8HAcP34cHTt2VLoUIiIiquaq5BmnPXv2IC0tDY8fPwYAXL16FVu3bgUADBgwACYmJpg0aRI2bNiA27dvw9XVFQDQu3dvdO/eHS1btpQah3/++edQqVRYtGiRYttDRERENUOVDE5TpkxBZGSk9POWLVuwZcsWAHlnkNzc3JCbm4vc3FwIIaTpWrRogcDAQCxfvhwZGRmwt7dHz549MX/+fDRu3LjSt4OIiIhqFpUomDxIkpKSAktLSyQnJ8PCwkLpcoiIiEiGiv7+rvFtnIiIiIjKC4MTERERkUxVso0TERFVHbm5ucjOzla6DKqldHV1oa+vr3QZEgYnIiIqlBACMTExSE5OBpvDkpIMDQ1Rp06dKtHmmMGJiIgKlZycjKSkJNjZ2cHU1BQqlUrpkqiWEUIgOzsbycnJiI6OBgDFwxODExERaRFCIC4uDhYWFqhTp47S5VAtZmxsDHNzc9y7dw/x8fGKByc2DiciIi35feUp/SVFBAAqlQqWlpbIzMxUvL0dgxMREWnJyckBAOjp8cIEVQ35DcRzc3MVrYPBiYiIisR2TVRVVJXPIoMTERERkUwMTkREREQyMTgRERFVM25ubnBzc1O6jFqJwYmIiKiC+fv7Q6VS4dChQ0qXojiVSgVfX1+lyyg13i5BRERUzRw4cEDpEmotBiciIqJqpkGDBkqXUGvxUh0RESnq33tJGPtdKP69l6R0KRoOHToElUoFf39/nDx5En379oWVlZV0W7wQAj/99BO6dOkCCwsLmJiYoH379vjpp580luPr64uFCxcCAHr06AGVSgWVSqXRRikkJAQTJ06Ep6cnzMzMYGZmhvbt2+O7774rtLbC2jgVvBwYFBSEtm3bwtjYGPXq1cP06dORkZFRou0PCQlB//794ejoCENDQzg6OsLX1xc//PCD1rTh4eGYPHky6tevD0NDQ9SrVw/jx49HZGSk1v4EgMOHD0v7QaVSYf369SWqTUk840RERIrafj4aJ+8kYPv5aLR0tlK6HC0nTpzAkiVL0KNHD7z++uu4e/cuhBB46aWX8Ntvv6Fx48YYN24cDAwM8Pfff2PSpEm4evUqli9fDgAYP348gLyw8Oqrr0qBx8rKSlrHZ599hlu3bqFjx44YPnw4kpKSsHfvXrzxxhsICwvDihUrZNf7zTffYM+ePRg6dCh8fX2xd+9efPXVV0hISMCmTZtkLePPP//E4MGDYWVlhaFDh6JevXp4+PAhLl68iE2bNmHy5MnStKdOnULfvn2RlpaGwYMHo2HDhoiIiMCmTZuwZ88enDx5Eh4eHnBzc8OCBQuwcOFCuLq6SvsFAFq3bi17+xQnqFDJyckCgEhOTla6FCKiSpeRkSGuXr0qMjIyNIar1WqRlpld5teN2BRxOjxenAlPEG0+2S9cP9gt2nyyX5wJTxCnw+PFjdiUMi1frVaXeR+EhIQIAAKA+PHHHzXGfffddwKAmDRpksjOzpaGZ2ZmisGDBwsA4uzZs9LwBQsWCAAiJCSk0HXduXNHa1h2drZ4/vnnha6uroiMjNQY5+rqKlxdXTWG5a/D0tJSXL9+XRqenp4uGjduLFQqlYiOjpa17SNGjBAAxD///KM1Lj4+Xvp/VlaWcHNzE+bm5uLixYsa0x09elTo6uqKQYMGaQwHIHx8fGTVUVBRn8mnVfT3N884ERGRbBnZuWj68b4KWXZiWhZGrjtZLsu6+klfmBiUz1dcmzZtMHHiRI1hX3/9NUxNTfH1119rPJbGwMAAixcvxq5du7B582a0a9dO1jrc3d21hunp6eHNN9/E33//jZCQELz66quylvXOO+/A09NT+tnY2Bhjx47FwoULce7cOTg6OspaTv68T7O1tZX+v3v3bkRERGDRokVo1aqVxnRdu3bF0KFD8fvvvyMlJaXGPPeQwYmIiKgYHTp00Pg5PT0dly5dgqOjI5YtW6Y1ff5DaK9fvy57HY8fP8by5cvx+++/4/bt20hLS9MYf//+fdnLatu2rdYwZ2dnAEBSUpI0zN/fX2u6GTNmwMrKCqNHj8b27dvh7e2NsWPHomfPnujWrRvs7e01pg8NDQWQt62FLS8mJgZqtRo3btxA+/btZW9DVcbgREREshnr6+LqJ33LZVlX76cUeoZp65ud0NSxbGcnjPV1yzR/QQ4ODho/P3r0CEIIREdHS42+C/N0+ClKVlYWfH19cf78ebRp0wYvv/wybG1toaenh4iICGzYsAGZmZmy67W0tNQaln9WrOADcgurffz48bCysoKfnx/09fWxatUqfPvtt1i7dq3U/9KXX34ptUlKTEwEgGe2nZK7L6oDBiciIpJNpVKV2yUwo/8PNyoVIMR//xrp65bbOsrD0w+Xzb/k1K5dO5w9e7bMy9+5cyfOnz+PyZMn4/vvv9cYFxAQgA0bNpR5HYURQhQ7fsSIERgxYgRSUlJw4sQJbN++HT/++CP69u2LsLAwWFlZSfti165dGDRoUIXUWdWwOwIiIlKErZkB7MwM0cLJEouHN0cLJ0vYmRnC1sxA6dKKZW5ujiZNmuDatWsal76Ko6ubFxILnvHJd/v2bQDAkCFDtMYdPXq09IWWEwsLC/Tr1w/fffcdxo8fj7i4OJw6dQoA4O3tDQA4eVJ+2zQdHZ1C90N1weBERESKqGdpjGMf9sDOt7rgRW9X7HyrC4592AP1LLUbJFc106dPR3p6Ol577bVCL0OFh4cjIiJC+tnGxgYAcO/ePa1pXV1dAQDHjh3TGH748GGtM1CV5cCBA3jy5InW8Li4OAD/NRofOnQo6tevjy+//BJHjhzRmj47O1tru2xsbArdD9VF1TkXSkREtY6h3n9tkVQqlcbPVdkbb7yB0NBQbNiwAcePH0fv3r3h6OiI2NhYXL9+HadOncJvv/0m9dmU3/Hl3Llzcf36dVhaWsLS0hJTpkzB4MGD4ebmhs8//xyXL19G8+bNERYWht27d2PYsGHYtm1bpW/fe++9h7t378LX1xdubm5QqVQ4duwYTp8+jc6dO6NLly4AAENDQ2zduhX9+/eHj48PevXqhebNmwMA7t69i6NHj8LW1lajoXzPnj0RFBSEkSNHok2bNtDV1cXAgQPRokWLSt/O0mBwIiIiKqH83q4HDBiA77//Hrt370Zqairs7e3RqFEjLF++HL1795amb9q0KX7++WesWLECK1euRGZmJlxdXTFlyhSYmZnh4MGDmDVrFo4cOYJDhw6hWbNm2LRpExwcHBQJTnPmzMH27dtx7tw57Nu3D/r6+nB3d8fnn3+OqVOnSpceAeC5557DP//8gy+++AJ//fUXjh07BkNDQzg5OWHYsGEYO3asxrJXr14NADh48CB27NgBtVqNunXrVpvgpBLPah1WS6WkpMDS0hLJyck1pu8JIiK5njx5gvDwcLi7u8PIyEjpcohkfyYr+vubbZyIiIiIZCp1cPrkk0+wcePG8qyFiIiIqEordXD69NNPcenSpfKshYiIiKhKK3VwcnV1lXoMJSIiIqoNSh2cxo4di3379iE5Obk86yEiIiKqskodnObNm4eWLVuiZ8+e+PPPP6VOsYiIiIhqqlL345Tfa6gQotBu4vOpVCrk5OSUdjVEREREVUapg1O3bt20HnxIREREVJOVOjgdOnSoHMsgIiIiqvrYASYRERGRTOXyrLro6Gj8888/UvfmrVu3hpOTU3ksmoiIiKjKKFNwunPnDt58800cOHBAa1yvXr2wdu1aNGzYsCyrICIiIqoySh2c7t27hy5duiA2NhZNmjRB9+7dUbduXcTGxuLo0aMIDg5Gt27dcPr0abi4uJRnzURERESKKHUbJ39/f8TGxuK7777DlStX8L///Q8LFizA2rVrcenSJXz//feIi4vDJ598Up71EhERVYqsrCzMmzcPDRo0gIGBAVQqFW+MotIHp3379mHIkCGYPHlyoeMnTZqEwYMHY8+ePaUujoiISCnLly/H4sWLUb9+fcyePRsLFiyAm5ub0mWVyKFDh6BSqeDv7690KZL169dDpVJh/fr1SpdSKqW+VBcXF4dmzZoVO02zZs0YnIiIqFr666+/YGZmhv3790NfX1/pcqiKKHVwsrOzw5UrV4qd5urVq7CzsyvtKoiIqKZT5wKRJ4DUWMDMAXDtDOjoKl0VAOD+/fuwtbVlaCINpb5U17dvX+zatQs//vhjoeN/+ukn7Nq1C/369St1cUREVINd/QNY1RzYMAjYNinv31XN84YryN/fHyqVCuHh4YiMjIRKpYJKpYKvry8AICcnBytXrkSrVq1gbGwMS0tL9OjRA3/++afWsgpelvrzzz/RrVs3mJuba1zyi4iIgJ+fH2xsbGBmZgYfHx8cOXJEqqOwdlVHjhzB4MGDUadOHRgaGqJRo0aYN28e0tPTNbajR48eAICFCxdK26FSqRAREfHM/fDkyROsWLECrVq1gqWlJczMzNCgQQOMHTsWly5d0pp+586d6NWrF6ytrWFkZITmzZtj+fLlyM3NlaYZP348JkyYAACYMGGCRk3VRanPOPn7+2P37t14/fXXsWrVKvj4+MDBwQGxsbE4cuQIrly5gjp16mDBggXlWS8REdUEV/8Agl4BIDSHpzzIGz76F6Bp0c9BrUj5AWnVqlUAgBkzZgAA3NzcIISAn58ftm/fjsaNG+Ott95CWloagoKCMGjQIKxevRrTp0/XWuaWLVuwf/9+DBo0CFOnTsXjx48B5PWD2LlzZzx48AADBgxAq1atEBYWhj59+kih52nr1q3D1KlTYW1tjcGDB8POzg5nzpzB4sWLERISgpCQEBgYGMDX1xcRERHYsGEDfHx8pO0CACsrq2fuh1dffRVBQUFo2bIlJkyYAENDQ9y9exchISHo27cvWrRoIU370UcfYenSpXB2dsYLL7wACwsLHDlyBLNmzcKpU6ewZcsWAMCwYcOQlJSEnTt3YujQoWjduvUz66hyRBncuHFD9OzZU6hUKq1Xz549RVhYWFkWr6jk5GQBQCQnJytdChFRpcvIyBBXr14VGRkZmiPUaiEyU8v2ykgWYrmnEAssinhZCrHCK2+60q5DrS7zPnB1dRWurq4aw3755RcBQPj4+IjMzExpeFRUlLC3txf6+vrizp070vCff/5ZABAqlUr8/fffWut46aWXBADxxRdfaAzPnw+ACAkJkYZfuXJF6OnpiTZt2oiEhASNeZYuXSoAiOXLl0vDQkJCBACxYMGCEm17UlKSUKlUon379iInJ0djXE5Ojnj06JH08/79+wUA0b9/f5GWliYNV6vV4s033xQAxNatW7W27eeffy5RTUV+Jp9S0d/fZeoAs1GjRjhw4ADu3buHCxcuICUlReo5nH03ERHVQNnpwBLHCl6JAFLuA8vK8D3y0X3AwLT8Svp/+XeCff755zAwMJCGOzs7Y+bMmZgzZw42bdqEefPmacw3bNgw9O7dW2NYZmYmtmzZAgcHB62zVK+++io+++wzXL9+XWP4t99+i5ycHKxZswY2NjYa42bPno0vv/wSmzdvxnvvvVem7VSpVBBCwNDQELq6mm3OdHV1Nc5Yff3111JtJiYmGstYtmwZvv32W2zevBkvvPBCmWqqKkodnHr27ImuXbvik08+gbOzM5ydncuzLiIioirnwoULMDY2RocOHbTG5V8Ku3jxota4wqYPCwtDZmYm2rdvrxHCgLzQ0alTJ63gFBoaCgDYu3cvgoODtZapr6+vNU9RLl68iN9//11jmJubG8aPHw8LCwv069cPe/fuRdu2bTFy5Eh069YN3t7eWrWGhobC1NS0yDbPxsbGsmuqDkodnE6dOoWOHTuWZy1ERFTV6Zvknc0pi8gTwKaRz57uxa15d9mVhr7Js6cphZSUlCKvqNStWxcAkJycrDXOwcGh0GUBKPLu88LmSUxMBAAsXrxYXsHFuHjxIhYuXKgxzMfHB+PHjwcAbN26FUuWLMHmzZsxd+5cAIC5uTkmTpyIJUuWSGeXEhMTkZOTo7WsgtLS0spcb1VR6uDUpEkTWa3yiYioBlGpyn4JrEFPwMIxryH4043D81aSN75BzyrTNUE+CwsLxMbGFjouf7iFhYXWuMLuGsuf7uHDh8Uur7B5UlJSYG5uLq/oIowfP14KSYUxNTXF4sWLsXjxYoSHhyMkJATr1q3D6tWrkZGRgW+//VaqSaVSIT4+vkz1VBel7o5g2rRp+OOPP3D16tXyrIeIiGo6HV2g32f//8PTgeL/f+63rMqFJgBo06YNMjIycPr0aa1xhw8fBgDZd4p5enrC0NAQ586dQ1ZWlsY4IYR0Wa4gb29vACh0XGHy2ycV7BKgNNzd3TFx4kQcPnwYZmZm+OOP/7qM8Pb2RkJCAm7evFmpNSml1MHJ3d0dvr6+6NixI2bNmoWgoCAcPnwYR44c0XoRERFpaDokr8sBi3qawy0cFe2K4FleffVVAMCcOXOQnZ0tDY+OjsaXX34JPT09vPjii7KWZWhoiJEjRyImJgZr1qzRGPfLL7/g2rVrWvNMnToVenp6mDZtGqKiorTGJyUl4cKFC9LP+Q3I7927J6umfA8fPiw0HD569AiZmZkwNjaWhuU3bJ84cSISEhK05omJidHYltLWVFWU+lKdr6+v1Op+xYoVxXZeVV1TJRERVaCmQwCvgVW25/DCvPzyy9i+fTt27tyJli1bYtCgQVI/TgkJCVixYgU8PDxkL2/p0qUIDg7GrFmzEBISgtatWyMsLAy7d++WGmfr6Px3jqN58+ZYu3YtpkyZAk9PTwwYMAANGjRASkoK7ty5g8OHD2P8+PFYt24dAMDLywuOjo4ICAiAiYkJnJ2doVKpMGXKFFhaWhZZV3R0NLy9vdGsWTO0bdsWTk5OSEhIwM6dO5GdnY3Zs2dL0/br1w/z58/HokWL0LBhQ/Tr1w+urq5ISEjArVu3cPToUXz66ado0qQJAKBTp04wNjbGqlWrkJKSIrXx+vDDD0v0Xiil1MHp448/rlY9fRIRURWkowu4d1O6CtlUKhW2bt2K1atXY8OGDfjqq69gYGCAtm3b4t1338WQISU7U+bi4oKTJ0/igw8+wP79+3Ho0CG0a9cO+/fvlzqNfLrN1GuvvYbWrVvjyy+/xJEjR/DHH3/A0tIS9evXx8yZM6WzYkDeZbHt27fjgw8+wK+//ip1vDlmzJhig5Obmxv8/f1x8OBBBAcHIyEhAXXq1EHbtm0xc+ZM9OnTR2P6Tz75BN27d8eaNWtw4MABJCUlwdbWFu7u7vD399c4C2djY4OtW7fC398f//vf/5CRkQGg+gQnlRCisJZ5tV5KSgosLS2RnJxcaEM/IqKa7MmTJwgPD4e7uzuMjIyULqdW6tq1K06ePInk5GSYmZkpXY7i5H4mK/r7u9RtnHR1dWVfxyUiIqLCPXjwQGvYpk2bcPz4cfTu3ZuhqYop9aU6CwsL9g5ORERURs2bN0ebNm3QtGlT6Orq4uLFizh06BDMzc2xfPlypcujp5Q6OHXo0AH//PNPedZCRERU67z55pvYtWsXzp49i7S0NNjZ2WHcuHGYP38+vLy8lC6PnlLqNk6hoaHw8fHBd999p9EQraZgGyciqs3YxomqmqrSxqnUZ5z2798PX19fTJw4EV999RU6dOgABwcHrTvtVCoV5s+fX+ZCiYiIiJRW6jNOBfuVKHYFKlW17MeJZ5yIqDbjGSeqaqr9GaeQkJDyrEPy+PFjLFq0CBcvXsSFCxcQHx+PBQsWwN/fX9b8cXFxmD17Nnbv3o309HS0atUKn376KXr16lUh9RIREVHtUerg5OPjU551SBISEvDdd9+hVatWGDZsGH744QfZ82ZmZqJXr15ISkrC6tWrYW9vj2+++Qb9+vVDcHBwhdVMREREtUOp+3ECgJycHKxcuRIdOnSAhYUF9PT+y2EXL17E1KlTcePGjRIt09XVFY8ePcLhw4exdOnSEs37448/4vLlywgKCsKLL76I559/Hlu3bkXjxo01uocnIiIiKo1SB6eMjAz06NED77//PiIjI2FhYYGCzaXc3d3x888/45dffinRclUqVakf5bJjxw54enqiU6dO0jA9PT289NJLOH36NKKjo0u1XCIiIiKgDMFpyZIlOH78OJYuXYqYmBhMnjxZY7ylpSV8fHywb9++Mhcp1+XLl9GyZUut4fnDrly5Umm1EBERUc1T6jZOgYGB8PX1lS6BFXaWyMPDAxcuXCh9dSWUkJAAGxsbreH5wxISEoqcNzMzE5mZmdLPKSkp5V8gERERVWulPuN09+5dPPfcc8VOY2FhgeTk5NKuolSKu8xX3LilS5fC0tJSevFxMkREVBF8fX21vo8OHToElUol+w7yopZDFa/Uwcnc3BwPHz4sdprbt2/Dzs6utKsoMVtb20LPKiUmJgJAoWej8s2ZMwfJycnSKyoqqsLqJCIiqilKE/qqs1JfquvYsSN27dqF5ORkWFpaao2/d+8e/vrrLwwbNqws9ZVIixYtcOnSJa3h+cOaN29e5LyGhoYwNDSssNqIiIiK0qFDB1y7dg116tRRuhR6hlKfcZo1axYSExPRu3dvnDhxAjk5OQCA9PR0HDhwAH369EF2djbefffdciv2WYYPH47r16/j1KlT0rCcnBxs3LgR3t7ecHR0rLRaiIiI5DIxMYGXlxeDUzVQ6uDUvXt3fPPNN/jnn3/QrVs3LFmyBEDeJbw+ffrg1q1bWLt2Ldq1a1fiZe/Zswdbt27Frl27AABXr17F1q1bsXXrVqSnpwMAJk2aBD09PURGRkrzTZw4Ec2aNcOoUaPw22+/ITg4GKNHj0ZYWBg+++yz0m4qERHVIkeOHIFKpcKkSZMKHX/v3j3o6upKT6Q4d+4c3n77bTRv3hyWlpYwNjZGixYtsGzZMmRnZ8taZ3GXu44dOwYfHx+YmprC1tYWfn5+pWpOolar8cMPP6BDhw6wsbGBiYkJ3NzcMGzYMBw5ckRr+iNHjmDw4MGoU6cODA0N0ahRI8ybN0/6HgYAf39/9OjRAwCwcOFCqUshlUqFiIiIEtdYHZT6Uh0AvPnmm/Dx8cG6detw6tQpJCYmwsLCAt7e3pg6dSqaNWtWquVOmTJFIxBt2bIFW7ZsAQCEh4fDzc0Nubm5yM3N1eg7ytDQEAcOHMDs2bMxbdo0pKeno3Xr1tizZw97DSciqqJi0mJwN+Uu6lvUR13TukqXg27dusHNzQ3btm3DN998o/VctE2bNkGtVuPll18GAHz//ffYtWsXunfvjgEDBiA9PR2HDh3CnDlzcObMGWzbtq3UtRw4cAD9+/eHjo4O/Pz84OjoiAMHDqBLly6wtrYu0bLmzJmDzz//HA0aNMC4ceNgbm6O6OhoHD16FAcPHkT37t2ladetW4epU6fC2toagwcPhp2dHc6cOYPFixcjJCQEISEhMDAwgK+vLyIiIrBhwwb4+PjA19dXWoaVlVWpt7tKE1So5ORkAUAkJycrXQoRUaXLyMgQV69eFRkZGRrD1Wq1SMtKK7fX5mubRcv1LUXz9c1Fy/UtxeZrm8tluWq1ukzbP3fuXAFABAUFaY1r0aKFMDY2FikpKUIIISIiIkROTo7Wfpo4caIAII4dO6YxzsfHRzz99RsSEiIAiAULFkjDcnNzhYeHh1CpVOLo0aMayx43bpwAoLWc4tjY2AgnJyeRlpamVWtCQoL085UrV4Senp5o06aNxnAhhFi6dKkAIJYvX15s7RWhqM/k0yr6+7tMZ5yIiKh2ycjJgPdv3hWybDXUWHxqMRafWlzmZZ0adwom+ialnv/ll1/G4sWLsXHjRowaNUoa/s8//+DSpUsYM2YMzM3NAeQ9KuxpKpUKb731Fn766ScEBwejS5cuJa7h2LFjuHPnDgYPHoyuXbtqLHvJkiUIDAxEbm5uiZZpYGCg8Xi0/OUVvOv822+/RU5ODtasWaN1N/rs2bPx5ZdfYvPmzXjvvfdKvE01AYMTERHRUzw9PdG+fXvs2bMHiYmJUoD49ddfAUC6TAcAWVlZ+PrrrxEQEIDr168jNTVVoxnJ/fv3S1XDP//8AyDv0uHTXF1d4eLiotGOKCIiAuvXr9eYzsrKCjNmzAAAjB49GuvWrUPz5s3h5+cHHx8fdOrUCaamphrzhIaGAgD27t2L4OBgrXXr6+vj+vXrpdqmmoDBiYiIZDPWM8apcaeePaEMsemxGPb7MKihlobpqHTw+9Df4WDiUKZlG+sZl7U8vPzyyzh79iyCgoLw5ptvQq1WY/PmzbC3t0efPn2k6UaOHIldu3ahcePG8PPzg729PfT19ZGUlITVq1drPJWiJPI7kLa3ty90vIODg1ZwWrhwocY0rq6uUnBas2YNPDw8sH79enz66af49NNPYWRkhNGjR2PFihXSHX35fR8uXlz2M381EYMTERHJplKpynQJrCB3S3cs6LwAC08uhFqooaPSwYJOC+Bu6V4uyy+rMWPG4L333sPGjRvx5ptv4uDBg7h//z7eeecd6XLXmTNnsGvXLvTt2xd//vkndHV1pflDQ0OxevXqUq8/v4/EuLi4QsfHxsZq/Ozr66txputp+vr6mDVrFmbNmoX79+/j8OHD+Pnnn/HLL78gJiZGerashYUFgLxHj+VfjqT/lLo7AiIiorIa0WgE9r2wDz/1/Qn7XtiHEY1GKF2SJP/M0okTJxAeHo6NGzcCAF566SVpmtu3bwMABg4cqBGaAODo0aNlWn+rVq2KXE5kZGSZnnDh6OiIsWPHYu/evWjUqBGCg4ORkZEBAPD2zmvDln/J7lnyt7uk7a2qKwYnIiJSVF3Tuniu7nNVoiuCp7388ssQQuCHH37A9u3b4eXlhfbt20vj8xuGHzt2TGO+K1euYOnSpWVad9euXeHu7o7du3drLF8IgY8++qhEQSUzMxMHDx7UOiOVlpaGx48fQ19fXwpAU6dOhZ6eHqZNm1ZoOEtKSsKFCxekn/Pbf927d69E21dd8VIdERFREYYOHQoLCwt88cUXyM7O1mgUDuQ9KqVDhw4ICgrCgwcP0LFjR9y9exd//PEHBg4ciK1bt5Z63To6Ovjuu+8wYMAA9O7dW+rH6eDBg3jw4AFatmyJf//9V9ayMjIy0KtXL3h4eMDb2xv169dHamoqdu/ejZiYGHzwwQcwMDAAkPd4srVr12LKlCnw9PTEgAED0KBBA6SkpODOnTs4fPgwxo8fj3Xr1gEAvLy84OjoiICAAJiYmMDZ2RkqlQpTpkwp9JFs1V6FdHJQA7AfJyKqzeT2mVMbTJgwQQAQKpVKREREaI2Pi4sTEydOFI6OjsLIyEi0aNFCfPPNN+LOnTsCgHj11Vc1ppfbj1O+I0eOiO7duwtjY2NhY2MjRo0aJSIjIwtdTlGysrLEZ599Jvr06SOcnZ2FgYGBcHBwED4+PiIgIKDQeU6fPi3GjBkjHB0dhb6+vqhTp45o27at+PDDD8W1a9c0pg0NDRU+Pj7C3Nxc6l8qPDxcVm1yVZV+nFRCFNOSrBZLSUmBpaUlkpOTpYZyRES1xZMnTxAeHg53d3etnrOJlCD3M1nR399s40REREQkE4MTERERkUwMTkREREQyMTgRERERycTgRERERCQTgxMRERGRTAxORERERDIxOBERUZHY1R9VFVXls8jgREREWvT08p7IlZOTo3AlRHmys7MBQOthypWNwYmIiLTo6upCV1cXKSkpSpdCBCEEkpOTYWhoCH19fUVr4UN+iYhIi0qlgr29PR48eABDQ0OYmppCpVIpXRbVMkIIZGdnIzk5GampqXByclK6JAYnIiIqnKWlJTIyMhAfH4+HDx8qXQ7VYoaGhnBycqoSz45lcCIiokKpVCrUq1cP9vb2UvsSosqmq6ur+OW5ghiciIioWPntnYiIjcOJiIiIZGNwIiIiIpKJwYmIiIhIJgYnIiIiIpkYnIiIiIhkYnAiIiIikonBiYiIiEgmBiciIiIimRiciIiIiGRicCIiIiKSicGJiIiISCYGJyIiIiKZGJyIiIiIZGJwIiIiIpKJwYmIiIhIJgYnIiIiIpkYnIiIiIhkYnAiIiIikonBiYiIiEgmBiciIiIimRiciIiIiGRicCIiIiKSicGJiIiISCYGJyIiIiKZGJyIiIiIZGJwIiIiIpKJwYmIiIhIJgYnIiIiIpkYnIiIiIhkYnAiIiIikonBiYiIiEgmBiciIiIimRiciIiIiGRicCIiIiKSicGJiIiISCYGJyIiIiKZGJyIiIiIZGJwIiIiIpKpSgan1NRUzJgxA46OjjAyMkLr1q0REBDwzPnWr18PlUpV6CsmJqYSKiciIqKaTE/pAgozYsQInDlzBsuWLUPjxo3x22+/YezYsVCr1Rg3btwz5//555/h5eWlMczW1raiyiUiIqJaosoFp7/++gt///23FJYAoEePHoiMjMSsWbPg5+cHXV3dYpfRvHlztG/fvjLKJSIiolqkyl2q27FjB8zMzDBq1CiN4RMmTMD9+/dx6tQphSojIiKi2q7KBafLly+jSZMm0NPTPBnWsmVLafyzDBo0CLq6urCxscGIESNkzUNERET0LFXuUl1CQgI8PDy0htvY2Ejji1K3bl3MnTsXHTt2hIWFBS5duoRly5ahY8eOOH78OFq1alXkvJmZmcjMzJR+TklJKcNWEBERUU1U5YITAKhUqlKN69evH/r16yf93L17dwwcOBAtWrTAxx9/jJ07dxY579KlS7Fw4cLSFUxERES1QpW7VGdra1voWaXExEQA/515ksvNzQ1du3ZFaGhosdPNmTMHycnJ0isqKqpE6yEiIqKar8oFpxYtWuDatWvIycnRGH7p0iUAeXfMlZQQAjo6xW+qoaEhLCwsNF5EREREBVW54DR8+HCkpqZi27ZtGsM3bNgAR0dHeHt7l2h54eHhOH78ODp27FieZRIREVEtVOXaOPXv3x/PP/88pkyZgpSUFDRs2BCbN2/G3r17sXHjRqkPp0mTJmHDhg24ffs2XF1dAQC9e/dG9+7d0bJlS6lx+Oeffw6VSoVFixYpuVlERERUA1S54AQA27dvx9y5c/Hxxx8jMTERXl5e2Lx5M8aMGSNNk5ubi9zcXAghpGEtWrRAYGAgli9fjoyMDNjb26Nnz56YP38+GjdurMSmEBERUQ2iEgWTB0lSUlJgaWmJ5ORktnciIiKqJir6+7vKtXEiIiIiqqoYnIiIiIhkYnAiIiIikonBiYiIiEgmBiciIiIimRiciIiIiGRicCIiIiKSicGJiIiISCYGJyIiIiKZGJyIiIiIZGJwIiIiIpKJwYmIiIhIJgYnIiIiIpkYnIiIiIhkYnAiIiIikonBiYiIiEgmBiciIiIimRiciIiIiGRicCIiIiKSicGJiIiISCYGJyIiIiKZGJyIiIiIZGJwIiIiIpKJwYmIiIhIJgYnIiIiIpkYnIiIiIhkYnAiIiIikonBiYiIiEgmBiciIiIimRiciIiIiGRicCIiIiKSicGJiIiISCYGJyIiIiKZGJyIiIiIZGJwIiIiIpKJwYmIiIhIJgYnIiIiIpkYnIiIiIhkYnAiIiIikonBiYiIiEgmBiciIiIimRiciIiIiGRicCIiIiKSicGJiIiISCYGJyIiIiKZGJyIiIiIZGJwIiIiIpKJwYmIiIhIJgYnIiIiIpkYnIiIiIhkYnAiIiIikonBiYiIiEgmBiciIiIimRiciIiIiGRicCIiIiKSicGJiIiISCYGJyIiIiKZGJyIiIiIZGJwIiIiIpKJwYmIiIhIJgYnIiIiIpkYnIiIiIhkqpLBKTU1FTNmzICjoyOMjIzQunVrBAQEyJo3Li4O48ePR506dWBiYoJOnTrhwIEDFVwxERER1QZ6ShdQmBEjRuDMmTNYtmwZGjdujN9++w1jx46FWq3GuHHjipwvMzMTvXr1QlJSElavXg17e3t888036NevH4KDg+Hj41OJW1EIdS4QeQJIjQXMHADXzoCOrrI11VTc15WD+7lycD9XDu7nylHN97NKCCGULqKgv/76CwMHDpTCUr4+ffrgypUruHv3LnR1C9/Ba9euxVtvvYUTJ06gU6dOAICcnBy0atUKZmZmOHXqlOw6UlJSYGlpieTkZFhYWJRtowDg6h/A3g+AlPv/DbNwBPp9BjQdUvbl03+4rysH93Pl4H6uHNzPlaMS9nO5f38/pcpdqtuxYwfMzMwwatQojeETJkzA/fv3iw0/O3bsgKenpxSaAEBPTw8vvfQSTp8+jejo6Aqru1hX/wCCXtH8oABAyoO84Vf/UKaumoj7unJwP1cO7ufKwf1cOWrIfq5yl+ouX76MJk2aQE9Ps7SWLVtK4zt37lzkvN26ddManj/vlStX4OTkVM4VP4M6Fzl/zYYuBFRaIwUEVMj5czaynbtWq1OVVZI6F/p/zoIe93XF4n6uHNzPlYP7uXLI2M+5f30APa+BVX4/V7nglJCQAA8PD63hNjY20vji5s2frqTzZmZmIjMzU/o5JSVFds3FijwBvdQHRY5WQUA/7QH0v3Qvn/VRkbivKwf3c+Xgfq4c3M+VQwUBvdT7eW2f3LVPgFQlVe5SHQCoVNp5VM64ssy7dOlSWFpaSi8XF5dnFypHamz5LIeIiKimqwbfmVXujJOtrW2hZ4YSExMBoNAzSuUx75w5c/Duu+9KP6ekpJRPeDJzkDXZk9GBUNfv9OwJqUg6d0/CKMjvmdNxX5cN93Pl4H6uHNzPlUPufpb7namkKhecWrRogc2bNyMnJ0ejndOlS5cAAM2bNy923vzpCpIzr6GhIQwNDUtbdtFcOwMWjhApD6CC9g2MAiqoLBxh5PV8lb+uW+V5Pc99XRm4nysH93Pl4H6uHDL3M1wLb8NclVS5S3XDhw9Hamoqtm3bpjF8w4YNcHR0hLe3d7HzXr9+XePOu5ycHGzcuBHe3t5wdHSssLqLpKObd5slAPVTo6Sf+y3jL2R54L6uHNzPlYP7uXJwP1eOGrSfq1xw6t+/P55//nlMmTIF33//PUJCQvD6669j7969+Pzzz6U+nCZNmgQ9PT1ERkZK806cOBHNmjXDqFGj8NtvvyE4OBijR49GWFgYPvvsM6U2CWg6BKrRv0BlrhncVBZOUI3+hX2ElCfu68rB/Vw5uJ8rB/dz5agh+7nKdYAJ5D1yZe7cuQgKCkJiYiK8vLwwZ84cjBkzRppm/Pjx2LBhA8LDw+Hm5iYNj42NxezZs7F7926kp6ejdevWWLRoEXr37l2iGiqkA61q3ltqtcJ9XTm4nysH93Pl4H6uHBW8nyu6A8wqGZyqgore8URERFT+al3P4URERERVFYMTERERkUwMTkREREQyMTgRERERycTgRERERCQTgxMRERGRTAxORERERDIxOBERERHJxOBEREREJJOe0gVUVfkdqqekpChcCREREcmV/71dUQ9GYXAqwuPHjwEALi4uCldCREREJfX48WNYWlqW+3L5rLoiqNVq3L9/H+bm5lCpVOW23JSUFLi4uCAqKorPwKtg3NeVg/u5cnA/Vw7u58pRkftZCIHHjx/D0dEROjrl3yKJZ5yKoKOjA2dn5wpbvoWFBX8pKwn3deXgfq4c3M+Vg/u5clTUfq6IM0352DiciIiISCYGJyIiIiKZGJwqmaGhIRYsWABDQ0OlS6nxuK8rB/dz5eB+rhzcz5WjOu9nNg4nIiIikolnnIiIiIhkYnAiIiIikonBiYiIiEgmBqdKkpqaihkzZsDR0RFGRkZo3bo1AgIClC6rxnn8+DFmz56NPn36wM7ODiqVCv7+/kqXVeMcPHgQEydOhJeXF0xNTeHk5IShQ4fi3LlzSpdWo1y8eBEDBw5E/fr1YWxsDBsbG3Tq1AkbN25UurQa74cffoBKpYKZmZnSpdQYhw4dgkqlKvQVGhqqdHmysQPMSjJixAicOXMGy5YtQ+PGjfHbb79h7NixUKvVGDdunNLl1RgJCQn47rvv0KpVKwwbNgw//PCD0iXVSP/73/+QkJCAd955B02bNsXDhw+xYsUKdOzYEfv27UPPnj2VLrFGSEpKgouLC8aOHQsnJyekpaVh06ZNePnllxEREYF58+YpXWKNFB0djffffx+Ojo5ITk5WupwaZ8mSJejRo4fGsObNmytUTcnxrrpK8Ndff2HgwIFSWMrXp08fXLlyBXfv3oWurq6CFdYc+R9nlUqF+Ph42NnZYcGCBTzrVM7i4uJgb2+vMSw1NRUNGzZE8+bNERwcrFBltUPHjh1x//593L17V+lSaqTBgwdDpVLBxsYGW7duRWpqqtIl1QiHDh1Cjx49sGXLFowcOVLpckqNl+oqwY4dO2BmZoZRo0ZpDJ8wYQLu37+PU6dOKVRZzZN/2pcq1tOhCQDMzMzQtGlTREVFKVBR7VKnTh3o6fGCQUXYuHEjDh8+jLVr1ypdClVRDE6V4PLly2jSpInWga5ly5bSeKLqLjk5GefPn0ezZs2ULqXGUavVyMnJwcOHD7F27Vrs27cPH3zwgdJl1ThxcXGYMWMGli1bVqHPKq3t3nrrLejp6cHCwgJ9+/bFsWPHlC6pRPgnSyVISEiAh4eH1nAbGxtpPFF199ZbbyEtLQ1z585VupQaZ+rUqfj2228BAAYGBlizZg3eeOMNhauqeaZOnQpPT09MmTJF6VJqJEtLS7zzzjvw9fWFra0tbt26hS+++AK+vr74888/0bdvX6VLlIXBqZIUd/mIl5aoups/fz42bdqEr776Cu3atVO6nBrno48+wuTJkxEXF4ddu3bh7bffRlpaGt5//32lS6sxtm3bhl27duHChQs8JleQNm3aoE2bNtLP3bp1w/Dhw9GiRQvMnj2bwYn+Y2trW+hZpcTERAD/nXkiqo4WLlyITz/9FIsXL8bbb7+tdDk1Uv369VG/fn0AwIABAwAAc+bMwauvvgo7OzslS6sRUlNT8dZbb2HatGlwdHREUlISACArKwtA3t2N+vr6MDU1VbDKmsnKygqDBg3CunXrkJGRAWNjY6VLeia2caoELVq0wLVr15CTk6Mx/NKlSwCq122YRAUtXLgQ/v7+8Pf3x0cffaR0ObVGhw4dkJOTgzt37ihdSo0QHx+P2NhYrFixAtbW1tJr8+bNSEtLg7W1NV588UWly6yxCt4NXR3wjFMlGD58OL7//nts27YNfn5+0vANGzbA0dER3t7eClZHVDqLFi2Cv78/5s2bhwULFihdTq0SEhICHR2dQttOUsnVrVsXISEhWsOXLVuGw4cPY8+ePahTp44CldV8jx49wu7du9G6dWsYGRkpXY4sDE6VoH///nj++ecxZcoUpKSkoGHDhti8eTP27t2LjRs3sg+ncrZnzx6kpaXh8ePHAICrV69i69atAPIuc5iYmChZXo2wYsUKfPzxx+jXrx8GDhyo1etvx44dFaqsZnn99ddhYWGBDh06wMHBAfHx8diyZQsCAwMxa9YsXqYrJ0ZGRvD19dUavn79eujq6hY6jkpu3LhxqF+/Ptq3b486derg5s2bWLFiBWJjY7F+/Xqly5ONHWBWktTUVMydOxdBQUFITEyEl5cX5syZgzFjxihdWo3j5uaGyMjIQseFh4fDzc2tcguqgXx9fXH48OEix/OwUj5+/vln/Pzzz7h27RqSkpJgZmaGVq1aYfLkyXjppZeULq/GGz9+PDvALEfLli1DYGAgwsPDkZqaChsbG3Tt2hVz5szBc889p3R5sjE4EREREcnExuFEREREMjE4EREREcnE4EREREQkE4MTERERkUwMTkREREQyMTgRERERycTgRERERCQTgxMRUSlFRERApVJh/PjxSpdCRJWEwYmIiIhIJgYnIiIiIpkYnIiIiIhkYnAioirjyJEjGDx4MOrUqQNDQ0M0atQI8+bNQ3p6ujTNoUOHoFKp4O/vjyNHjsDHxwdmZmawsbHBuHHjcO/evUKXfeXKFfj5+cHe3h6GhoZwd3fHzJkzkZiYWOj0cXFxeP/99+Hp6QkjIyPY2NigY8eOWLFiRaHT37lzByNHjoS1tTVMTU3Ru3dv/PPPP2XfKURUpfAhv0RUJaxbtw5Tp06FtbU1Bg8eDDs7O5w5cwaHDx9G586dERISAgMDAxw6dAg9evRA3759ERISgoEDB8LLywvnz5/Hvn374OLigjNnzsDBwUFa9okTJ9CnTx9kZmZi5MiRcHNzQ2hoKA4dOoRGjRrh5MmTsLW1laa/efMmevTogejoaHTt2hWdO3dGWloaLl++jH///VcKWxEREXB3d4ePjw+uXLmCpk2bon379rh9+zZ27twJa2trXLt2TaMWIqrmBBGRwq5cuSL09PREmzZtREJCgsa4pUuXCgBi+fLlQgghQkJCBAABQPzwww8a0y5cuFAAEBMnTpSG5ebmikaNGgkAYu/evRrTz5kzRwAQkyZN0hjeoUMHAUB89913WrVGRUVJ/w8PD5dqWbZsmcZ08+bNEwDE0qVLS7AniKiqY3AiIsVNnz5dABBHjx7VGpebmyvs7OxEu3bthBD/BSdPT0+hVqs1pk1PTxd2dnbC2NhYZGZmCiGEOHLkiAAg+vfvr7Xs1NRUYWtrqzH96dOnBQDRvXv3Z9adH5zc3d1Fbm5uoeNGjBghbycQUbWgV/nnuIiINIWGhgIA9u7di+DgYK3x+vr6uH79usawLl26QKVSaQwzNjZGu3btsHfvXty4cQPNmzfHhQsXAAC+vr5ayzU1NUX79u2xb98+afrTp08DAPr06SO7/latWkFHR7PJqLOzMwAgKSlJ9nKIqOpjcCIixeW3GVq8eLHseezt7Qsdnt+eKDk5GQCQkpKiMfxpdevW1Zg+P+g4OTnJrsXS0lJrmJ5e3uE1NzdX9nKIqOrjXXVEpDgLCwsAeSFH5DUhKPRVUFxcXKHLio2NBfBfmMlfdv7woqbPn87KygoAEB0dXYYtIqKaisGJiBTn7e0N4L9LdnIcP35cK0xlZGTg3LlzMDY2RuPGjQEAbdq0AZDXjcHT0tPTcfbsWRgbG8PT0xMA0KFDBwDA/v37S7wdRFTzMTgRkeKmTp0KPT09TJs2DVFRUVrjk5KSpLZK+cLCwvDTTz9pDPviiy/w8OFDjB07FgYGBgDy2kI1aNAAe/bs0Wo/tXTpUsTHx2tM/9xzz6FDhw44cuQIvv/+e61aeCaKqHZjGyciUlzz5s2xdu1aTJkyBZ6enhgwYAAaNGiAlJQU3LlzB4cPH8b48eOxbt06aZ4+ffpg6tSp+PPPP7X6cVqyZIk0nY6ODtavX4++fftiwIABGDVqFFxdXXHq1CkcPHgQDRo0wLJlyzTq2bhxI3x9ffH666/j119/RadOnfDkyRNcuXIFFy5cQEJCQqXtGyKqWnjGiYiqhNdeew0nT57E0KFDcfLkSaxcuRJbt25FfHw8Zs6ciRkzZmhM36lTJ/z999+Ij4/H6tWrcerUKYwZMwbHjx/XagjetWtXhIaGYujQodi/fz+WL1+O27dvY/r06QgNDYWdnZ3G9I0aNcL58+fxzjvvIDo6GqtWrcLGjRuRmpqKefPmVfSuIKIqjD2HE1G1kt9z+IIFC+Dv7690OURUy/CMExEREZFMDE5EREREMjE4EREREcnENk5EREREMvGMExEREZFMDE5EREREMjE4EREREcnE4EREREQkE4MTERERkUwMTkREREQyMTgRERERycTgRERERCQTgxMRERGRTP8Hqx9jjJJfgU4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.plot(indices, acc_vs, marker='.', alpha=1, label='valid-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('SCRUB retain-, valid- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Model_D_r -> Loss:0.003, Error:0.0\n",
      "Original_Model_D_f -> Loss:0.002, Error:0.0\n",
      "Original_Model_D_t -> Loss:0.066, Error:0.019\n"
     ]
    }
   ],
   "source": [
    "m_D_r_activations,m_D_r_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(retain_loader),'Original_Model_D_r')\n",
    "m_D_f_activations,m_D_f_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(forget_loader),'Original_Model_D_f')\n",
    "m_D_t_activations,m_D_t_predictions=activations_predictions(copy.deepcopy(model),copy.deepcopy(test_loader_full),'Original_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain_Model_D_r -> Loss:0.003, Error:0.0\n",
      "Retrain_Model_D_f -> Loss:0.02, Error:0.0\n",
      "Retrain_Model_D_t -> Loss:0.067, Error:0.019\n"
     ]
    }
   ],
   "source": [
    "m0_D_r_activations,m0_D_r_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(retain_loader),'Retrain_Model_D_r')\n",
    "m0_D_f_activations,m0_D_f_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(forget_loader),'Retrain_Model_D_f')\n",
    "m0_D_t_activations,m0_D_t_predictions=activations_predictions(copy.deepcopy(model0),copy.deepcopy(test_loader_full),'Retrain_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRUB_D_r -> Loss:0.003, Error:0.0\n",
      "SCRUB_D_f -> Loss:0.001, Error:0.0\n",
      "SCRUB_D_t -> Loss:0.064, Error:0.019\n"
     ]
    }
   ],
   "source": [
    "ntk_D_r_activations,ntk_D_r_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(retain_loader),'SCRUB_D_r')\n",
    "ntk_D_f_activations,ntk_D_f_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(forget_loader),'SCRUB_D_f')\n",
    "ntk_D_t_activations,ntk_D_t_predictions=activations_predictions(copy.deepcopy(model_s),copy.deepcopy(test_loader_full),'SCRUB_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, modelf)\n",
    "#hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-7\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_D_r_activations,fisher_D_r_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(retain_loader),'Fisher_D_r')\n",
    "fisher_D_f_activations,fisher_D_f_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(forget_loader),'Fisher_D_f')\n",
    "fisher_D_t_activations,fisher_D_t_predictions=activations_predictions(copy.deepcopy(modelf),copy.deepcopy(test_loader_full),'Fisher_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information in the Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_D_r_activations,finetune_D_r_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(retain_loader),'Finetune_D_r')\n",
    "finetune_D_f_activations,finetune_D_f_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(forget_loader),'Finetune_D_f')\n",
    "finetune_D_t_activations,finetune_D_t_predictions=activations_predictions(copy.deepcopy(model_ft),copy.deepcopy(test_loader_full),'Finetune_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(5,1,figsize=(9,20))\n",
    "ax = [0,0,0,0,0]\n",
    "plot_entropy_dist(copy.deepcopy(model_s),retain_loader,forget_loader,test_loader_full, 'SCRUB',ax[4])\n",
    "plot_entropy_dist(copy.deepcopy(model),retain_loader,forget_loader,test_loader_full, 'original', ax[0])\n",
    "plot_entropy_dist(copy.deepcopy(model0),retain_loader,forget_loader,test_loader_full, 'retrain', ax[1])\n",
    "plot_entropy_dist(copy.deepcopy(model_ft),retain_loader,forget_loader,test_loader_full, 'finetuned',ax[2])\n",
    "plot_entropy_dist(copy.deepcopy(modelf),retain_loader,forget_loader,test_loader_full, 'fisher',ax[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
