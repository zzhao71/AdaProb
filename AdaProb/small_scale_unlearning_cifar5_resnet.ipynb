{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_316924/2931265111.py:24: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import variational\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from itertools import cycle\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: cifar100_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in cifar100_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 100\n",
      "[0] train metrics:{\"loss\": 4.004363647079468, \"error\": 0.914425}\n",
      "Learning Rate : 0.1\n",
      "[0] dry_run metrics:{\"loss\": 3.6652862831115725, \"error\": 0.865175}\n",
      "Learning Rate : 0.1\n",
      "[0] test metrics:{\"loss\": 3.6771464149475097, \"error\": 0.8661}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 12.53 sec\n",
      "[1] train metrics:{\"loss\": 3.4841514293670652, \"error\": 0.823875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.14 sec\n",
      "[2] train metrics:{\"loss\": 3.0799445030212405, \"error\": 0.73755}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.02 sec\n",
      "[3] train metrics:{\"loss\": 2.761986903381348, \"error\": 0.657675}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.19 sec\n",
      "[4] train metrics:{\"loss\": 2.560997309112549, \"error\": 0.5965}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.18 sec\n",
      "[5] train metrics:{\"loss\": 2.414651040649414, \"error\": 0.545675}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.35 sec\n",
      "[6] train metrics:{\"loss\": 2.3048879302978516, \"error\": 0.50115}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.27 sec\n",
      "[7] train metrics:{\"loss\": 2.2363017753601073, \"error\": 0.461625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.33 sec\n",
      "[8] train metrics:{\"loss\": 2.176711745834351, \"error\": 0.429525}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.42 sec\n",
      "[9] train metrics:{\"loss\": 2.1533882205963133, \"error\": 0.40325}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.39 sec\n",
      "[10] train metrics:{\"loss\": 2.1217486545562743, \"error\": 0.375275}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.21 sec\n",
      "[11] train metrics:{\"loss\": 2.1148677562713623, \"error\": 0.3513}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.18 sec\n",
      "[12] train metrics:{\"loss\": 2.1146700733184813, \"error\": 0.333675}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.4 sec\n",
      "[13] train metrics:{\"loss\": 2.129422161102295, \"error\": 0.31615}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.4 sec\n",
      "[14] train metrics:{\"loss\": 2.1356759124755857, \"error\": 0.30355}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.44 sec\n",
      "[15] train metrics:{\"loss\": 2.140548034667969, \"error\": 0.285875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.08 sec\n",
      "[16] train metrics:{\"loss\": 2.1746923412322996, \"error\": 0.280475}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.08 sec\n",
      "[17] train metrics:{\"loss\": 2.1801952964782716, \"error\": 0.2681}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.17 sec\n",
      "[18] train metrics:{\"loss\": 2.165601744842529, \"error\": 0.250775}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.41 sec\n",
      "[19] train metrics:{\"loss\": 2.164006868362427, \"error\": 0.2464}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.39 sec\n",
      "[20] train metrics:{\"loss\": 2.1943339420318604, \"error\": 0.241625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.3 sec\n",
      "[21] train metrics:{\"loss\": 2.209365211868286, \"error\": 0.237475}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.16 sec\n",
      "[22] train metrics:{\"loss\": 2.228347407913208, \"error\": 0.237}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.29 sec\n",
      "[23] train metrics:{\"loss\": 2.19557455406189, \"error\": 0.22395}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.33 sec\n",
      "[24] train metrics:{\"loss\": 2.2061470024108885, \"error\": 0.220575}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.05 sec\n",
      "[25] train metrics:{\"loss\": 2.2127228797912597, \"error\": 0.220075}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.5 sec\n",
      "[26] train metrics:{\"loss\": 2.229200707626343, \"error\": 0.219875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.44 sec\n",
      "[27] train metrics:{\"loss\": 2.208640114212036, \"error\": 0.210725}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.1 sec\n",
      "[28] train metrics:{\"loss\": 2.2325496906280518, \"error\": 0.21615}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.1 sec\n",
      "[29] train metrics:{\"loss\": 2.222390404510498, \"error\": 0.209625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.05 sec\n",
      "[30] train metrics:{\"loss\": 2.218616831588745, \"error\": 0.207125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 6.25 sec\n",
      "Pure training time: 194.71 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset cifar100 --dataroot=data/cifar-100-python --model resnet --filters 0.4 --lr 0.1 --lossfn ce --num-classes 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_cifar5_resnet_0_4_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in small_cifar5_resnet_0_4_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 5\n",
      "[0] train metrics:{\"loss\": 1.6939151763916016, \"error\": 0.8179999995231628}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.6212269515991211, \"error\": 0.7820000019073486}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.49 sec\n",
      "[1] train metrics:{\"loss\": 1.6345634737014771, \"error\": 0.7880000023841858}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[2] train metrics:{\"loss\": 1.5389062576293946, \"error\": 0.6999999985694886}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[3] train metrics:{\"loss\": 1.4388466606140136, \"error\": 0.6040000014305115}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[4] train metrics:{\"loss\": 1.3492646703720093, \"error\": 0.5499999957084656}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[5] train metrics:{\"loss\": 1.272855809211731, \"error\": 0.5179999995231629}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[6] train metrics:{\"loss\": 1.2097751388549804, \"error\": 0.4800000021457672}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[7] train metrics:{\"loss\": 1.156522560119629, \"error\": 0.4400000004768372}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[8] train metrics:{\"loss\": 1.1067329597473146, \"error\": 0.39999999618530274}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[9] train metrics:{\"loss\": 1.0586243705749512, \"error\": 0.3720000033378601}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[10] train metrics:{\"loss\": 1.0163477210998535, \"error\": 0.3419999942779541}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[11] train metrics:{\"loss\": 0.9775122790336609, \"error\": 0.32399999713897704}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[12] train metrics:{\"loss\": 0.9428045825958252, \"error\": 0.3059999952316284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[13] train metrics:{\"loss\": 0.910737292766571, \"error\": 0.3020000047683716}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[14] train metrics:{\"loss\": 0.8812295255661011, \"error\": 0.29200000667572024}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[15] train metrics:{\"loss\": 0.8548699331283569, \"error\": 0.27600000429153443}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[16] train metrics:{\"loss\": 0.829343240737915, \"error\": 0.2700000071525574}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[17] train metrics:{\"loss\": 0.8058587074279785, \"error\": 0.2599999952316284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[18] train metrics:{\"loss\": 0.7844323315620423, \"error\": 0.2479999976158142}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[19] train metrics:{\"loss\": 0.7643776535987854, \"error\": 0.23999999809265138}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[20] train metrics:{\"loss\": 0.7454054098129272, \"error\": 0.23400000286102296}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[21] train metrics:{\"loss\": 0.7278691210746765, \"error\": 0.22199999809265136}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[22] train metrics:{\"loss\": 0.7109976725578309, \"error\": 0.21600000286102294}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[23] train metrics:{\"loss\": 0.6951841106414794, \"error\": 0.21200000286102294}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[24] train metrics:{\"loss\": 0.6799676637649537, \"error\": 0.21200000095367433}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[25] train metrics:{\"loss\": 0.6662802705764771, \"error\": 0.20400000047683717}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[26] train metrics:{\"loss\": 0.65282355260849, \"error\": 0.19800000047683716}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[27] train metrics:{\"loss\": 0.6395654606819153, \"error\": 0.19799999856948852}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[28] train metrics:{\"loss\": 0.6277284893989563, \"error\": 0.19000000762939454}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[29] train metrics:{\"loss\": 0.6160875129699707, \"error\": 0.18799999618530275}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[30] train metrics:{\"loss\": 0.6051100845336914, \"error\": 0.17599999809265138}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "Pure training time: 2.83 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset small_cifar5 --model resnet --dataroot=data/cifar10/ --filters 0.4 --lr 0.001 \\\n",
    "--resume checkpoints/cifar100_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_cifar5_resnet_0_4_forget_[0]_num_25_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in small_cifar5_resnet_0_4_forget_[0]_num_25_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Replacing indexes [83 30 56 24 16 23  2 27 28 13 99 92 76 14  0 21  3 29 61 79 35 11 84 44\n",
      " 73]\n",
      "Number of Classes: 5\n",
      "[0] train metrics:{\"loss\": 1.690088017463684, \"error\": 0.8139999995231628}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.619256443977356, \"error\": 0.7760000019073486}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.43 sec\n",
      "[1] train metrics:{\"loss\": 1.620905743598938, \"error\": 0.7719999978542328}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[2] train metrics:{\"loss\": 1.5128525915145874, \"error\": 0.6740000011920929}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[3] train metrics:{\"loss\": 1.4038205814361573, \"error\": 0.5759999973773956}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[4] train metrics:{\"loss\": 1.3108713741302491, \"error\": 0.5379999980926514}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[5] train metrics:{\"loss\": 1.238055965423584, \"error\": 0.5000000019073486}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[6] train metrics:{\"loss\": 1.1776326780319213, \"error\": 0.48200000476837157}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[7] train metrics:{\"loss\": 1.126336269378662, \"error\": 0.4500000052452087}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[8] train metrics:{\"loss\": 1.0779955778121948, \"error\": 0.41000000524520874}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[9] train metrics:{\"loss\": 1.0310990581512451, \"error\": 0.3719999966621399}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[10] train metrics:{\"loss\": 0.9900825247764588, \"error\": 0.33399999904632566}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[11] train metrics:{\"loss\": 0.9528926639556885, \"error\": 0.32599999475479124}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[12] train metrics:{\"loss\": 0.9191102304458618, \"error\": 0.3140000042915344}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[13] train metrics:{\"loss\": 0.8881525592803955, \"error\": 0.29600000715255737}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[14] train metrics:{\"loss\": 0.8592401347160339, \"error\": 0.282}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[15] train metrics:{\"loss\": 0.8333497009277344, \"error\": 0.27599999523162844}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[16] train metrics:{\"loss\": 0.808522810459137, \"error\": 0.26199999570846555}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[17] train metrics:{\"loss\": 0.7855033950805664, \"error\": 0.2499999976158142}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[18] train metrics:{\"loss\": 0.7644262280464172, \"error\": 0.2419999952316284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[19] train metrics:{\"loss\": 0.7445968685150146, \"error\": 0.23000000762939454}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[20] train metrics:{\"loss\": 0.7259157733917236, \"error\": 0.22400000047683716}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.11 sec\n",
      "[21] train metrics:{\"loss\": 0.7086338791847229, \"error\": 0.21600000715255738}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[22] train metrics:{\"loss\": 0.6916295642852783, \"error\": 0.21200000286102294}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[23] train metrics:{\"loss\": 0.6758940873146058, \"error\": 0.20600000524520873}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[24] train metrics:{\"loss\": 0.6607305450439453, \"error\": 0.20199999856948853}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[25] train metrics:{\"loss\": 0.6469281468391418, \"error\": 0.18800000286102295}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[26] train metrics:{\"loss\": 0.6333822975158692, \"error\": 0.18000000762939453}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "[27] train metrics:{\"loss\": 0.6199471464157105, \"error\": 0.17400000095367432}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.07 sec\n",
      "[28] train metrics:{\"loss\": 0.6079850153923034, \"error\": 0.16999999856948853}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.09 sec\n",
      "[29] train metrics:{\"loss\": 0.5961054925918579, \"error\": 0.1660000057220459}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.08 sec\n",
      "[30] train metrics:{\"loss\": 0.5850776405334472, \"error\": 0.15800000762939453}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.1 sec\n",
      "Pure training time: 2.8700000000000006 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset small_cifar5 --model resnet --dataroot=data/cifar10/ --filters 0.4 --lr 0.001 \\\n",
    "--resume checkpoints/cifar100_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 \\\n",
    "--forget-class 0 --num-to-forget 25 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 1787741\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model0 = copy.deepcopy(model)\n",
    "model_initial = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 300#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "model_initial.load_state_dict(torch.load(init_checkpoint))\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)\n",
    "\n",
    "model.cuda()\n",
    "model0.cuda()\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between w(D) and w(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(model,model0):\n",
    "    distance=0\n",
    "    normalization=0\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space='  ' if 'bias' in k else ''\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "        distance+=current_dist\n",
    "        normalization+=current_norm\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 0.13081099063947524\n",
      "Normalized Distance: 0.0014516105589520325\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Retrain']=distance(model,model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance of w(D) from initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntk_init(resume,seed=1):\n",
    "    manual_seed(seed)\n",
    "    model_init = models.get_model(arch, num_classes=num_classes, filters_percentage=filters).to(args.device)\n",
    "    model_init.load_state_dict(torch.load(resume))\n",
    "    return model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "for p in model_init.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 0.9763150743117927\n",
      "Normalized Distance: 0.010837351707806465\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Original_init']=distance(model_init,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.retain_bs = 32\n",
    "args.forget_bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [83 30 56 24 16 23  2 27 28 13 99 92 76 14  0 21  3 29 61 79 35 11 84 44\n",
      " 73]\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _ = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=args.forget_bs, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=args.retain_bs, seed=seed, shuffle=True)\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "475\n",
      "500\n",
      "500\n",
      "{0: 100, 1: 100, 2: 100, 3: 100, 4: 100}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'sgd'\n",
    "args.gamma = 1\n",
    "args.alpha = 0.5\n",
    "args.beta = 0.95\n",
    "args.smoothing = 0.5\n",
    "args.msteps = 10\n",
    "args.clip = 0.2\n",
    "args.sstart = 10\n",
    "args.kd_T = 2\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_epochs = 10\n",
    "args.sgda_learning_rate = 0.05\n",
    "args.lr_decay_epochs = [5,8,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 0.1#5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> scrub unlearning ...\n",
      " * Acc@1 98.947 \n",
      "maximize loss: -3.42\t minimize loss: 0.46\t train_acc: 98.9473648071289\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.368 \n",
      "maximize loss: -3.41\t minimize loss: 0.45\t train_acc: 99.36841583251953\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.579 \n",
      "maximize loss: -3.41\t minimize loss: 0.44\t train_acc: 99.57894134521484\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 98.737 \n",
      "maximize loss: -3.41\t minimize loss: 0.47\t train_acc: 98.7368392944336\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.158 \n",
      "maximize loss: -3.41\t minimize loss: 0.47\t train_acc: 99.15789031982422\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 98.737 \n",
      "maximize loss: -3.41\t minimize loss: 0.78\t train_acc: 98.7368392944336\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 98.947 \n",
      "maximize loss: -2.85\t minimize loss: 1.07\t train_acc: 98.9473648071289\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 98.526 \n",
      "maximize loss: -2.71\t minimize loss: 1.12\t train_acc: 98.52631378173828\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.579 \n",
      "maximize loss: -2.74\t minimize loss: 1.07\t train_acc: 99.57894134521484\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 99.579 \n",
      "maximize loss: -2.87\t minimize loss: 1.01\t train_acc: 99.57894134521484\n"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> scrub unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True, prefix=\"train_cifar5_resnet\")\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True, prefix=\"forget_cifar5_resnet\")\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\",)\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('scrub retain- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42105865478515625, 0.42105865478515625, 0.42105865478515625, 0.42105865478515625, 0.42105865478515625, 0.42105865478515625, 0.42105865478515625, 0.42105865478515625, 0.42105865478515625, 0.42105865478515625]\n",
      "[36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0]\n"
     ]
    }
   ],
   "source": [
    "print (acc_rs)\n",
    "print (acc_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTK based Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NTK Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_w_utils(model_init,dataloader,name='complete'):\n",
    "    model_init.eval()\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    G_list = []\n",
    "    f0_minus_y = []\n",
    "    for idx, batch in enumerate(dataloader):#(tqdm(dataloader,leave=False)):\n",
    "        batch = [tensor.to(next(model_init.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        if 'mnist' in args.dataset:\n",
    "            input = input.view(input.shape[0],-1)\n",
    "        target = target.cpu().detach().numpy()\n",
    "        output = model_init(input)\n",
    "        G_sample=[]\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output[0,cls],model_init.parameters(),retain_graph=True)\n",
    "            grads = np.concatenate([g.view(-1).cpu().numpy() for g in grads])\n",
    "            G_sample.append(grads)\n",
    "            G_list.append(grads)\n",
    "        if args.lossfn=='mse':\n",
    "            p = output.cpu().detach().numpy().transpose()\n",
    "            #loss_hess = np.eye(len(p))\n",
    "            target = 2*target-1\n",
    "            f0_y_update = p-target\n",
    "        elif args.lossfn=='ce':\n",
    "            p = torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().transpose()\n",
    "            p[target]-=1\n",
    "            f0_y_update = copy.deepcopy(p)\n",
    "        f0_minus_y.append(f0_y_update)\n",
    "    return np.stack(G_list).transpose(),np.vstack(f0_minus_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobians and Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NTK_data/G_r.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zihao/data/SCRUB-main/small_scale_unlearning_cifar5_resnet.ipynb Cell 44\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zihao/data/SCRUB-main/small_scale_unlearning_cifar5_resnet.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m t2 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zihao/data/SCRUB-main/small_scale_unlearning_cifar5_resnet.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ntk_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m t2\u001b[39m-\u001b[39mt1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zihao/data/SCRUB-main/small_scale_unlearning_cifar5_resnet.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m np\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39mNTK_data/G_r.npy\u001b[39;49m\u001b[39m'\u001b[39;49m,G_r)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zihao/data/SCRUB-main/small_scale_unlearning_cifar5_resnet.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mNTK_data/f0_minus_y_r.npy\u001b[39m\u001b[39m'\u001b[39m,f0_minus_y_r)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zihao/data/SCRUB-main/small_scale_unlearning_cifar5_resnet.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdel\u001b[39;00m G_r, f0_minus_y_r\n",
      "File \u001b[0;32m~/anaconda3/envs/zihao/lib/python3.10/site-packages/numpy/lib/npyio.py:542\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m file\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    541\u001b[0m         file \u001b[39m=\u001b[39m file \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 542\u001b[0m     file_ctx \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(file, \u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    544\u001b[0m \u001b[39mwith\u001b[39;00m file_ctx \u001b[39mas\u001b[39;00m fid:\n\u001b[1;32m    545\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(arr)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NTK_data/G_r.npy'"
     ]
    }
   ],
   "source": [
    "ntk_time = 0\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_r,f0_minus_y_r = delta_w_utils(copy.deepcopy(model),retain_loader,'complete')\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_r.npy',G_r)\n",
    "np.save('NTK_data/f0_minus_y_r.npy',f0_minus_y_r)\n",
    "del G_r, f0_minus_y_r\n",
    "\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_f,f0_minus_y_f = delta_w_utils(copy.deepcopy(model),forget_loader,'retain') \n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_f.npy',G_f)\n",
    "np.save('NTK_data/f0_minus_y_f.npy',f0_minus_y_f)\n",
    "del G_f, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "G_f = np.load('NTK_data/G_f.npy')\n",
    "G = np.concatenate([G_r,G_f],axis=1)\n",
    "\n",
    "np.save('NTK_data/G.npy',G)\n",
    "del G, G_f, G_r\n",
    "\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "f0_minus_y_f = np.load('NTK_data/f0_minus_y_f.npy')\n",
    "f0_minus_y = np.concatenate([f0_minus_y_r,f0_minus_y_f])\n",
    "\n",
    "np.save('NTK_data/f0_minus_y.npy',f0_minus_y)\n",
    "del f0_minus_y, f0_minus_y_r, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only requires access to the gradients and the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.load('NTK_data/G.npy')\n",
    "t1 = time.time()\n",
    "theta = G.transpose().dot(G) + num_total*args.weight_decay*np.eye(G.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G\n",
    "\n",
    "t1 = time.time()\n",
    "theta_inv = np.linalg.inv(theta)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta.npy',theta)\n",
    "del theta\n",
    "\n",
    "G = np.load('NTK_data/G.npy')\n",
    "f0_minus_y = np.load('NTK_data/f0_minus_y.npy')\n",
    "t1 = time.time()\n",
    "w_complete = -G.dot(theta_inv.dot(f0_minus_y))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_inv.npy',theta_inv)\n",
    "np.save('NTK_data/w_complete.npy',w_complete)\n",
    "\n",
    "del G, f0_minus_y, theta_inv, w_complete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "t1 = time.time()\n",
    "theta_r = G_r.transpose().dot(G_r) + num_to_retain*args.weight_decay*np.eye(G_r.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G_r\n",
    "\n",
    "t1 = time.time()\n",
    "theta_r_inv = np.linalg.inv(theta_r)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r.npy',theta_r)\n",
    "del theta_r\n",
    "\n",
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "t1 = time.time()\n",
    "w_retain = -G_r.dot(theta_r_inv.dot(f0_minus_y_r))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r_inv.npy',theta_r_inv)\n",
    "np.save('NTK_data/w_retain.npy',w_retain)\n",
    "\n",
    "del G_r, f0_minus_y_r, theta_r_inv, w_retain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrubbing Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Scrubbing Direction\n",
    "w_complete = np.load('NTK_data/w_complete.npy')\n",
    "w_retain = np.load('NTK_data/w_retain.npy')\n",
    "delta_w = (w_retain-w_complete).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_copy = copy.deepcopy(delta_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Change in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_actual = vectorize_params(model0)-vectorize_params(model)\n",
    "\n",
    "print(f'Actual Norm-: {np.linalg.norm(delta_w_actual)}')\n",
    "print(f'Predtn Norm-: {np.linalg.norm(delta_w)}')\n",
    "scale_ratio = np.linalg.norm(delta_w_actual)/np.linalg.norm(delta_w)\n",
    "print('Actual Scale: {}'.format(scale_ratio))\n",
    "log_dict['actual_scale_ratio']=scale_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trapezium Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred_error = vectorize_params(model)-vectorize_params(model_init)-w_retain.squeeze()\n",
    "print(f\"Delta w -------: {np.linalg.norm(delta_w)}\")\n",
    "\n",
    "inner = np.inner(delta_w/np.linalg.norm(delta_w),m_pred_error/np.linalg.norm(m_pred_error))\n",
    "print(f\"Inner Product--: {inner}\")\n",
    "\n",
    "if inner<0:\n",
    "    angle = np.arccos(inner)-np.pi/2\n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.sin(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "else:\n",
    "    angle = np.arccos(inner) \n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.cos(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "\n",
    "predicted_scale=predicted_norm/np.linalg.norm(delta_w)\n",
    "predicted_scale\n",
    "print(f\"Predicted Scale:  {predicted_scale}\")\n",
    "log_dict['predicted_scale_ratio']=predicted_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Inner Product between Prediction and Actual Scrubbing Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIP(v1,v2):\n",
    "    nip = (np.inner(v1/np.linalg.norm(v1),v2/np.linalg.norm(v2)))\n",
    "    print(nip)\n",
    "    return nip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nip=NIP(delta_w_actual,delta_w)\n",
    "log_dict['nip']=nip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_w_dict(delta_w,model):\n",
    "    # Give normalized delta_w\n",
    "    delta_w_dict = OrderedDict()\n",
    "    params_visited = 0\n",
    "    for k,p in model.named_parameters():\n",
    "        num_params = np.prod(list(p.shape))\n",
    "        update_params = delta_w[params_visited:params_visited+num_params]\n",
    "        delta_w_dict[k] = torch.Tensor(update_params).view_as(p)\n",
    "        params_visited+=num_params\n",
    "    return delta_w_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics,activations,predictions=get_metrics(model,dataloader,criterion,True)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "\n",
    "    return activations,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub using NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=predicted_scale\n",
    "direction = get_delta_w_dict(delta_w,model)\n",
    "\n",
    "model_scrub = copy.deepcopy(model)\n",
    "for k,p in model_scrub.named_parameters():\n",
    "    p.data += (direction[k]*scale).to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune and Fisher Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg\n",
    "\n",
    "def run_neggrad_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    forget_loader: torch.utils.data.DataLoader,\n",
    "                    alpha: float,\n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    quiet=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, (batch_retain,batch_forget) in enumerate(tqdm(zip(data_loader,cycle(forget_loader)), leave=False)):\n",
    "            batch_retain = [tensor.to(next(model.parameters()).device) for tensor in batch_retain]\n",
    "            batch_forget = [tensor.to(next(model.parameters()).device) for tensor in batch_forget]\n",
    "            input_r, target_r = batch_retain\n",
    "            input_f, target_f = batch_forget\n",
    "            output_r = model(input_r)\n",
    "            output_f = model(input_f)\n",
    "            loss = alpha*(loss_fn(output_r, target_r) + l2_penalty(model,model_init,args.weight_decay)) - (1-alpha)*loss_fn(output_f, target_f)\n",
    "            metrics.update(n=input_r.size(0), loss=loss_fn(output_r,target_r).item(), error=get_error(output_r, target_r))\n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def negative_grad(model: nn.Module, data_loader: torch.utils.data.DataLoader, forget_loader: torch.utils.data.DataLoader, alpha: float, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_neggrad_epoch(model, model_init, data_loader, forget_loader, alpha, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "\n",
    "def fk_fientune(model: nn.Module, data_loader: torch.utils.data.DataLoader, args, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.1, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computes the amount of information not forgotten at all layers using the given alpha\n",
    "alpha = 1e-7\n",
    "total_kl = 0\n",
    "torch.manual_seed(seed)\n",
    "for (k, p), (k0, p0) in zip(modelf.named_parameters(), modelf0.named_parameters()):\n",
    "    mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "    mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "    kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "    total_kl += kl\n",
    "    print(k, f'{kl:.1f}')\n",
    "print(\"Total:\", total_kl)\n",
    "log_dict['fisher_info']=total_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-6\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())\n",
    "\n",
    "for i, p in enumerate(modelf0.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test(modelf, retain_loader))\n",
    "print(test(modelf, forget_loader))\n",
    "print(test(modelf, valid_loader_full))\n",
    "print(test(modelf, test_loader_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.ng_alpha = 0.95\n",
    "args.ng_epochs = 10\n",
    "args.ng_lr = 0.01\n",
    "model_ng = copy.deepcopy(model)    \n",
    "negative_grad(model_ng, retain_loader, forget_loader, alpha=args.ng_alpha, epochs=args.ng_epochs, quiet=True, lr=args.ng_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catastrophic Forgetting k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.cfk_lr = 0.01\n",
    "args.cfk_epochs = 10\n",
    "\n",
    "model_cfk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_cfk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    layers = [9]\n",
    "    for k in layers:\n",
    "        for param in model_cfk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    for param in model_cfk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "fk_fientune(model_cfk, retain_loader, args=args, epochs=args.cfk_epochs, quiet=True, lr=args.cfk_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Unlearning k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The last block and classifier of resnet-18\n",
    "(layer4): Sequential(\n",
    "    (0): _ResBlock(\n",
    "      (bn1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(102, 204, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (shortcut): Sequential(\n",
    "        (0): Conv2d(102, 204, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "      )\n",
    "    )\n",
    "    (1): _ResBlock(\n",
    "      (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    )\n",
    "  )\n",
    "(linear): Linear(in_features=204, out_features=5, bias=True)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" The last block and classifier of allcnn\n",
    "AllCNN(\n",
    "  (features): Sequential(\n",
    "    ...\n",
    "    (9): Conv(\n",
    "      (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (2): ReLU()\n",
    "    )\n",
    "    (10): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
    "    (11): Flatten()\n",
    "  )\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=192, out_features=5, bias=True)\n",
    "  )\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.euk_lr = 0.01\n",
    "args.euk_epochs = training_epochs\n",
    "model_euk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_euk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    with torch.no_grad():\n",
    "        for k in layers:\n",
    "            for i in range(0,3):\n",
    "                try:\n",
    "                    model_euk.features[k][i].weight.copy_(model_initial.features[k][i].weight)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have weights\".format(k,i))\n",
    "                try:\n",
    "                    model_euk.features[k][i].bias.copy_(model_initial.features[k][i].bias)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have bias\".format(k,i))\n",
    "        model_euk.classifier[0].weight.copy_(model_initial.classifier[0].weight)\n",
    "        model_euk.classifier[0].bias.copy_(model_initial.classifier[0].bias)\n",
    "    \n",
    "    for k in layers:\n",
    "        for param in model_euk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,2):\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.weight.copy_(model_initial.layer4[i].bn1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.bias.copy_(model_initial.layer4[i].bn1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.weight.copy_(model_initial.layer4[i].conv1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.bias.copy_(model_initial.layer4[i].conv1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.weight.copy_(model_initial.layer4[i].bn2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.bias.copy_(model_initial.layer4[i].bn2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.weight.copy_(model_initial.layer4[i].conv2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.bias.copy_(model_initial.layer4[i].conv2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "        model_euk.layer4[0].shortcut[0].weight.copy_(model_initial.layer4[0].shortcut[0].weight)\n",
    "        \n",
    "    for param in model_euk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "fk_fientune(model_euk, retain_loader, epochs=args.euk_epochs, quiet=True, lr=args.euk_lr, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "_,_=activations_predictions(copy.deepcopy(model),forget_loader,'Original_Model_D_f')\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"c\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_ng),thresh,'NegGrad')\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model_cfk),thresh,'CF-k')\n",
    "readouts[\"f\"] = all_readouts(copy.deepcopy(model_euk),thresh,'EU-k')\n",
    "readouts[\"g\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"h\"] = all_readouts(copy.deepcopy(model_scrub),thresh,'NTK')\n",
    "readouts[\"i\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
